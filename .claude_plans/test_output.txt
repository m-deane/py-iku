============================= test session starts ==============================
platform linux -- Python 3.11.14, pytest-9.0.2, pluggy-1.6.0 -- /usr/local/bin/python
cachedir: .pytest_cache
rootdir: /home/user/py-iku
configfile: pyproject.toml
plugins: cov-7.0.0
collecting ... collected 843 items

tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesRegistry::test_combination_examples_dict_not_empty PASSED [  0%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesRegistry::test_list_combination_examples_returns_list PASSED [  0%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesRegistry::test_all_examples_are_strings PASSED [  0%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesRegistry::test_get_combination_example_returns_code PASSED [  0%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesRegistry::test_get_nonexistent_example_returns_empty PASSED [  0%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesRegistry::test_minimum_combination_count PASSED [  0%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesRegistry::test_settings_examples_dict_not_empty PASSED [  0%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesRegistry::test_list_settings_examples_returns_list PASSED [  0%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesRegistry::test_all_settings_examples_are_strings PASSED [  1%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_converts_successfully[prepare_grouping_prepare-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('raw_transactions.csv')\n\n# PREPARE: Clean data\ndf['customer_name'] = df['customer_name'].str.strip().str.title()\ndf['transaction_date'] = pd.to_datetime(df['transaction_date'])\ndf['amount'] = df['amount'].fillna(0)\ndf = df[df['amount'] > 0]\n\n# GROUPING: Aggregate by customer\ncustomer_summary = df.groupby('customer_id').agg({\n    'amount': ['sum', 'mean', 'count'],\n    'customer_name': 'first',\n    'transaction_date': 'max'\n}).reset_index()\ncustomer_summary.columns = ['customer_id', 'total_amount', 'avg_amount',\n                            'transaction_count', 'customer_name', 'last_transaction']\n\n# PREPARE: Format output\ncustomer_summary['total_amount'] = customer_summary['total_amount'].round(2)\ncustomer_summary['avg_amount'] = customer_summary['avg_amount'].round(2)\ncustomer_summary['customer_tier'] = pd.cut(\n    customer_summary['total_amount'],\n    bins=[0, 100, 500, 1000, float('inf')],\n    labels=['Bronze', 'Silver', 'Gold', 'Platinum']\n)\n\ncustomer_summary.to_csv('customer_summary.csv', index=False)\n] PASSED [  1%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_converts_successfully[join_window_split-\nimport pandas as pd\n\n# Load data\norders = pd.read_csv('orders.csv')\nproducts = pd.read_csv('products.csv')\n\n# JOIN: Combine orders with products\norders_enriched = pd.merge(orders, products, on='product_id', how='left')\n\n# WINDOW: Calculate running totals and rankings\norders_enriched = orders_enriched.sort_values(['customer_id', 'order_date'])\norders_enriched['customer_running_total'] = orders_enriched.groupby('customer_id')['amount'].cumsum()\norders_enriched['customer_order_rank'] = orders_enriched.groupby('customer_id').cumcount() + 1\n\n# SPLIT: Partition into first-time and repeat customers\nfirst_time = orders_enriched[orders_enriched['customer_order_rank'] == 1]\nrepeat = orders_enriched[orders_enriched['customer_order_rank'] > 1]\n\nfirst_time.to_csv('first_time_orders.csv', index=False)\nrepeat.to_csv('repeat_orders.csv', index=False)\n] PASSED [  1%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_converts_successfully[stack_distinct_sort-\nimport pandas as pd\n\n# Load multiple data sources\nsource1 = pd.read_csv('customers_crm.csv')\nsource2 = pd.read_csv('customers_web.csv')\nsource3 = pd.read_csv('customers_mobile.csv')\n\n# STACK: Combine all sources\nsource1['source'] = 'crm'\nsource2['source'] = 'web'\nsource3['source'] = 'mobile'\nall_customers = pd.concat([source1, source2, source3], ignore_index=True)\n\n# DISTINCT: Remove duplicates (keep first by source priority)\nall_customers['source_priority'] = all_customers['source'].map({'crm': 1, 'web': 2, 'mobile': 3})\nall_customers = all_customers.sort_values('source_priority')\nunique_customers = all_customers.drop_duplicates(subset=['email'], keep='first')\n\n# SORT: Order by registration date\nunique_customers = unique_customers.sort_values('registration_date', ascending=False)\n\nunique_customers.to_csv('unified_customers.csv', index=False)\n] PASSED [  1%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_converts_successfully[grouping_pivot_prepare-\nimport pandas as pd\n\n# Load data\nsales = pd.read_csv('sales_detail.csv')\nsales['date'] = pd.to_datetime(sales['date'])\nsales['month'] = sales['date'].dt.strftime('%Y-%m')\n\n# GROUPING: Aggregate by product and month\nmonthly_sales = sales.groupby(['product_id', 'month']).agg({\n    'amount': 'sum',\n    'quantity': 'sum'\n}).reset_index()\n\n# PIVOT: Reshape to wide format\npivot_amount = monthly_sales.pivot(index='product_id', columns='month', values='amount')\npivot_amount = pivot_amount.reset_index()\n\n# PREPARE: Clean and calculate totals\npivot_amount = pivot_amount.fillna(0)\nnumeric_cols = pivot_amount.select_dtypes(include='number').columns\npivot_amount['total'] = pivot_amount[numeric_cols].sum(axis=1)\npivot_amount['avg_monthly'] = pivot_amount[numeric_cols].mean(axis=1).round(2)\n\npivot_amount.to_csv('product_monthly_pivot.csv', index=False)\n] PASSED [  1%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_converts_successfully[split_join_grouping-\nimport pandas as pd\n\n# Load data\ntransactions = pd.read_csv('transactions.csv')\ncustomers = pd.read_csv('customers.csv')\n\n# SPLIT: Filter to high-value transactions\nhigh_value = transactions[transactions['amount'] >= 100]\n\n# JOIN: Enrich with customer data\nhigh_value_enriched = pd.merge(\n    high_value,\n    customers[['customer_id', 'segment', 'region']],\n    on='customer_id',\n    how='left'\n)\n\n# GROUPING: Aggregate by segment and region\nsegment_summary = high_value_enriched.groupby(['segment', 'region']).agg({\n    'amount': ['sum', 'mean', 'count'],\n    'customer_id': 'nunique'\n}).reset_index()\nsegment_summary.columns = ['segment', 'region', 'total_amount', 'avg_amount',\n                           'transaction_count', 'unique_customers']\n\nsegment_summary.to_csv('high_value_segment_summary.csv', index=False)\n] PASSED [  1%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_converts_successfully[prepare_topn_prepare-\nimport pandas as pd\n\n# Load data\nproducts = pd.read_csv('products.csv')\n\n# PREPARE: Clean product data\nproducts['product_name'] = products['product_name'].str.strip().str.title()\nproducts['revenue'] = products['revenue'].fillna(0)\nproducts['margin'] = products['margin'].fillna(0)\nproducts['composite_score'] = products['revenue'] * 0.7 + products['margin'] * 0.3\n\n# TOP_N: Get top 100 products\ntop_products = products.nlargest(100, 'composite_score')\n\n# PREPARE: Format for report\ntop_products['rank'] = range(1, len(top_products) + 1)\ntop_products['revenue_formatted'] = top_products['revenue'].apply(lambda x: f'${x:,.2f}')\ntop_products['performance'] = pd.cut(\n    top_products['composite_score'],\n    bins=[0, 50, 100, float('inf')],\n    labels=['Underperformer', 'Average', 'Star']\n)\n\ntop_products.to_csv('top_100_products.csv', index=False)\n] PASSED [  1%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_converts_successfully[sampling_prepare_grouping-\nimport pandas as pd\n\n# Load data\nlarge_dataset = pd.read_csv('large_logs.csv')\n\n# SAMPLING: Take 10% sample\nsample = large_dataset.sample(frac=0.1, random_state=42)\n\n# PREPARE: Clean and transform\nsample['timestamp'] = pd.to_datetime(sample['timestamp'])\nsample['hour'] = sample['timestamp'].dt.hour\nsample['is_error'] = sample['status_code'] >= 400\nsample['response_time_ms'] = sample['response_time'] * 1000\n\n# GROUPING: Aggregate by hour\nhourly_stats = sample.groupby('hour').agg({\n    'request_id': 'count',\n    'response_time_ms': ['mean', 'median', 'std'],\n    'is_error': 'sum'\n}).reset_index()\nhourly_stats.columns = ['hour', 'request_count', 'avg_response_ms',\n                        'median_response_ms', 'std_response_ms', 'error_count']\nhourly_stats['error_rate'] = (hourly_stats['error_count'] / hourly_stats['request_count'] * 100).round(2)\n\nhourly_stats.to_csv('hourly_stats_sample.csv', index=False)\n] PASSED [  1%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_converts_successfully[multi_join_grouping-\nimport pandas as pd\n\n# Load data\norders = pd.read_csv('orders.csv')\ncustomers = pd.read_csv('customers.csv')\nproducts = pd.read_csv('products.csv')\ncategories = pd.read_csv('categories.csv')\n\n# JOIN 1: Orders with customers\norders_customers = pd.merge(orders, customers[['customer_id', 'segment', 'region']],\n                             on='customer_id', how='left')\n\n# JOIN 2: With products and categories\norders_full = pd.merge(orders_customers, products[['product_id', 'category_id', 'product_name']],\n                        on='product_id', how='left')\norders_full = pd.merge(orders_full, categories[['category_id', 'category_name']],\n                        on='category_id', how='left')\n\n# GROUPING: Multi-dimensional aggregation\nsummary = orders_full.groupby(['segment', 'region', 'category_name']).agg({\n    'order_id': 'count',\n    'amount': 'sum',\n    'customer_id': 'nunique'\n}).reset_index()\nsummary.columns = ['segment', 'region', 'category', 'order_count', 'total_revenue', 'unique_customers']\n\nsummary.to_csv('multi_dimensional_summary.csv', index=False)\n] PASSED [  2%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_converts_successfully[multi_stack_distinct-\nimport pandas as pd\n\n# Load data from multiple years and sources\nsales_2022_web = pd.read_csv('sales_2022_web.csv')\nsales_2022_store = pd.read_csv('sales_2022_store.csv')\nsales_2023_web = pd.read_csv('sales_2023_web.csv')\nsales_2023_store = pd.read_csv('sales_2023_store.csv')\n\n# STACK 1: Combine 2022 sources\nsales_2022_web['source'] = 'web'\nsales_2022_store['source'] = 'store'\nsales_2022 = pd.concat([sales_2022_web, sales_2022_store], ignore_index=True)\nsales_2022['year'] = 2022\n\n# STACK 2: Combine 2023 sources\nsales_2023_web['source'] = 'web'\nsales_2023_store['source'] = 'store'\nsales_2023 = pd.concat([sales_2023_web, sales_2023_store], ignore_index=True)\nsales_2023['year'] = 2023\n\n# STACK 3: Combine all years\nall_sales = pd.concat([sales_2022, sales_2023], ignore_index=True)\n\n# DISTINCT: Remove duplicates\nall_sales = all_sales.drop_duplicates(subset=['transaction_id'])\n\nall_sales.to_csv('all_sales_combined.csv', index=False)\n] PASSED [  2%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_converts_successfully[window_grouping_sort-\nimport pandas as pd\n\n# Load data\ndaily_metrics = pd.read_csv('daily_metrics.csv')\ndaily_metrics['date'] = pd.to_datetime(daily_metrics['date'])\n\n# WINDOW: Calculate rolling metrics\ndaily_metrics = daily_metrics.sort_values(['product_id', 'date'])\ndaily_metrics['rolling_7d_avg'] = daily_metrics.groupby('product_id')['revenue'].transform(\n    lambda x: x.rolling(7, min_periods=1).mean()\n)\ndaily_metrics['rolling_7d_sum'] = daily_metrics.groupby('product_id')['revenue'].transform(\n    lambda x: x.rolling(7, min_periods=1).sum()\n)\n\n# GROUPING: Aggregate by product\nproduct_summary = daily_metrics.groupby('product_id').agg({\n    'revenue': 'sum',\n    'rolling_7d_avg': 'last',\n    'rolling_7d_sum': 'last',\n    'date': ['min', 'max']\n}).reset_index()\nproduct_summary.columns = ['product_id', 'total_revenue', 'latest_7d_avg',\n                            'latest_7d_sum', 'first_date', 'last_date']\n\n# SORT: Order by total revenue\nproduct_summary = product_summary.sort_values('total_revenue', ascending=False)\n\nproduct_summary.to_csv('product_summary_windowed.csv', index=False)\n] PASSED [  2%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_converts_successfully[full_etl_pipeline-\nimport pandas as pd\nimport numpy as np\n\n# Load raw data sources\ncustomers = pd.read_csv('raw_customers.csv')\norders = pd.read_csv('raw_orders.csv')\nproducts = pd.read_csv('raw_products.csv')\n\n# PREPARE 1: Clean customers\ncustomers['email'] = customers['email'].str.lower().str.strip()\ncustomers['name'] = customers['name'].str.strip().str.title()\ncustomers = customers.dropna(subset=['customer_id', 'email'])\ncustomers = customers.drop_duplicates(subset=['email'])\n\n# JOIN 1: Orders with customers\norders_enriched = pd.merge(\n    orders,\n    customers[['customer_id', 'name', 'email', 'segment']],\n    on='customer_id',\n    how='inner'\n)\n\n# JOIN 2: With products\norders_full = pd.merge(\n    orders_enriched,\n    products[['product_id', 'product_name', 'category', 'unit_cost']],\n    on='product_id',\n    how='left'\n)\n\n# PREPARE 2: Calculate metrics\norders_full['order_date'] = pd.to_datetime(orders_full['order_date'])\norders_full['margin'] = orders_full['amount'] - orders_full['unit_cost'] * orders_full['quantity']\norders_full['margin_pct'] = (orders_full['margin'] / orders_full['amount'] * 100).round(2)\n\n# GROUPING: Customer-level aggregation\ncustomer_metrics = orders_full.groupby(['customer_id', 'name', 'segment']).agg({\n    'order_id': 'count',\n    'amount': 'sum',\n    'margin': 'sum',\n    'order_date': ['min', 'max']\n}).reset_index()\ncustomer_metrics.columns = ['customer_id', 'name', 'segment', 'order_count',\n                             'total_revenue', 'total_margin', 'first_order', 'last_order']\n\n# WINDOW: Add customer rankings\ncustomer_metrics = customer_metrics.sort_values('total_revenue', ascending=False)\ncustomer_metrics['revenue_rank'] = range(1, len(customer_metrics) + 1)\ncustomer_metrics['revenue_percentile'] = (\n    customer_metrics['revenue_rank'] / len(customer_metrics) * 100\n).round(1)\n\n# PREPARE 3: Final formatting\ncustomer_metrics['clv_tier'] = pd.cut(\n    customer_metrics['total_revenue'],\n    bins=[0, 100, 500, 2000, float('inf')],\n    labels=['Low', 'Medium', 'High', 'VIP']\n)\n\ncustomer_metrics.to_csv('customer_analytics.csv', index=False)\n] PASSED [  2%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_converts_successfully[text_pipeline-\nimport pandas as pd\n\ndf = pd.read_csv('raw_text_data.csv')\n\n# STRING_TRANSFORMER: Clean text\ndf['name'] = df['name'].str.strip().str.title()\ndf['email'] = df['email'].str.lower().str.strip()\ndf['code'] = df['code'].str.upper()\n\n# FILL_EMPTY: Handle missing values\ndf['name'] = df['name'].fillna('Unknown')\ndf['email'] = df['email'].fillna('')\ndf['code'] = df['code'].fillna('N/A')\n\n# TYPE_SETTER: Ensure correct types\ndf['customer_id'] = df['customer_id'].astype(str)\ndf['is_active'] = df['is_active'].astype(bool)\n\ndf.to_csv('cleaned_text_data.csv', index=False)\n] PASSED [  2%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_converts_successfully[date_pipeline-\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\ndf = pd.read_csv('events.csv')\n\n# DATE_PARSER: Parse date strings\ndf['event_date'] = pd.to_datetime(df['event_date_str'])\ndf['created_at'] = pd.to_datetime(df['created_at_str'])\n\n# FORMULA: Calculate date-based metrics\ndf['days_since_event'] = (datetime.now() - df['event_date']).dt.days\ndf['processing_time'] = (df['event_date'] - df['created_at']).dt.total_seconds() / 3600\ndf['event_year'] = df['event_date'].dt.year\ndf['event_month'] = df['event_date'].dt.month\ndf['is_recent'] = (df['days_since_event'] <= 30).astype(int)\n\n# FILTER_ON_DATE_RANGE: Keep only last 365 days\ncutoff_date = datetime.now() - timedelta(days=365)\ndf = df[df['event_date'] >= cutoff_date]\n\ndf.to_csv('processed_events.csv', index=False)\n] PASSED [  2%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_converts_successfully[column_pipeline-\nimport pandas as pd\n\ndf = pd.read_csv('raw_export.csv')\n\n# COLUMN_RENAMER: Standardize column names\ndf = df.rename(columns={\n    'cust_id': 'customer_id',\n    'prod_nm': 'product_name',\n    'qty': 'quantity',\n    'amt': 'amount',\n    'dt': 'date'\n})\n\n# COLUMN_DELETER: Remove unnecessary columns\ncolumns_to_drop = ['temp_col', 'debug_info', 'internal_id', '_row_num']\ndf = df.drop(columns=[c for c in columns_to_drop if c in df.columns])\n\n# COLUMNS_SELECTOR: Select final columns in order\nfinal_columns = ['customer_id', 'product_name', 'quantity', 'amount', 'date']\ndf = df[[c for c in final_columns if c in df.columns]]\n\ndf.to_csv('cleaned_export.csv', index=False)\n] PASSED [  2%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_converts_successfully[numeric_pipeline-\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('raw_metrics.csv')\n\n# NUMERICAL_TRANSFORMER: Mathematical transformations\ndf['log_value'] = np.log1p(df['value'])\ndf['normalized'] = (df['value'] - df['value'].mean()) / df['value'].std()\ndf['pct_of_max'] = df['value'] / df['value'].max() * 100\n\n# ROUND: Round to appropriate precision\ndf['value_rounded'] = df['value'].round(2)\ndf['pct_of_max'] = df['pct_of_max'].round(1)\n\n# CLIP: Constrain to valid range\ndf['score'] = df['score'].clip(lower=0, upper=100)\ndf['normalized'] = df['normalized'].clip(lower=-3, upper=3)\n\n# BINNER: Create categories\ndf['value_bucket'] = pd.cut(\n    df['value_rounded'],\n    bins=[0, 10, 50, 100, 500, float('inf')],\n    labels=['XS', 'S', 'M', 'L', 'XL']\n)\ndf['score_quartile'] = pd.qcut(df['score'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n\ndf.to_csv('processed_metrics.csv', index=False)\n] PASSED [  2%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_converts_successfully[text_extraction_pipeline-\nimport pandas as pd\n\ndf = pd.read_csv('raw_contacts.csv')\n\n# REGEXP_EXTRACTOR: Extract patterns\ndf['area_code'] = df['phone'].str.extract(r'\\((\\d{3})\\)')\ndf['email_domain'] = df['email'].str.extract(r'@(.+)$')\ndf['zip_code'] = df['address'].str.extract(r'(\\d{5})(?:-\\d{4})?$')\n\n# SPLIT_COLUMN: Split into parts\nname_parts = df['full_name'].str.split(' ', n=1, expand=True)\ndf['first_name'] = name_parts[0]\ndf['last_name'] = name_parts[1]\n\naddress_parts = df['address'].str.split(',', expand=True)\ndf['street'] = address_parts[0]\ndf['city_state'] = address_parts[1]\n\n# CONCAT_COLUMNS: Combine fields\ndf['display_name'] = df['first_name'] + ' ' + df['last_name'].str[0] + '.'\ndf['contact_info'] = df['email'] + ' | ' + df['phone']\n\ndf.to_csv('parsed_contacts.csv', index=False)\n] PASSED [  2%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_converts_successfully[cleaning_pipeline-\nimport pandas as pd\n\ndf = pd.read_csv('dirty_data.csv')\n\n# FILL_EMPTY: Fill non-critical fields\ndf['category'] = df['category'].fillna('Other')\ndf['score'] = df['score'].fillna(df['score'].median())\ndf['notes'] = df['notes'].fillna('')\n\n# REMOVE_ROWS_ON_EMPTY: Remove rows with critical missing values\ndf = df.dropna(subset=['customer_id', 'email', 'amount'])\n\n# REMOVE_DUPLICATES: Deduplicate\ndf = df.drop_duplicates(subset=['customer_id', 'transaction_id'], keep='first')\ndf = df.drop_duplicates(subset=['email'], keep='last')\n\ndf.to_csv('clean_data.csv', index=False)\n] PASSED [  3%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_converts_successfully[flagging_pipeline-\nimport pandas as pd\n\ndf = pd.read_csv('transactions.csv')\n\n# FILTER_ON_VALUE: Keep valid transactions\ndf = df[df['status'] != 'cancelled']\ndf = df[df['amount'] > 0]\n\n# FLAG_ON_VALUE: Create flag columns\ndf['is_premium'] = (df['customer_tier'] == 'premium').astype(int)\ndf['is_high_value'] = (df['amount'] >= 1000).astype(int)\ndf['needs_review'] = (df['risk_score'] >= 0.8).astype(int)\n\n# CREATE_COLUMN_WITH_GREL: Complex computed columns\ndf['priority'] = df.apply(\n    lambda row: 'Critical' if row['is_high_value'] and row['needs_review']\n    else 'High' if row['is_high_value'] or row['needs_review']\n    else 'Normal',\n    axis=1\n)\n\ndf['display_label'] = df.apply(\n    lambda row: f"{row['transaction_id']} - {row['customer_name']} (${row['amount']:,.2f})",\n    axis=1\n)\n\ndf.to_csv('flagged_transactions.csv', index=False)\n] PASSED [  3%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_converts_successfully[ml_prep_pipeline-\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('raw_features.csv')\n\n# TYPE_SETTER: Ensure correct types\ndf['age'] = df['age'].astype(float)\ndf['income'] = df['income'].astype(float)\ndf['category'] = df['category'].astype(str)\n\n# NORMALIZER: Normalize numeric features\nnumeric_cols = ['age', 'income', 'score']\nfor col in numeric_cols:\n    df[f'{col}_normalized'] = (df[col] - df[col].mean()) / df[col].std()\n    df[f'{col}_minmax'] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())\n\n# CATEGORICAL_ENCODER: Encode categorical features\ndf_encoded = pd.get_dummies(df, columns=['category', 'region'], prefix=['cat', 'reg'])\n\ndf_encoded.to_csv('ml_features.csv', index=False)\n] PASSED [  3%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_converts_successfully[all_filter_processors-\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\ndf = pd.read_csv('data.csv')\ndf['date'] = pd.to_datetime(df['date'])\n\n# FILTER_ON_VALUE: Exact value filter\ndf = df[df['status'] == 'active']\n\n# FILTER_ON_FORMULA: Complex condition\ndf = df[(df['amount'] > 100) | (df['priority'] == 'high')]\n\n# FILTER_ON_DATE_RANGE: Date filter\ncutoff = datetime.now() - timedelta(days=90)\ndf = df[df['date'] >= cutoff]\n\n# FILTER_ON_NUMERIC_RANGE: Numeric filter\ndf = df[df['score'].between(0, 100)]\n\n# FILTER_ON_BAD_TYPE: Type validation\ndf['amount_valid'] = pd.to_numeric(df['amount'], errors='coerce')\ndf = df[df['amount_valid'].notna()]\n\ndf.to_csv('filtered_data.csv', index=False)\n] PASSED [  3%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_converts_successfully[all_flag_processors-\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\ndf = pd.read_csv('data.csv')\ndf['date'] = pd.to_datetime(df['date'])\n\n# FLAG_ON_VALUE\ndf['is_premium'] = (df['tier'] == 'premium').astype(int)\n\n# FLAG_ON_FORMULA\ndf['high_value_recent'] = (\n    (df['amount'] >= 1000) &\n    (df['date'] >= datetime.now() - timedelta(days=30))\n).astype(int)\n\n# FLAG_ON_BAD_TYPE\ndf['has_valid_email'] = df['email'].str.contains('@', na=False).astype(int)\n\n# FLAG_ON_DATE_RANGE\ndf['is_q1'] = df['date'].dt.quarter.eq(1).astype(int)\n\n# FLAG_ON_NUMERIC_RANGE\ndf['score_in_range'] = df['score'].between(50, 100).astype(int)\n\ndf.to_csv('flagged_data.csv', index=False)\n] PASSED [  3%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_converts_successfully[all_missing_value_processors-\nimport pandas as pd\n\ndf = pd.read_csv('data_with_nulls.csv')\n\n# FILL_EMPTY_WITH_VALUE: Constant fill\ndf['category'] = df['category'].fillna('Unknown')\n\n# FILL_EMPTY_WITH_COMPUTED_VALUE: Computed fill\ndf['amount'] = df['amount'].fillna(df['amount'].mean())\ndf['score'] = df.groupby('category')['score'].transform(lambda x: x.fillna(x.median()))\n\n# FILL_EMPTY_WITH_PREVIOUS_NEXT: Forward/backward fill\ndf = df.sort_values('date')\ndf['metric'] = df['metric'].ffill()\ndf['metric'] = df['metric'].bfill()\n\n# REMOVE_ROWS_ON_EMPTY: Drop rows\ndf = df.dropna(subset=['customer_id', 'email'])\n\ndf.to_csv('no_missing_data.csv', index=False)\n] PASSED [  3%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_produces_flow[prepare_grouping_prepare-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('raw_transactions.csv')\n\n# PREPARE: Clean data\ndf['customer_name'] = df['customer_name'].str.strip().str.title()\ndf['transaction_date'] = pd.to_datetime(df['transaction_date'])\ndf['amount'] = df['amount'].fillna(0)\ndf = df[df['amount'] > 0]\n\n# GROUPING: Aggregate by customer\ncustomer_summary = df.groupby('customer_id').agg({\n    'amount': ['sum', 'mean', 'count'],\n    'customer_name': 'first',\n    'transaction_date': 'max'\n}).reset_index()\ncustomer_summary.columns = ['customer_id', 'total_amount', 'avg_amount',\n                            'transaction_count', 'customer_name', 'last_transaction']\n\n# PREPARE: Format output\ncustomer_summary['total_amount'] = customer_summary['total_amount'].round(2)\ncustomer_summary['avg_amount'] = customer_summary['avg_amount'].round(2)\ncustomer_summary['customer_tier'] = pd.cut(\n    customer_summary['total_amount'],\n    bins=[0, 100, 500, 1000, float('inf')],\n    labels=['Bronze', 'Silver', 'Gold', 'Platinum']\n)\n\ncustomer_summary.to_csv('customer_summary.csv', index=False)\n] PASSED [  3%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_produces_flow[join_window_split-\nimport pandas as pd\n\n# Load data\norders = pd.read_csv('orders.csv')\nproducts = pd.read_csv('products.csv')\n\n# JOIN: Combine orders with products\norders_enriched = pd.merge(orders, products, on='product_id', how='left')\n\n# WINDOW: Calculate running totals and rankings\norders_enriched = orders_enriched.sort_values(['customer_id', 'order_date'])\norders_enriched['customer_running_total'] = orders_enriched.groupby('customer_id')['amount'].cumsum()\norders_enriched['customer_order_rank'] = orders_enriched.groupby('customer_id').cumcount() + 1\n\n# SPLIT: Partition into first-time and repeat customers\nfirst_time = orders_enriched[orders_enriched['customer_order_rank'] == 1]\nrepeat = orders_enriched[orders_enriched['customer_order_rank'] > 1]\n\nfirst_time.to_csv('first_time_orders.csv', index=False)\nrepeat.to_csv('repeat_orders.csv', index=False)\n] PASSED [  3%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_produces_flow[stack_distinct_sort-\nimport pandas as pd\n\n# Load multiple data sources\nsource1 = pd.read_csv('customers_crm.csv')\nsource2 = pd.read_csv('customers_web.csv')\nsource3 = pd.read_csv('customers_mobile.csv')\n\n# STACK: Combine all sources\nsource1['source'] = 'crm'\nsource2['source'] = 'web'\nsource3['source'] = 'mobile'\nall_customers = pd.concat([source1, source2, source3], ignore_index=True)\n\n# DISTINCT: Remove duplicates (keep first by source priority)\nall_customers['source_priority'] = all_customers['source'].map({'crm': 1, 'web': 2, 'mobile': 3})\nall_customers = all_customers.sort_values('source_priority')\nunique_customers = all_customers.drop_duplicates(subset=['email'], keep='first')\n\n# SORT: Order by registration date\nunique_customers = unique_customers.sort_values('registration_date', ascending=False)\n\nunique_customers.to_csv('unified_customers.csv', index=False)\n] PASSED [  4%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_produces_flow[grouping_pivot_prepare-\nimport pandas as pd\n\n# Load data\nsales = pd.read_csv('sales_detail.csv')\nsales['date'] = pd.to_datetime(sales['date'])\nsales['month'] = sales['date'].dt.strftime('%Y-%m')\n\n# GROUPING: Aggregate by product and month\nmonthly_sales = sales.groupby(['product_id', 'month']).agg({\n    'amount': 'sum',\n    'quantity': 'sum'\n}).reset_index()\n\n# PIVOT: Reshape to wide format\npivot_amount = monthly_sales.pivot(index='product_id', columns='month', values='amount')\npivot_amount = pivot_amount.reset_index()\n\n# PREPARE: Clean and calculate totals\npivot_amount = pivot_amount.fillna(0)\nnumeric_cols = pivot_amount.select_dtypes(include='number').columns\npivot_amount['total'] = pivot_amount[numeric_cols].sum(axis=1)\npivot_amount['avg_monthly'] = pivot_amount[numeric_cols].mean(axis=1).round(2)\n\npivot_amount.to_csv('product_monthly_pivot.csv', index=False)\n] PASSED [  4%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_produces_flow[split_join_grouping-\nimport pandas as pd\n\n# Load data\ntransactions = pd.read_csv('transactions.csv')\ncustomers = pd.read_csv('customers.csv')\n\n# SPLIT: Filter to high-value transactions\nhigh_value = transactions[transactions['amount'] >= 100]\n\n# JOIN: Enrich with customer data\nhigh_value_enriched = pd.merge(\n    high_value,\n    customers[['customer_id', 'segment', 'region']],\n    on='customer_id',\n    how='left'\n)\n\n# GROUPING: Aggregate by segment and region\nsegment_summary = high_value_enriched.groupby(['segment', 'region']).agg({\n    'amount': ['sum', 'mean', 'count'],\n    'customer_id': 'nunique'\n}).reset_index()\nsegment_summary.columns = ['segment', 'region', 'total_amount', 'avg_amount',\n                           'transaction_count', 'unique_customers']\n\nsegment_summary.to_csv('high_value_segment_summary.csv', index=False)\n] PASSED [  4%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_produces_flow[prepare_topn_prepare-\nimport pandas as pd\n\n# Load data\nproducts = pd.read_csv('products.csv')\n\n# PREPARE: Clean product data\nproducts['product_name'] = products['product_name'].str.strip().str.title()\nproducts['revenue'] = products['revenue'].fillna(0)\nproducts['margin'] = products['margin'].fillna(0)\nproducts['composite_score'] = products['revenue'] * 0.7 + products['margin'] * 0.3\n\n# TOP_N: Get top 100 products\ntop_products = products.nlargest(100, 'composite_score')\n\n# PREPARE: Format for report\ntop_products['rank'] = range(1, len(top_products) + 1)\ntop_products['revenue_formatted'] = top_products['revenue'].apply(lambda x: f'${x:,.2f}')\ntop_products['performance'] = pd.cut(\n    top_products['composite_score'],\n    bins=[0, 50, 100, float('inf')],\n    labels=['Underperformer', 'Average', 'Star']\n)\n\ntop_products.to_csv('top_100_products.csv', index=False)\n] PASSED [  4%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_produces_flow[sampling_prepare_grouping-\nimport pandas as pd\n\n# Load data\nlarge_dataset = pd.read_csv('large_logs.csv')\n\n# SAMPLING: Take 10% sample\nsample = large_dataset.sample(frac=0.1, random_state=42)\n\n# PREPARE: Clean and transform\nsample['timestamp'] = pd.to_datetime(sample['timestamp'])\nsample['hour'] = sample['timestamp'].dt.hour\nsample['is_error'] = sample['status_code'] >= 400\nsample['response_time_ms'] = sample['response_time'] * 1000\n\n# GROUPING: Aggregate by hour\nhourly_stats = sample.groupby('hour').agg({\n    'request_id': 'count',\n    'response_time_ms': ['mean', 'median', 'std'],\n    'is_error': 'sum'\n}).reset_index()\nhourly_stats.columns = ['hour', 'request_count', 'avg_response_ms',\n                        'median_response_ms', 'std_response_ms', 'error_count']\nhourly_stats['error_rate'] = (hourly_stats['error_count'] / hourly_stats['request_count'] * 100).round(2)\n\nhourly_stats.to_csv('hourly_stats_sample.csv', index=False)\n] PASSED [  4%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_produces_flow[multi_join_grouping-\nimport pandas as pd\n\n# Load data\norders = pd.read_csv('orders.csv')\ncustomers = pd.read_csv('customers.csv')\nproducts = pd.read_csv('products.csv')\ncategories = pd.read_csv('categories.csv')\n\n# JOIN 1: Orders with customers\norders_customers = pd.merge(orders, customers[['customer_id', 'segment', 'region']],\n                             on='customer_id', how='left')\n\n# JOIN 2: With products and categories\norders_full = pd.merge(orders_customers, products[['product_id', 'category_id', 'product_name']],\n                        on='product_id', how='left')\norders_full = pd.merge(orders_full, categories[['category_id', 'category_name']],\n                        on='category_id', how='left')\n\n# GROUPING: Multi-dimensional aggregation\nsummary = orders_full.groupby(['segment', 'region', 'category_name']).agg({\n    'order_id': 'count',\n    'amount': 'sum',\n    'customer_id': 'nunique'\n}).reset_index()\nsummary.columns = ['segment', 'region', 'category', 'order_count', 'total_revenue', 'unique_customers']\n\nsummary.to_csv('multi_dimensional_summary.csv', index=False)\n] PASSED [  4%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_produces_flow[multi_stack_distinct-\nimport pandas as pd\n\n# Load data from multiple years and sources\nsales_2022_web = pd.read_csv('sales_2022_web.csv')\nsales_2022_store = pd.read_csv('sales_2022_store.csv')\nsales_2023_web = pd.read_csv('sales_2023_web.csv')\nsales_2023_store = pd.read_csv('sales_2023_store.csv')\n\n# STACK 1: Combine 2022 sources\nsales_2022_web['source'] = 'web'\nsales_2022_store['source'] = 'store'\nsales_2022 = pd.concat([sales_2022_web, sales_2022_store], ignore_index=True)\nsales_2022['year'] = 2022\n\n# STACK 2: Combine 2023 sources\nsales_2023_web['source'] = 'web'\nsales_2023_store['source'] = 'store'\nsales_2023 = pd.concat([sales_2023_web, sales_2023_store], ignore_index=True)\nsales_2023['year'] = 2023\n\n# STACK 3: Combine all years\nall_sales = pd.concat([sales_2022, sales_2023], ignore_index=True)\n\n# DISTINCT: Remove duplicates\nall_sales = all_sales.drop_duplicates(subset=['transaction_id'])\n\nall_sales.to_csv('all_sales_combined.csv', index=False)\n] PASSED [  4%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_produces_flow[window_grouping_sort-\nimport pandas as pd\n\n# Load data\ndaily_metrics = pd.read_csv('daily_metrics.csv')\ndaily_metrics['date'] = pd.to_datetime(daily_metrics['date'])\n\n# WINDOW: Calculate rolling metrics\ndaily_metrics = daily_metrics.sort_values(['product_id', 'date'])\ndaily_metrics['rolling_7d_avg'] = daily_metrics.groupby('product_id')['revenue'].transform(\n    lambda x: x.rolling(7, min_periods=1).mean()\n)\ndaily_metrics['rolling_7d_sum'] = daily_metrics.groupby('product_id')['revenue'].transform(\n    lambda x: x.rolling(7, min_periods=1).sum()\n)\n\n# GROUPING: Aggregate by product\nproduct_summary = daily_metrics.groupby('product_id').agg({\n    'revenue': 'sum',\n    'rolling_7d_avg': 'last',\n    'rolling_7d_sum': 'last',\n    'date': ['min', 'max']\n}).reset_index()\nproduct_summary.columns = ['product_id', 'total_revenue', 'latest_7d_avg',\n                            'latest_7d_sum', 'first_date', 'last_date']\n\n# SORT: Order by total revenue\nproduct_summary = product_summary.sort_values('total_revenue', ascending=False)\n\nproduct_summary.to_csv('product_summary_windowed.csv', index=False)\n] PASSED [  4%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_produces_flow[full_etl_pipeline-\nimport pandas as pd\nimport numpy as np\n\n# Load raw data sources\ncustomers = pd.read_csv('raw_customers.csv')\norders = pd.read_csv('raw_orders.csv')\nproducts = pd.read_csv('raw_products.csv')\n\n# PREPARE 1: Clean customers\ncustomers['email'] = customers['email'].str.lower().str.strip()\ncustomers['name'] = customers['name'].str.strip().str.title()\ncustomers = customers.dropna(subset=['customer_id', 'email'])\ncustomers = customers.drop_duplicates(subset=['email'])\n\n# JOIN 1: Orders with customers\norders_enriched = pd.merge(\n    orders,\n    customers[['customer_id', 'name', 'email', 'segment']],\n    on='customer_id',\n    how='inner'\n)\n\n# JOIN 2: With products\norders_full = pd.merge(\n    orders_enriched,\n    products[['product_id', 'product_name', 'category', 'unit_cost']],\n    on='product_id',\n    how='left'\n)\n\n# PREPARE 2: Calculate metrics\norders_full['order_date'] = pd.to_datetime(orders_full['order_date'])\norders_full['margin'] = orders_full['amount'] - orders_full['unit_cost'] * orders_full['quantity']\norders_full['margin_pct'] = (orders_full['margin'] / orders_full['amount'] * 100).round(2)\n\n# GROUPING: Customer-level aggregation\ncustomer_metrics = orders_full.groupby(['customer_id', 'name', 'segment']).agg({\n    'order_id': 'count',\n    'amount': 'sum',\n    'margin': 'sum',\n    'order_date': ['min', 'max']\n}).reset_index()\ncustomer_metrics.columns = ['customer_id', 'name', 'segment', 'order_count',\n                             'total_revenue', 'total_margin', 'first_order', 'last_order']\n\n# WINDOW: Add customer rankings\ncustomer_metrics = customer_metrics.sort_values('total_revenue', ascending=False)\ncustomer_metrics['revenue_rank'] = range(1, len(customer_metrics) + 1)\ncustomer_metrics['revenue_percentile'] = (\n    customer_metrics['revenue_rank'] / len(customer_metrics) * 100\n).round(1)\n\n# PREPARE 3: Final formatting\ncustomer_metrics['clv_tier'] = pd.cut(\n    customer_metrics['total_revenue'],\n    bins=[0, 100, 500, 2000, float('inf')],\n    labels=['Low', 'Medium', 'High', 'VIP']\n)\n\ncustomer_metrics.to_csv('customer_analytics.csv', index=False)\n] PASSED [  4%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_produces_flow[text_pipeline-\nimport pandas as pd\n\ndf = pd.read_csv('raw_text_data.csv')\n\n# STRING_TRANSFORMER: Clean text\ndf['name'] = df['name'].str.strip().str.title()\ndf['email'] = df['email'].str.lower().str.strip()\ndf['code'] = df['code'].str.upper()\n\n# FILL_EMPTY: Handle missing values\ndf['name'] = df['name'].fillna('Unknown')\ndf['email'] = df['email'].fillna('')\ndf['code'] = df['code'].fillna('N/A')\n\n# TYPE_SETTER: Ensure correct types\ndf['customer_id'] = df['customer_id'].astype(str)\ndf['is_active'] = df['is_active'].astype(bool)\n\ndf.to_csv('cleaned_text_data.csv', index=False)\n] PASSED [  5%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_produces_flow[date_pipeline-\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\ndf = pd.read_csv('events.csv')\n\n# DATE_PARSER: Parse date strings\ndf['event_date'] = pd.to_datetime(df['event_date_str'])\ndf['created_at'] = pd.to_datetime(df['created_at_str'])\n\n# FORMULA: Calculate date-based metrics\ndf['days_since_event'] = (datetime.now() - df['event_date']).dt.days\ndf['processing_time'] = (df['event_date'] - df['created_at']).dt.total_seconds() / 3600\ndf['event_year'] = df['event_date'].dt.year\ndf['event_month'] = df['event_date'].dt.month\ndf['is_recent'] = (df['days_since_event'] <= 30).astype(int)\n\n# FILTER_ON_DATE_RANGE: Keep only last 365 days\ncutoff_date = datetime.now() - timedelta(days=365)\ndf = df[df['event_date'] >= cutoff_date]\n\ndf.to_csv('processed_events.csv', index=False)\n] PASSED [  5%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_produces_flow[column_pipeline-\nimport pandas as pd\n\ndf = pd.read_csv('raw_export.csv')\n\n# COLUMN_RENAMER: Standardize column names\ndf = df.rename(columns={\n    'cust_id': 'customer_id',\n    'prod_nm': 'product_name',\n    'qty': 'quantity',\n    'amt': 'amount',\n    'dt': 'date'\n})\n\n# COLUMN_DELETER: Remove unnecessary columns\ncolumns_to_drop = ['temp_col', 'debug_info', 'internal_id', '_row_num']\ndf = df.drop(columns=[c for c in columns_to_drop if c in df.columns])\n\n# COLUMNS_SELECTOR: Select final columns in order\nfinal_columns = ['customer_id', 'product_name', 'quantity', 'amount', 'date']\ndf = df[[c for c in final_columns if c in df.columns]]\n\ndf.to_csv('cleaned_export.csv', index=False)\n] PASSED [  5%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_produces_flow[numeric_pipeline-\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('raw_metrics.csv')\n\n# NUMERICAL_TRANSFORMER: Mathematical transformations\ndf['log_value'] = np.log1p(df['value'])\ndf['normalized'] = (df['value'] - df['value'].mean()) / df['value'].std()\ndf['pct_of_max'] = df['value'] / df['value'].max() * 100\n\n# ROUND: Round to appropriate precision\ndf['value_rounded'] = df['value'].round(2)\ndf['pct_of_max'] = df['pct_of_max'].round(1)\n\n# CLIP: Constrain to valid range\ndf['score'] = df['score'].clip(lower=0, upper=100)\ndf['normalized'] = df['normalized'].clip(lower=-3, upper=3)\n\n# BINNER: Create categories\ndf['value_bucket'] = pd.cut(\n    df['value_rounded'],\n    bins=[0, 10, 50, 100, 500, float('inf')],\n    labels=['XS', 'S', 'M', 'L', 'XL']\n)\ndf['score_quartile'] = pd.qcut(df['score'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n\ndf.to_csv('processed_metrics.csv', index=False)\n] PASSED [  5%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_produces_flow[text_extraction_pipeline-\nimport pandas as pd\n\ndf = pd.read_csv('raw_contacts.csv')\n\n# REGEXP_EXTRACTOR: Extract patterns\ndf['area_code'] = df['phone'].str.extract(r'\\((\\d{3})\\)')\ndf['email_domain'] = df['email'].str.extract(r'@(.+)$')\ndf['zip_code'] = df['address'].str.extract(r'(\\d{5})(?:-\\d{4})?$')\n\n# SPLIT_COLUMN: Split into parts\nname_parts = df['full_name'].str.split(' ', n=1, expand=True)\ndf['first_name'] = name_parts[0]\ndf['last_name'] = name_parts[1]\n\naddress_parts = df['address'].str.split(',', expand=True)\ndf['street'] = address_parts[0]\ndf['city_state'] = address_parts[1]\n\n# CONCAT_COLUMNS: Combine fields\ndf['display_name'] = df['first_name'] + ' ' + df['last_name'].str[0] + '.'\ndf['contact_info'] = df['email'] + ' | ' + df['phone']\n\ndf.to_csv('parsed_contacts.csv', index=False)\n] PASSED [  5%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_produces_flow[cleaning_pipeline-\nimport pandas as pd\n\ndf = pd.read_csv('dirty_data.csv')\n\n# FILL_EMPTY: Fill non-critical fields\ndf['category'] = df['category'].fillna('Other')\ndf['score'] = df['score'].fillna(df['score'].median())\ndf['notes'] = df['notes'].fillna('')\n\n# REMOVE_ROWS_ON_EMPTY: Remove rows with critical missing values\ndf = df.dropna(subset=['customer_id', 'email', 'amount'])\n\n# REMOVE_DUPLICATES: Deduplicate\ndf = df.drop_duplicates(subset=['customer_id', 'transaction_id'], keep='first')\ndf = df.drop_duplicates(subset=['email'], keep='last')\n\ndf.to_csv('clean_data.csv', index=False)\n] PASSED [  5%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_produces_flow[flagging_pipeline-\nimport pandas as pd\n\ndf = pd.read_csv('transactions.csv')\n\n# FILTER_ON_VALUE: Keep valid transactions\ndf = df[df['status'] != 'cancelled']\ndf = df[df['amount'] > 0]\n\n# FLAG_ON_VALUE: Create flag columns\ndf['is_premium'] = (df['customer_tier'] == 'premium').astype(int)\ndf['is_high_value'] = (df['amount'] >= 1000).astype(int)\ndf['needs_review'] = (df['risk_score'] >= 0.8).astype(int)\n\n# CREATE_COLUMN_WITH_GREL: Complex computed columns\ndf['priority'] = df.apply(\n    lambda row: 'Critical' if row['is_high_value'] and row['needs_review']\n    else 'High' if row['is_high_value'] or row['needs_review']\n    else 'Normal',\n    axis=1\n)\n\ndf['display_label'] = df.apply(\n    lambda row: f"{row['transaction_id']} - {row['customer_name']} (${row['amount']:,.2f})",\n    axis=1\n)\n\ndf.to_csv('flagged_transactions.csv', index=False)\n] PASSED [  5%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_produces_flow[ml_prep_pipeline-\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('raw_features.csv')\n\n# TYPE_SETTER: Ensure correct types\ndf['age'] = df['age'].astype(float)\ndf['income'] = df['income'].astype(float)\ndf['category'] = df['category'].astype(str)\n\n# NORMALIZER: Normalize numeric features\nnumeric_cols = ['age', 'income', 'score']\nfor col in numeric_cols:\n    df[f'{col}_normalized'] = (df[col] - df[col].mean()) / df[col].std()\n    df[f'{col}_minmax'] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())\n\n# CATEGORICAL_ENCODER: Encode categorical features\ndf_encoded = pd.get_dummies(df, columns=['category', 'region'], prefix=['cat', 'reg'])\n\ndf_encoded.to_csv('ml_features.csv', index=False)\n] PASSED [  5%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_produces_flow[all_filter_processors-\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\ndf = pd.read_csv('data.csv')\ndf['date'] = pd.to_datetime(df['date'])\n\n# FILTER_ON_VALUE: Exact value filter\ndf = df[df['status'] == 'active']\n\n# FILTER_ON_FORMULA: Complex condition\ndf = df[(df['amount'] > 100) | (df['priority'] == 'high')]\n\n# FILTER_ON_DATE_RANGE: Date filter\ncutoff = datetime.now() - timedelta(days=90)\ndf = df[df['date'] >= cutoff]\n\n# FILTER_ON_NUMERIC_RANGE: Numeric filter\ndf = df[df['score'].between(0, 100)]\n\n# FILTER_ON_BAD_TYPE: Type validation\ndf['amount_valid'] = pd.to_numeric(df['amount'], errors='coerce')\ndf = df[df['amount_valid'].notna()]\n\ndf.to_csv('filtered_data.csv', index=False)\n] PASSED [  6%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_produces_flow[all_flag_processors-\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\ndf = pd.read_csv('data.csv')\ndf['date'] = pd.to_datetime(df['date'])\n\n# FLAG_ON_VALUE\ndf['is_premium'] = (df['tier'] == 'premium').astype(int)\n\n# FLAG_ON_FORMULA\ndf['high_value_recent'] = (\n    (df['amount'] >= 1000) &\n    (df['date'] >= datetime.now() - timedelta(days=30))\n).astype(int)\n\n# FLAG_ON_BAD_TYPE\ndf['has_valid_email'] = df['email'].str.contains('@', na=False).astype(int)\n\n# FLAG_ON_DATE_RANGE\ndf['is_q1'] = df['date'].dt.quarter.eq(1).astype(int)\n\n# FLAG_ON_NUMERIC_RANGE\ndf['score_in_range'] = df['score'].between(50, 100).astype(int)\n\ndf.to_csv('flagged_data.csv', index=False)\n] PASSED [  6%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExamplesConversion::test_combination_produces_flow[all_missing_value_processors-\nimport pandas as pd\n\ndf = pd.read_csv('data_with_nulls.csv')\n\n# FILL_EMPTY_WITH_VALUE: Constant fill\ndf['category'] = df['category'].fillna('Unknown')\n\n# FILL_EMPTY_WITH_COMPUTED_VALUE: Computed fill\ndf['amount'] = df['amount'].fillna(df['amount'].mean())\ndf['score'] = df.groupby('category')['score'].transform(lambda x: x.fillna(x.median()))\n\n# FILL_EMPTY_WITH_PREVIOUS_NEXT: Forward/backward fill\ndf = df.sort_values('date')\ndf['metric'] = df['metric'].ffill()\ndf['metric'] = df['metric'].bfill()\n\n# REMOVE_ROWS_ON_EMPTY: Drop rows\ndf = df.dropna(subset=['customer_id', 'email'])\n\ndf.to_csv('no_missing_data.csv', index=False)\n] PASSED [  6%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[join_inner-\nimport pandas as pd\n\nleft = pd.read_csv('customers.csv')\nright = pd.read_csv('orders.csv')\n\n# INNER join - only matching rows from both tables\nresult = pd.merge(left, right, on='customer_id', how='inner')\n\nresult.to_csv('inner_join.csv', index=False)\n] PASSED [  6%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[join_left-\nimport pandas as pd\n\nleft = pd.read_csv('customers.csv')\nright = pd.read_csv('orders.csv')\n\n# LEFT join - all rows from left, matching from right\nresult = pd.merge(left, right, on='customer_id', how='left')\n\nresult.to_csv('left_join.csv', index=False)\n] PASSED [  6%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[join_right-\nimport pandas as pd\n\nleft = pd.read_csv('customers.csv')\nright = pd.read_csv('orders.csv')\n\n# RIGHT join - all rows from right, matching from left\nresult = pd.merge(left, right, on='customer_id', how='right')\n\nresult.to_csv('right_join.csv', index=False)\n] PASSED [  6%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[join_outer-\nimport pandas as pd\n\nleft = pd.read_csv('customers.csv')\nright = pd.read_csv('orders.csv')\n\n# OUTER/FULL join - all rows from both tables\nresult = pd.merge(left, right, on='customer_id', how='outer')\n\nresult.to_csv('outer_join.csv', index=False)\n] PASSED [  6%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[join_cross-\nimport pandas as pd\n\nproducts = pd.read_csv('products.csv')\nregions = pd.read_csv('regions.csv')\n\n# CROSS join - cartesian product\nresult = pd.merge(products, regions, how='cross')\n\nresult.to_csv('cross_join.csv', index=False)\n] PASSED [  6%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[join_left_anti-\nimport pandas as pd\n\nall_customers = pd.read_csv('customers.csv')\nactive_orders = pd.read_csv('orders.csv')\n\n# LEFT ANTI join - rows in left that don't match right\nmerged = pd.merge(all_customers, active_orders, on='customer_id', how='left', indicator=True)\nresult = merged[merged['_merge'] == 'left_only'].drop('_merge', axis=1)\n\nresult.to_csv('left_anti_join.csv', index=False)\n] PASSED [  6%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[agg_sum-\nimport pandas as pd\n\ndf = pd.read_csv('sales.csv')\n\n# SUM aggregation\nresult = df.groupby('category').agg({'amount': 'sum'}).reset_index()\nresult.columns = ['category', 'total_amount']\n\nresult.to_csv('sum_agg.csv', index=False)\n] PASSED [  7%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[agg_avg-\nimport pandas as pd\n\ndf = pd.read_csv('sales.csv')\n\n# AVG/MEAN aggregation\nresult = df.groupby('category').agg({'amount': 'mean'}).reset_index()\nresult.columns = ['category', 'avg_amount']\n\nresult.to_csv('avg_agg.csv', index=False)\n] PASSED [  7%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[agg_count-\nimport pandas as pd\n\ndf = pd.read_csv('sales.csv')\n\n# COUNT aggregation\nresult = df.groupby('category').agg({'transaction_id': 'count'}).reset_index()\nresult.columns = ['category', 'transaction_count']\n\nresult.to_csv('count_agg.csv', index=False)\n] PASSED [  7%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[agg_min-\nimport pandas as pd\n\ndf = pd.read_csv('sales.csv')\n\n# MIN aggregation\nresult = df.groupby('category').agg({'amount': 'min'}).reset_index()\nresult.columns = ['category', 'min_amount']\n\nresult.to_csv('min_agg.csv', index=False)\n] PASSED [  7%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[agg_max-\nimport pandas as pd\n\ndf = pd.read_csv('sales.csv')\n\n# MAX aggregation\nresult = df.groupby('category').agg({'amount': 'max'}).reset_index()\nresult.columns = ['category', 'max_amount']\n\nresult.to_csv('max_agg.csv', index=False)\n] PASSED [  7%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[agg_first-\nimport pandas as pd\n\ndf = pd.read_csv('sales.csv')\n\n# FIRST aggregation\nresult = df.groupby('category').agg({'date': 'first'}).reset_index()\nresult.columns = ['category', 'first_date']\n\nresult.to_csv('first_agg.csv', index=False)\n] PASSED [  7%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[agg_last-\nimport pandas as pd\n\ndf = pd.read_csv('sales.csv')\n\n# LAST aggregation\nresult = df.groupby('category').agg({'date': 'last'}).reset_index()\nresult.columns = ['category', 'last_date']\n\nresult.to_csv('last_agg.csv', index=False)\n] PASSED [  7%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[agg_std-\nimport pandas as pd\n\ndf = pd.read_csv('sales.csv')\n\n# STD (standard deviation) aggregation\nresult = df.groupby('category').agg({'amount': 'std'}).reset_index()\nresult.columns = ['category', 'amount_std']\n\nresult.to_csv('std_agg.csv', index=False)\n] PASSED [  7%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[agg_var-\nimport pandas as pd\n\ndf = pd.read_csv('sales.csv')\n\n# VAR (variance) aggregation\nresult = df.groupby('category').agg({'amount': 'var'}).reset_index()\nresult.columns = ['category', 'amount_var']\n\nresult.to_csv('var_agg.csv', index=False)\n] PASSED [  8%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[agg_nunique-\nimport pandas as pd\n\ndf = pd.read_csv('sales.csv')\n\n# NUNIQUE (count unique) aggregation\nresult = df.groupby('category').agg({'customer_id': 'nunique'}).reset_index()\nresult.columns = ['category', 'unique_customers']\n\nresult.to_csv('nunique_agg.csv', index=False)\n] PASSED [  8%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[agg_median-\nimport pandas as pd\n\ndf = pd.read_csv('sales.csv')\n\n# MEDIAN aggregation\nresult = df.groupby('category').agg({'amount': 'median'}).reset_index()\nresult.columns = ['category', 'median_amount']\n\nresult.to_csv('median_agg.csv', index=False)\n] PASSED [  8%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[agg_mode-\nimport pandas as pd\n\ndf = pd.read_csv('sales.csv')\n\n# MODE aggregation\nresult = df.groupby('category')['product'].agg(\n    lambda x: x.mode()[0] if len(x.mode()) > 0 else None\n).reset_index()\nresult.columns = ['category', 'most_common_product']\n\nresult.to_csv('mode_agg.csv', index=False)\n] PASSED [  8%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[agg_percentile-\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('sales.csv')\n\n# PERCENTILE aggregations\nresult = df.groupby('category').agg({\n    'amount': [\n        lambda x: np.percentile(x, 25),  # P25\n        lambda x: np.percentile(x, 50),  # P50 (median)\n        lambda x: np.percentile(x, 75),  # P75\n        lambda x: np.percentile(x, 90),  # P90\n        lambda x: np.percentile(x, 95),  # P95\n        lambda x: np.percentile(x, 99),  # P99\n    ]\n}).reset_index()\nresult.columns = ['category', 'p25', 'p50', 'p75', 'p90', 'p95', 'p99']\n\nresult.to_csv('percentile_agg.csv', index=False)\n] PASSED [  8%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[agg_mixed-\nimport pandas as pd\n\ndf = pd.read_csv('sales.csv')\n\n# Mixed aggregations on single groupby\nresult = df.groupby('category').agg({\n    'amount': ['sum', 'mean', 'std', 'min', 'max'],\n    'quantity': ['sum', 'mean'],\n    'customer_id': 'nunique',\n    'transaction_id': 'count'\n}).reset_index()\n\n# Flatten column names\nresult.columns = ['_'.join(col).strip('_') for col in result.columns]\n\nresult.to_csv('mixed_agg.csv', index=False)\n] PASSED [  8%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[string_upper-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# TO_UPPER mode\ndf['name_upper'] = df['name'].str.upper()\n\ndf.to_csv('upper.csv', index=False)\n] PASSED [  8%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[string_lower-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# TO_LOWER mode\ndf['email_lower'] = df['email'].str.lower()\n\ndf.to_csv('lower.csv', index=False)\n] PASSED [  8%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[string_title-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# TITLECASE mode\ndf['name_title'] = df['name'].str.title()\n\ndf.to_csv('title.csv', index=False)\n] PASSED [  9%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[string_capitalize-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# CAPITALIZE mode (first letter uppercase)\ndf['sentence_cap'] = df['sentence'].str.capitalize()\n\ndf.to_csv('capitalize.csv', index=False)\n] PASSED [  9%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[string_swapcase-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# SWAPCASE mode\ndf['text_swapped'] = df['text'].str.swapcase()\n\ndf.to_csv('swapcase.csv', index=False)\n] PASSED [  9%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[string_trim-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# TRIM mode (both sides)\ndf['name_trimmed'] = df['name'].str.strip()\n\ndf.to_csv('trim.csv', index=False)\n] PASSED [  9%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[string_ltrim-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# TRIM_LEFT mode\ndf['text_ltrim'] = df['text'].str.lstrip()\n\ndf.to_csv('ltrim.csv', index=False)\n] PASSED [  9%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[string_rtrim-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# TRIM_RIGHT mode\ndf['text_rtrim'] = df['text'].str.rstrip()\n\ndf.to_csv('rtrim.csv', index=False)\n] PASSED [  9%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[string_normalize_ws-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# NORMALIZE_WHITESPACE mode (multiple spaces to single)\ndf['text_normalized'] = df['text'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n\ndf.to_csv('normalize_whitespace.csv', index=False)\n] PASSED [  9%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[string_remove_ws-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# REMOVE_WHITESPACE mode\ndf['code_no_spaces'] = df['code'].str.replace(' ', '')\n\ndf.to_csv('remove_whitespace.csv', index=False)\n] PASSED [  9%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[string_remove_accents-\nimport pandas as pd\nimport unicodedata\n\ndf = pd.read_csv('data.csv')\n\n# REMOVE_ACCENTS mode\ndef remove_accents(text):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', str(text))\n        if unicodedata.category(c) != 'Mn'\n    )\n\ndf['name_no_accents'] = df['name'].apply(remove_accents)\n\ndf.to_csv('remove_accents.csv', index=False)\n] PASSED [  9%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[string_all_modes-\nimport pandas as pd\nimport unicodedata\n\ndf = pd.read_csv('data.csv')\n\n# Apply all string transformer modes to demonstrate each\ndf['mode_upper'] = df['text'].str.upper()\ndf['mode_lower'] = df['text'].str.lower()\ndf['mode_title'] = df['text'].str.title()\ndf['mode_capitalize'] = df['text'].str.capitalize()\ndf['mode_swapcase'] = df['text'].str.swapcase()\ndf['mode_trim'] = df['text'].str.strip()\ndf['mode_ltrim'] = df['text'].str.lstrip()\ndf['mode_rtrim'] = df['text'].str.rstrip()\ndf['mode_normalize_ws'] = df['text'].str.replace(r'\\s+', ' ', regex=True)\ndf['mode_remove_ws'] = df['text'].str.replace(' ', '')\n\ndf.to_csv('all_string_modes.csv', index=False)\n] PASSED [ 10%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[num_multiply-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# MULTIPLY mode\ndf['amount_doubled'] = df['amount'] * 2\ndf['cents'] = df['dollars'] * 100\n\ndf.to_csv('multiply.csv', index=False)\n] PASSED [ 10%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[num_divide-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# DIVIDE mode\ndf['amount_halved'] = df['amount'] / 2\ndf['thousands'] = df['value'] / 1000\n\ndf.to_csv('divide.csv', index=False)\n] PASSED [ 10%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[num_add-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# ADD mode\ndf['score_plus_10'] = df['score'] + 10\ndf['adjusted'] = df['value'] + 100\n\ndf.to_csv('add.csv', index=False)\n] PASSED [ 10%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[num_subtract-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# SUBTRACT mode\ndf['net'] = df['gross'] - df['deductions']\ndf['adjusted'] = df['value'] - 50\n\ndf.to_csv('subtract.csv', index=False)\n] PASSED [ 10%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[num_power-\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('data.csv')\n\n# POWER mode\ndf['squared'] = df['value'] ** 2\ndf['cubed'] = df['value'] ** 3\ndf['sqrt'] = df['value'] ** 0.5\n\ndf.to_csv('power.csv', index=False)\n] PASSED [ 10%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[num_log-\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('data.csv')\n\n# LOG mode variants\ndf['log_natural'] = np.log(df['value'])\ndf['log10'] = np.log10(df['value'])\ndf['log2'] = np.log2(df['value'])\ndf['log1p'] = np.log1p(df['value'])\n\ndf.to_csv('log.csv', index=False)\n] PASSED [ 10%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[num_exp-\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('data.csv')\n\n# EXP mode\ndf['exp_value'] = np.exp(df['value'])\ndf['expm1'] = np.expm1(df['value'])\n\ndf.to_csv('exp.csv', index=False)\n] PASSED [ 10%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[num_abs-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# ABS mode\ndf['abs_change'] = df['change'].abs()\ndf['abs_delta'] = df['delta'].abs()\n\ndf.to_csv('abs.csv', index=False)\n] PASSED [ 11%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[num_round-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# ROUND mode\ndf['rounded_2'] = df['amount'].round(2)\ndf['rounded_0'] = df['amount'].round(0)\ndf['rounded_neg1'] = df['amount'].round(-1)  # To nearest 10\n\ndf.to_csv('round.csv', index=False)\n] PASSED [ 11%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[num_floor-\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('data.csv')\n\n# FLOOR mode\ndf['floored'] = np.floor(df['value'])\n\ndf.to_csv('floor.csv', index=False)\n] PASSED [ 11%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[num_ceil-\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('data.csv')\n\n# CEIL mode\ndf['ceiling'] = np.ceil(df['value'])\n\ndf.to_csv('ceil.csv', index=False)\n] PASSED [ 11%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[num_truncate-\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('data.csv')\n\n# TRUNCATE mode\ndf['truncated'] = np.trunc(df['value'])\n\ndf.to_csv('truncate.csv', index=False)\n] PASSED [ 11%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[num_sin-\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('data.csv')\n\n# SIN mode (trigonometric)\ndf['sin_value'] = np.sin(df['angle_radians'])\n\ndf.to_csv('sin.csv', index=False)\n] PASSED [ 11%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[num_cos-\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('data.csv')\n\n# COS mode (trigonometric)\ndf['cos_value'] = np.cos(df['angle_radians'])\n\ndf.to_csv('cos.csv', index=False)\n] PASSED [ 11%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[num_all_modes-\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('data.csv')\n\n# Apply all numerical transformer modes\ndf['mode_multiply'] = df['value'] * 2\ndf['mode_divide'] = df['value'] / 2\ndf['mode_add'] = df['value'] + 10\ndf['mode_subtract'] = df['value'] - 10\ndf['mode_power'] = df['value'] ** 2\ndf['mode_sqrt'] = np.sqrt(df['value'].abs())\ndf['mode_log'] = np.log(df['value'].clip(lower=0.001))\ndf['mode_exp'] = np.exp(df['value'].clip(upper=10))\ndf['mode_abs'] = df['value'].abs()\ndf['mode_round'] = df['value'].round(2)\ndf['mode_floor'] = np.floor(df['value'])\ndf['mode_ceil'] = np.ceil(df['value'])\n\ndf.to_csv('all_numerical_modes.csv', index=False)\n] PASSED [ 11%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[window_row_number-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\ndf = df.sort_values(['category', 'date'])\n\n# ROW_NUMBER\ndf['row_num'] = df.groupby('category').cumcount() + 1\n\ndf.to_csv('row_number.csv', index=False)\n] PASSED [ 11%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[window_rank-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# RANK\ndf['rank'] = df.groupby('category')['score'].rank(method='min')\n\ndf.to_csv('rank.csv', index=False)\n] PASSED [ 12%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[window_dense_rank-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# DENSE_RANK\ndf['dense_rank'] = df.groupby('category')['score'].rank(method='dense')\n\ndf.to_csv('dense_rank.csv', index=False)\n] PASSED [ 12%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[window_lag-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\ndf = df.sort_values(['category', 'date'])\n\n# LAG\ndf['prev_value'] = df.groupby('category')['value'].shift(1)\ndf['prev_2_value'] = df.groupby('category')['value'].shift(2)\n\ndf.to_csv('lag.csv', index=False)\n] PASSED [ 12%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[window_lead-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\ndf = df.sort_values(['category', 'date'])\n\n# LEAD\ndf['next_value'] = df.groupby('category')['value'].shift(-1)\ndf['next_2_value'] = df.groupby('category')['value'].shift(-2)\n\ndf.to_csv('lead.csv', index=False)\n] PASSED [ 12%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[window_running_sum-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\ndf = df.sort_values(['category', 'date'])\n\n# RUNNING_SUM\ndf['running_sum'] = df.groupby('category')['value'].cumsum()\n\ndf.to_csv('running_sum.csv', index=False)\n] PASSED [ 12%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[window_running_avg-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\ndf = df.sort_values(['category', 'date'])\n\n# RUNNING_AVG\ndf['running_avg'] = df.groupby('category')['value'].expanding().mean().reset_index(level=0, drop=True)\n\ndf.to_csv('running_avg.csv', index=False)\n] PASSED [ 12%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[window_moving_avg-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\ndf = df.sort_values(['category', 'date'])\n\n# MOVING_AVG (rolling)\ndf['moving_avg_7'] = df.groupby('category')['value'].transform(\n    lambda x: x.rolling(window=7, min_periods=1).mean()\n)\n\ndf.to_csv('moving_avg.csv', index=False)\n] PASSED [ 12%]
tests/test_py2dataiku/test_combination_examples.py::TestSettingsExamplesConversion::test_settings_converts_successfully[window_moving_sum-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\ndf = df.sort_values(['category', 'date'])\n\n# MOVING_SUM (rolling)\ndf['moving_sum_7'] = df.groupby('category')['value'].transform(\n    lambda x: x.rolling(window=7, min_periods=1).sum()\n)\n\ndf.to_csv('moving_sum.csv', index=False)\n] PASSED [ 12%]
tests/test_py2dataiku/test_combination_examples.py::TestRecipeCombinations::test_prepare_grouping_prepare_has_all_operations PASSED [ 13%]
tests/test_py2dataiku/test_combination_examples.py::TestRecipeCombinations::test_join_window_split_has_all_operations PASSED [ 13%]
tests/test_py2dataiku/test_combination_examples.py::TestRecipeCombinations::test_stack_distinct_sort_has_all_operations PASSED [ 13%]
tests/test_py2dataiku/test_combination_examples.py::TestRecipeCombinations::test_grouping_pivot_prepare_has_all_operations PASSED [ 13%]
tests/test_py2dataiku/test_combination_examples.py::TestRecipeCombinations::test_split_join_grouping_has_all_operations PASSED [ 13%]
tests/test_py2dataiku/test_combination_examples.py::TestRecipeCombinations::test_multi_join_grouping_has_multiple_joins PASSED [ 13%]
tests/test_py2dataiku/test_combination_examples.py::TestRecipeCombinations::test_full_etl_pipeline_is_comprehensive PASSED [ 13%]
tests/test_py2dataiku/test_combination_examples.py::TestProcessorCombinations::test_text_pipeline_has_string_operations PASSED [ 13%]
tests/test_py2dataiku/test_combination_examples.py::TestProcessorCombinations::test_date_pipeline_has_date_operations PASSED [ 13%]
tests/test_py2dataiku/test_combination_examples.py::TestProcessorCombinations::test_column_pipeline_has_column_operations PASSED [ 14%]
tests/test_py2dataiku/test_combination_examples.py::TestProcessorCombinations::test_numeric_pipeline_has_numeric_operations PASSED [ 14%]
tests/test_py2dataiku/test_combination_examples.py::TestProcessorCombinations::test_cleaning_pipeline_has_cleaning_operations PASSED [ 14%]
tests/test_py2dataiku/test_combination_examples.py::TestProcessorCombinations::test_flagging_pipeline_has_flag_operations PASSED [ 14%]
tests/test_py2dataiku/test_combination_examples.py::TestProcessorCombinations::test_ml_prep_pipeline_has_ml_operations PASSED [ 14%]
tests/test_py2dataiku/test_combination_examples.py::TestAllProcessorTypes::test_all_filter_processors PASSED [ 14%]
tests/test_py2dataiku/test_combination_examples.py::TestAllProcessorTypes::test_all_flag_processors PASSED [ 14%]
tests/test_py2dataiku/test_combination_examples.py::TestAllProcessorTypes::test_all_missing_value_processors PASSED [ 14%]
tests/test_py2dataiku/test_combination_examples.py::TestJoinTypeSettings::test_inner_join_settings PASSED [ 15%]
tests/test_py2dataiku/test_combination_examples.py::TestJoinTypeSettings::test_left_join_settings PASSED [ 15%]
tests/test_py2dataiku/test_combination_examples.py::TestJoinTypeSettings::test_right_join_settings PASSED [ 15%]
tests/test_py2dataiku/test_combination_examples.py::TestJoinTypeSettings::test_outer_join_settings PASSED [ 15%]
tests/test_py2dataiku/test_combination_examples.py::TestJoinTypeSettings::test_cross_join_settings PASSED [ 15%]
tests/test_py2dataiku/test_combination_examples.py::TestAggregationSettings::test_sum_aggregation PASSED [ 15%]
tests/test_py2dataiku/test_combination_examples.py::TestAggregationSettings::test_avg_aggregation PASSED [ 15%]
tests/test_py2dataiku/test_combination_examples.py::TestAggregationSettings::test_count_aggregation PASSED [ 15%]
tests/test_py2dataiku/test_combination_examples.py::TestAggregationSettings::test_min_aggregation PASSED [ 16%]
tests/test_py2dataiku/test_combination_examples.py::TestAggregationSettings::test_max_aggregation PASSED [ 16%]
tests/test_py2dataiku/test_combination_examples.py::TestAggregationSettings::test_mixed_aggregation PASSED [ 16%]
tests/test_py2dataiku/test_combination_examples.py::TestStringModeSettings::test_upper_mode PASSED [ 16%]
tests/test_py2dataiku/test_combination_examples.py::TestStringModeSettings::test_lower_mode PASSED [ 16%]
tests/test_py2dataiku/test_combination_examples.py::TestStringModeSettings::test_title_mode PASSED [ 16%]
tests/test_py2dataiku/test_combination_examples.py::TestStringModeSettings::test_trim_mode PASSED [ 16%]
tests/test_py2dataiku/test_combination_examples.py::TestStringModeSettings::test_all_string_modes PASSED [ 16%]
tests/test_py2dataiku/test_combination_examples.py::TestNumericalModeSettings::test_multiply_mode PASSED [ 16%]
tests/test_py2dataiku/test_combination_examples.py::TestNumericalModeSettings::test_divide_mode PASSED [ 17%]
tests/test_py2dataiku/test_combination_examples.py::TestNumericalModeSettings::test_add_mode PASSED [ 17%]
tests/test_py2dataiku/test_combination_examples.py::TestNumericalModeSettings::test_log_mode PASSED [ 17%]
tests/test_py2dataiku/test_combination_examples.py::TestNumericalModeSettings::test_all_numerical_modes PASSED [ 17%]
tests/test_py2dataiku/test_combination_examples.py::TestWindowFunctionSettings::test_row_number_window PASSED [ 17%]
tests/test_py2dataiku/test_combination_examples.py::TestWindowFunctionSettings::test_rank_window PASSED [ 17%]
tests/test_py2dataiku/test_combination_examples.py::TestWindowFunctionSettings::test_lag_window PASSED [ 17%]
tests/test_py2dataiku/test_combination_examples.py::TestWindowFunctionSettings::test_lead_window PASSED [ 17%]
tests/test_py2dataiku/test_combination_examples.py::TestWindowFunctionSettings::test_moving_avg_window PASSED [ 18%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationVisualization::test_svg_visualization[prepare_grouping_prepare] PASSED [ 18%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationVisualization::test_svg_visualization[join_window_split] PASSED [ 18%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationVisualization::test_svg_visualization[stack_distinct_sort] PASSED [ 18%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationVisualization::test_svg_visualization[grouping_pivot_prepare] PASSED [ 18%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationVisualization::test_svg_visualization[split_join_grouping] PASSED [ 18%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationVisualization::test_mermaid_visualization[prepare_grouping_prepare] PASSED [ 18%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationVisualization::test_mermaid_visualization[join_window_split] PASSED [ 18%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationVisualization::test_mermaid_visualization[stack_distinct_sort] PASSED [ 18%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationVisualization::test_mermaid_visualization[grouping_pivot_prepare] PASSED [ 19%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationVisualization::test_mermaid_visualization[split_join_grouping] PASSED [ 19%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationVisualization::test_ascii_visualization[prepare_grouping_prepare] PASSED [ 19%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationVisualization::test_ascii_visualization[join_window_split] PASSED [ 19%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationVisualization::test_ascii_visualization[stack_distinct_sort] PASSED [ 19%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationVisualization::test_ascii_visualization[grouping_pivot_prepare] PASSED [ 19%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationVisualization::test_ascii_visualization[split_join_grouping] PASSED [ 19%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExport::test_to_dict_export[prepare_grouping_prepare] PASSED [ 19%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExport::test_to_dict_export[join_window_split] PASSED [ 20%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExport::test_to_dict_export[stack_distinct_sort] PASSED [ 20%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExport::test_to_dict_export[grouping_pivot_prepare] PASSED [ 20%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExport::test_to_dict_export[split_join_grouping] PASSED [ 20%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExport::test_to_json_export[prepare_grouping_prepare] PASSED [ 20%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExport::test_to_json_export[join_window_split] PASSED [ 20%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExport::test_to_json_export[stack_distinct_sort] PASSED [ 20%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExport::test_to_json_export[grouping_pivot_prepare] PASSED [ 20%]
tests/test_py2dataiku/test_combination_examples.py::TestCombinationExport::test_to_json_export[split_join_grouping] PASSED [ 20%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineBasics::test_all_complex_examples_exist PASSED [ 21%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineBasics::test_all_examples_have_metadata PASSED [ 21%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineBasics::test_get_complex_example_returns_code PASSED [ 21%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineBasics::test_get_nonexistent_example_returns_empty PASSED [ 21%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineBasics::test_all_examples_are_nonempty_strings PASSED [ 21%]
tests/test_py2dataiku/test_complex_pipelines.py::TestFraudDetectionPipeline::test_conversion_succeeds PASSED [ 21%]
tests/test_py2dataiku/test_complex_pipelines.py::TestFraudDetectionPipeline::test_detects_multiple_data_sources PASSED [ 21%]
tests/test_py2dataiku/test_complex_pipelines.py::TestFraudDetectionPipeline::test_produces_mermaid_output PASSED [ 21%]
tests/test_py2dataiku/test_complex_pipelines.py::TestFraudDetectionPipeline::test_produces_svg_output PASSED [ 22%]
tests/test_py2dataiku/test_complex_pipelines.py::TestFraudDetectionPipeline::test_flow_to_dict_exists PASSED [ 22%]
tests/test_py2dataiku/test_complex_pipelines.py::TestCustomer360Pipeline::test_conversion_succeeds PASSED [ 22%]
tests/test_py2dataiku/test_complex_pipelines.py::TestCustomer360Pipeline::test_detects_many_data_sources PASSED [ 22%]
tests/test_py2dataiku/test_complex_pipelines.py::TestCustomer360Pipeline::test_has_complex_joins PASSED [ 22%]
tests/test_py2dataiku/test_complex_pipelines.py::TestCustomer360Pipeline::test_has_aggregations PASSED [ 22%]
tests/test_py2dataiku/test_complex_pipelines.py::TestCustomer360Pipeline::test_produces_ascii_output PASSED [ 22%]
tests/test_py2dataiku/test_complex_pipelines.py::TestSupplyChainPipeline::test_conversion_succeeds PASSED [ 22%]
tests/test_py2dataiku/test_complex_pipelines.py::TestSupplyChainPipeline::test_has_time_series_operations PASSED [ 23%]
tests/test_py2dataiku/test_complex_pipelines.py::TestSupplyChainPipeline::test_has_window_functions PASSED [ 23%]
tests/test_py2dataiku/test_complex_pipelines.py::TestSupplyChainPipeline::test_multiple_outputs PASSED [ 23%]
tests/test_py2dataiku/test_complex_pipelines.py::TestSupplyChainPipeline::test_produces_plantuml_output PASSED [ 23%]
tests/test_py2dataiku/test_complex_pipelines.py::TestMarketingAttributionPipeline::test_conversion_succeeds PASSED [ 23%]
tests/test_py2dataiku/test_complex_pipelines.py::TestMarketingAttributionPipeline::test_has_attribution_logic PASSED [ 23%]
tests/test_py2dataiku/test_complex_pipelines.py::TestMarketingAttributionPipeline::test_has_journey_analysis PASSED [ 23%]
tests/test_py2dataiku/test_complex_pipelines.py::TestMarketingAttributionPipeline::test_roi_calculations PASSED [ 23%]
tests/test_py2dataiku/test_complex_pipelines.py::TestMarketingAttributionPipeline::test_produces_html_output PASSED [ 23%]
tests/test_py2dataiku/test_complex_pipelines.py::TestIoTPredictiveMaintenancePipeline::test_conversion_succeeds PASSED [ 24%]
tests/test_py2dataiku/test_complex_pipelines.py::TestIoTPredictiveMaintenancePipeline::test_has_sensor_processing PASSED [ 24%]
tests/test_py2dataiku/test_complex_pipelines.py::TestIoTPredictiveMaintenancePipeline::test_has_anomaly_detection PASSED [ 24%]
tests/test_py2dataiku/test_complex_pipelines.py::TestIoTPredictiveMaintenancePipeline::test_has_predictive_features PASSED [ 24%]
tests/test_py2dataiku/test_complex_pipelines.py::TestIoTPredictiveMaintenancePipeline::test_has_maintenance_scheduling PASSED [ 24%]
tests/test_py2dataiku/test_complex_pipelines.py::TestGenomicAnalysisPipeline::test_conversion_succeeds PASSED [ 24%]
tests/test_py2dataiku/test_complex_pipelines.py::TestGenomicAnalysisPipeline::test_has_variant_processing PASSED [ 24%]
tests/test_py2dataiku/test_complex_pipelines.py::TestGenomicAnalysisPipeline::test_has_quality_filtering PASSED [ 24%]
tests/test_py2dataiku/test_complex_pipelines.py::TestGenomicAnalysisPipeline::test_has_annotation_logic PASSED [ 25%]
tests/test_py2dataiku/test_complex_pipelines.py::TestGenomicAnalysisPipeline::test_has_burden_analysis PASSED [ 25%]
tests/test_py2dataiku/test_complex_pipelines.py::TestClickstreamAnalysisPipeline::test_conversion_succeeds PASSED [ 25%]
tests/test_py2dataiku/test_complex_pipelines.py::TestClickstreamAnalysisPipeline::test_has_event_processing PASSED [ 25%]
tests/test_py2dataiku/test_complex_pipelines.py::TestClickstreamAnalysisPipeline::test_has_session_analysis PASSED [ 25%]
tests/test_py2dataiku/test_complex_pipelines.py::TestClickstreamAnalysisPipeline::test_has_user_metrics PASSED [ 25%]
tests/test_py2dataiku/test_complex_pipelines.py::TestClickstreamAnalysisPipeline::test_has_conversion_tracking PASSED [ 25%]
tests/test_py2dataiku/test_complex_pipelines.py::TestPortfolioRiskPipeline::test_conversion_succeeds PASSED [ 25%]
tests/test_py2dataiku/test_complex_pipelines.py::TestPortfolioRiskPipeline::test_has_return_calculations PASSED [ 25%]
tests/test_py2dataiku/test_complex_pipelines.py::TestPortfolioRiskPipeline::test_has_risk_metrics PASSED [ 26%]
tests/test_py2dataiku/test_complex_pipelines.py::TestPortfolioRiskPipeline::test_has_beta_calculation PASSED [ 26%]
tests/test_py2dataiku/test_complex_pipelines.py::TestPortfolioRiskPipeline::test_has_sharpe_ratio PASSED [ 26%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineVisualizations::test_svg_visualization[fraud_detection-\nimport pandas as pd\nimport numpy as np\n\n# Load transaction streams\ntransactions = pd.read_csv('transactions.csv')\nuser_profiles = pd.read_csv('user_profiles.csv')\ndevice_fingerprints = pd.read_csv('device_fingerprints.csv')\nmerchant_risk_scores = pd.read_csv('merchant_risk_scores.csv')\nhistorical_fraud = pd.read_csv('historical_fraud.csv')\nip_geolocation = pd.read_csv('ip_geolocation.csv')\n\n# Parse timestamps\ntransactions['timestamp'] = pd.to_datetime(transactions['timestamp'])\ntransactions['date'] = transactions['timestamp'].dt.date\ntransactions['hour'] = transactions['timestamp'].dt.hour\ntransactions['minute'] = transactions['timestamp'].dt.minute\ntransactions['day_of_week'] = transactions['timestamp'].dt.dayofweek\ntransactions['is_weekend'] = transactions['day_of_week'].isin([5, 6]).astype(int)\ntransactions['is_night'] = transactions['hour'].between(0, 6).astype(int)\n\n# Enrich with user profile data\ntransactions_enriched = pd.merge(\n    transactions,\n    user_profiles[['user_id', 'account_age_days', 'verification_level',\n                   'avg_transaction_amount', 'typical_login_hour',\n                   'home_country', 'risk_tier']],\n    on='user_id',\n    how='left'\n)\n\n# Add device fingerprint risk\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    device_fingerprints[['device_id', 'device_trust_score', 'is_known_device',\n                         'browser_anomaly_score', 'last_seen_country']],\n    on='device_id',\n    how='left'\n)\n\n# Add merchant risk scores\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    merchant_risk_scores[['merchant_id', 'merchant_risk_score',\n                          'chargeback_rate', 'merchant_category']],\n    on='merchant_id',\n    how='left'\n)\n\n# Add IP geolocation\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    ip_geolocation[['ip_address', 'ip_country', 'ip_city',\n                    'is_vpn', 'is_tor', 'ip_risk_score']],\n    on='ip_address',\n    how='left'\n)\n\n# Calculate velocity features - transactions per user in last N minutes\ntransactions_enriched = transactions_enriched.sort_values(['user_id', 'timestamp'])\n\n# Rolling window aggregations per user\nuser_velocity = transactions_enriched.groupby('user_id').agg({\n    'transaction_id': 'count',\n    'amount': ['sum', 'mean', 'std', 'max'],\n    'merchant_id': 'nunique',\n    'ip_address': 'nunique',\n    'device_id': 'nunique'\n}).reset_index()\nuser_velocity.columns = ['user_id', 'tx_count_session', 'total_amount_session',\n                         'avg_amount_session', 'std_amount_session', 'max_amount_session',\n                         'unique_merchants', 'unique_ips', 'unique_devices']\n\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    user_velocity,\n    on='user_id',\n    how='left'\n)\n\n# Calculate deviation from user's typical behavior\ntransactions_enriched['amount_deviation'] = (\n    (transactions_enriched['amount'] - transactions_enriched['avg_transaction_amount']) /\n    transactions_enriched['avg_transaction_amount'].replace(0, 1)\n)\n\ntransactions_enriched['hour_deviation'] = abs(\n    transactions_enriched['hour'] - transactions_enriched['typical_login_hour']\n)\n\n# Geographic anomaly detection\ntransactions_enriched['country_mismatch'] = (\n    transactions_enriched['ip_country'] != transactions_enriched['home_country']\n).astype(int)\n\ntransactions_enriched['device_country_mismatch'] = (\n    transactions_enriched['ip_country'] != transactions_enriched['last_seen_country']\n).astype(int)\n\n# Composite risk score calculation\ntransactions_enriched['composite_risk_score'] = (\n    transactions_enriched['ip_risk_score'] * 0.2 +\n    transactions_enriched['merchant_risk_score'] * 0.15 +\n    (1 - transactions_enriched['device_trust_score']) * 0.2 +\n    transactions_enriched['amount_deviation'].clip(0, 5) * 0.15 +\n    transactions_enriched['country_mismatch'] * 0.15 +\n    transactions_enriched['is_vpn'] * 0.1 +\n    transactions_enriched['is_tor'] * 0.05\n)\n\n# Flag high-risk transactions\ntransactions_enriched['is_high_risk'] = (\n    transactions_enriched['composite_risk_score'] > 0.7\n).astype(int)\n\n# Join historical fraud patterns\nfraud_patterns = historical_fraud.groupby('user_id').agg({\n    'fraud_flag': 'sum',\n    'transaction_id': 'count'\n}).reset_index()\nfraud_patterns.columns = ['user_id', 'historical_fraud_count', 'historical_tx_count']\nfraud_patterns['historical_fraud_rate'] = (\n    fraud_patterns['historical_fraud_count'] / fraud_patterns['historical_tx_count']\n)\n\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    fraud_patterns[['user_id', 'historical_fraud_rate']],\n    on='user_id',\n    how='left'\n)\ntransactions_enriched['historical_fraud_rate'] = transactions_enriched['historical_fraud_rate'].fillna(0)\n\n# Final feature selection for ML model\nml_features = transactions_enriched[[\n    'transaction_id', 'user_id', 'amount', 'amount_deviation',\n    'hour_deviation', 'is_weekend', 'is_night',\n    'device_trust_score', 'is_known_device', 'browser_anomaly_score',\n    'merchant_risk_score', 'chargeback_rate',\n    'ip_risk_score', 'is_vpn', 'is_tor',\n    'country_mismatch', 'device_country_mismatch',\n    'tx_count_session', 'unique_merchants', 'unique_ips', 'unique_devices',\n    'composite_risk_score', 'historical_fraud_rate', 'is_high_risk'\n]].copy()\n\nml_features.to_csv('fraud_detection_features.csv', index=False)\ntransactions_enriched.to_csv('transactions_enriched_full.csv', index=False)\n] PASSED [ 26%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineVisualizations::test_svg_visualization[customer_360-\nimport pandas as pd\nimport numpy as np\n\n# Load data from multiple source systems\ncrm_customers = pd.read_csv('crm_customers.csv')\necommerce_users = pd.read_csv('ecommerce_users.csv')\nmobile_app_users = pd.read_csv('mobile_app_users.csv')\ncall_center_interactions = pd.read_csv('call_center_interactions.csv')\nemail_campaigns = pd.read_csv('email_campaigns.csv')\nweb_analytics = pd.read_csv('web_analytics.csv')\nsocial_media = pd.read_csv('social_media.csv')\nloyalty_program = pd.read_csv('loyalty_program.csv')\nsupport_tickets = pd.read_csv('support_tickets.csv')\nnps_surveys = pd.read_csv('nps_surveys.csv')\n\n# Standardize customer identifiers across systems\ncrm_customers['source'] = 'crm'\ncrm_customers['master_id'] = crm_customers['crm_id']\n\necommerce_users['source'] = 'ecommerce'\necommerce_users = pd.merge(\n    ecommerce_users,\n    crm_customers[['email', 'crm_id']],\n    on='email',\n    how='left'\n)\necommerce_users['master_id'] = ecommerce_users['crm_id'].fillna(\n    'ECO_' + ecommerce_users['ecommerce_user_id'].astype(str)\n)\n\nmobile_app_users['source'] = 'mobile'\nmobile_app_users = pd.merge(\n    mobile_app_users,\n    crm_customers[['phone', 'crm_id']],\n    on='phone',\n    how='left'\n)\nmobile_app_users['master_id'] = mobile_app_users['crm_id'].fillna(\n    'MOB_' + mobile_app_users['app_user_id'].astype(str)\n)\n\n# Create unified customer profile\ncustomer_base = crm_customers[['master_id', 'email', 'phone', 'first_name',\n                               'last_name', 'date_of_birth', 'gender',\n                               'address', 'city', 'state', 'country',\n                               'customer_since']].copy()\n\n# Aggregate e-commerce behavior\necommerce_metrics = ecommerce_users.groupby('master_id').agg({\n    'order_id': 'count',\n    'order_total': ['sum', 'mean', 'max'],\n    'items_purchased': 'sum',\n    'last_order_date': 'max',\n    'cart_abandonment_count': 'sum'\n}).reset_index()\necommerce_metrics.columns = ['master_id', 'ecom_order_count', 'ecom_total_spend',\n                              'ecom_avg_order', 'ecom_max_order', 'ecom_items_total',\n                              'ecom_last_order', 'ecom_cart_abandonments']\n\n# Aggregate mobile app engagement\nmobile_metrics = mobile_app_users.groupby('master_id').agg({\n    'session_count': 'sum',\n    'total_time_minutes': 'sum',\n    'push_notifications_clicked': 'sum',\n    'app_crashes': 'sum',\n    'features_used': lambda x: len(set(','.join(x.dropna()).split(','))),\n    'last_active_date': 'max'\n}).reset_index()\nmobile_metrics.columns = ['master_id', 'app_sessions', 'app_time_minutes',\n                          'push_clicks', 'app_crashes', 'features_used_count',\n                          'app_last_active']\n\n# Aggregate call center interactions\ncall_metrics = call_center_interactions.groupby('master_id').agg({\n    'call_id': 'count',\n    'call_duration_seconds': ['sum', 'mean'],\n    'issue_resolved': 'mean',\n    'escalated': 'sum',\n    'sentiment_score': 'mean'\n}).reset_index()\ncall_metrics.columns = ['master_id', 'call_count', 'total_call_duration',\n                        'avg_call_duration', 'resolution_rate',\n                        'escalation_count', 'call_sentiment']\n\n# Aggregate email engagement\nemail_metrics = email_campaigns.groupby('master_id').agg({\n    'email_sent': 'sum',\n    'email_opened': 'sum',\n    'email_clicked': 'sum',\n    'unsubscribed': 'max'\n}).reset_index()\nemail_metrics['email_open_rate'] = email_metrics['email_opened'] / email_metrics['email_sent']\nemail_metrics['email_click_rate'] = email_metrics['email_clicked'] / email_metrics['email_opened'].replace(0, 1)\n\n# Aggregate web analytics\nweb_metrics = web_analytics.groupby('master_id').agg({\n    'page_views': 'sum',\n    'unique_sessions': 'sum',\n    'bounce_rate': 'mean',\n    'avg_session_duration': 'mean',\n    'conversion_events': 'sum'\n}).reset_index()\n\n# Aggregate loyalty program data\nloyalty_metrics = loyalty_program.groupby('master_id').agg({\n    'points_earned': 'sum',\n    'points_redeemed': 'sum',\n    'tier_level': 'max',\n    'referrals_made': 'sum'\n}).reset_index()\nloyalty_metrics['points_balance'] = loyalty_metrics['points_earned'] - loyalty_metrics['points_redeemed']\n\n# Aggregate support tickets\nsupport_metrics = support_tickets.groupby('master_id').agg({\n    'ticket_id': 'count',\n    'resolution_time_hours': 'mean',\n    'satisfaction_rating': 'mean',\n    'ticket_reopened': 'sum'\n}).reset_index()\nsupport_metrics.columns = ['master_id', 'ticket_count', 'avg_resolution_time',\n                           'support_satisfaction', 'tickets_reopened']\n\n# Aggregate NPS surveys\nnps_metrics = nps_surveys.groupby('master_id').agg({\n    'nps_score': 'mean',\n    'survey_id': 'count',\n    'would_recommend': 'mean'\n}).reset_index()\nnps_metrics.columns = ['master_id', 'avg_nps_score', 'surveys_completed', 'recommend_rate']\n\n# Merge all metrics into unified customer 360 view\ncustomer_360 = customer_base.copy()\n\nfor metrics_df in [ecommerce_metrics, mobile_metrics, call_metrics,\n                   email_metrics, web_metrics, loyalty_metrics,\n                   support_metrics, nps_metrics]:\n    customer_360 = pd.merge(customer_360, metrics_df, on='master_id', how='left')\n\n# Fill missing values appropriately\nnumeric_cols = customer_360.select_dtypes(include=[np.number]).columns\ncustomer_360[numeric_cols] = customer_360[numeric_cols].fillna(0)\n\n# Calculate derived metrics\ncustomer_360['total_interactions'] = (\n    customer_360['ecom_order_count'] +\n    customer_360['app_sessions'] +\n    customer_360['call_count'] +\n    customer_360['ticket_count']\n)\n\ncustomer_360['digital_engagement_score'] = (\n    customer_360['app_sessions'] * 0.3 +\n    customer_360['email_open_rate'] * 20 +\n    customer_360['page_views'] * 0.1 +\n    customer_360['push_clicks'] * 0.5\n).clip(0, 100)\n\ncustomer_360['customer_health_score'] = (\n    customer_360['avg_nps_score'] / 10 * 25 +\n    customer_360['resolution_rate'] * 25 +\n    customer_360['support_satisfaction'] / 5 * 25 +\n    (1 - customer_360['escalation_count'].clip(0, 5) / 5) * 25\n)\n\n# Customer segmentation\ncustomer_360['value_segment'] = pd.cut(\n    customer_360['ecom_total_spend'],\n    bins=[-np.inf, 100, 500, 2000, 10000, np.inf],\n    labels=['Dormant', 'Bronze', 'Silver', 'Gold', 'Platinum']\n)\n\ncustomer_360['engagement_segment'] = pd.cut(\n    customer_360['digital_engagement_score'],\n    bins=[-np.inf, 20, 40, 60, 80, np.inf],\n    labels=['Inactive', 'Low', 'Medium', 'High', 'Power User']\n)\n\n# Calculate churn risk\ncustomer_360['days_since_activity'] = (\n    pd.to_datetime('today') - pd.to_datetime(customer_360['ecom_last_order'])\n).dt.days.fillna(999)\n\ncustomer_360['churn_risk_score'] = (\n    customer_360['days_since_activity'] / 365 * 30 +\n    (1 - customer_360['email_open_rate'].fillna(0)) * 20 +\n    customer_360['tickets_reopened'] * 10 +\n    (10 - customer_360['avg_nps_score'].fillna(5)) * 4\n).clip(0, 100)\n\n# Save outputs\ncustomer_360.to_csv('customer_360_unified.csv', index=False)\n] PASSED [ 26%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineVisualizations::test_svg_visualization[supply_chain-\nimport pandas as pd\nimport numpy as np\n\n# Load supply chain data\nsales_history = pd.read_csv('sales_history.csv')\ninventory_levels = pd.read_csv('inventory_levels.csv')\nsupplier_data = pd.read_csv('supplier_data.csv')\nwarehouse_locations = pd.read_csv('warehouse_locations.csv')\nshipping_costs = pd.read_csv('shipping_costs.csv')\nproduct_catalog = pd.read_csv('product_catalog.csv')\npromotions_calendar = pd.read_csv('promotions_calendar.csv')\nweather_data = pd.read_csv('weather_data.csv')\neconomic_indicators = pd.read_csv('economic_indicators.csv')\n\n# Parse dates\nsales_history['sale_date'] = pd.to_datetime(sales_history['sale_date'])\nsales_history['year'] = sales_history['sale_date'].dt.year\nsales_history['month'] = sales_history['sale_date'].dt.month\nsales_history['week'] = sales_history['sale_date'].dt.isocalendar().week\nsales_history['day_of_week'] = sales_history['sale_date'].dt.dayofweek\nsales_history['is_weekend'] = sales_history['day_of_week'].isin([5, 6]).astype(int)\n\n# Enrich sales with product info\nsales_enriched = pd.merge(\n    sales_history,\n    product_catalog[['product_id', 'category', 'subcategory', 'brand',\n                     'unit_cost', 'weight_kg', 'is_perishable', 'shelf_life_days']],\n    on='product_id',\n    how='left'\n)\n\n# Add promotion flags\npromotions_calendar['promo_date'] = pd.to_datetime(promotions_calendar['promo_date'])\nsales_enriched = pd.merge(\n    sales_enriched,\n    promotions_calendar[['product_id', 'promo_date', 'discount_pct', 'promo_type']],\n    left_on=['product_id', 'sale_date'],\n    right_on=['product_id', 'promo_date'],\n    how='left'\n)\nsales_enriched['is_promotion'] = sales_enriched['discount_pct'].notna().astype(int)\nsales_enriched['discount_pct'] = sales_enriched['discount_pct'].fillna(0)\n\n# Add weather impact\nweather_data['date'] = pd.to_datetime(weather_data['date'])\nsales_enriched = pd.merge(\n    sales_enriched,\n    weather_data[['date', 'region', 'temperature', 'precipitation', 'weather_condition']],\n    left_on=['sale_date', 'region'],\n    right_on=['date', 'region'],\n    how='left'\n)\n\n# Calculate time-based aggregations for demand patterns\ndaily_demand = sales_enriched.groupby(['product_id', 'warehouse_id', 'sale_date']).agg({\n    'quantity': 'sum',\n    'revenue': 'sum',\n    'is_promotion': 'max',\n    'temperature': 'mean'\n}).reset_index()\n\n# Calculate rolling averages for demand forecasting\ndaily_demand = daily_demand.sort_values(['product_id', 'warehouse_id', 'sale_date'])\ndaily_demand['demand_7d_avg'] = daily_demand.groupby(['product_id', 'warehouse_id'])['quantity'].transform(\n    lambda x: x.rolling(7, min_periods=1).mean()\n)\ndaily_demand['demand_30d_avg'] = daily_demand.groupby(['product_id', 'warehouse_id'])['quantity'].transform(\n    lambda x: x.rolling(30, min_periods=1).mean()\n)\ndaily_demand['demand_90d_avg'] = daily_demand.groupby(['product_id', 'warehouse_id'])['quantity'].transform(\n    lambda x: x.rolling(90, min_periods=1).mean()\n)\n\n# Calculate demand volatility\ndaily_demand['demand_7d_std'] = daily_demand.groupby(['product_id', 'warehouse_id'])['quantity'].transform(\n    lambda x: x.rolling(7, min_periods=1).std()\n)\ndaily_demand['demand_coefficient_variation'] = (\n    daily_demand['demand_7d_std'] / daily_demand['demand_7d_avg'].replace(0, 1)\n)\n\n# Seasonality detection\nmonthly_patterns = sales_enriched.groupby(['product_id', 'month']).agg({\n    'quantity': 'mean'\n}).reset_index()\nmonthly_patterns.columns = ['product_id', 'month', 'monthly_avg_demand']\n\nyearly_avg = monthly_patterns.groupby('product_id')['monthly_avg_demand'].transform('mean')\nmonthly_patterns['seasonality_index'] = monthly_patterns['monthly_avg_demand'] / yearly_avg\n\n# Join current inventory levels\ninventory_current = inventory_levels.groupby(['product_id', 'warehouse_id']).agg({\n    'quantity_on_hand': 'sum',\n    'quantity_reserved': 'sum',\n    'quantity_in_transit': 'sum',\n    'last_restock_date': 'max'\n}).reset_index()\ninventory_current['available_inventory'] = (\n    inventory_current['quantity_on_hand'] - inventory_current['quantity_reserved']\n)\n\n# Calculate days of supply\ninventory_analysis = pd.merge(\n    inventory_current,\n    daily_demand.groupby(['product_id', 'warehouse_id']).agg({\n        'demand_30d_avg': 'last'\n    }).reset_index(),\n    on=['product_id', 'warehouse_id'],\n    how='left'\n)\ninventory_analysis['days_of_supply'] = (\n    inventory_analysis['available_inventory'] /\n    inventory_analysis['demand_30d_avg'].replace(0, 0.1)\n)\n\n# Add supplier lead times\ninventory_analysis = pd.merge(\n    inventory_analysis,\n    supplier_data[['product_id', 'supplier_id', 'lead_time_days',\n                   'min_order_qty', 'unit_cost', 'reliability_score']],\n    on='product_id',\n    how='left'\n)\n\n# Calculate reorder points with safety stock\ninventory_analysis['safety_stock'] = (\n    inventory_analysis['demand_30d_avg'] *\n    inventory_analysis['lead_time_days'] * 0.5  # Safety factor\n)\ninventory_analysis['reorder_point'] = (\n    inventory_analysis['demand_30d_avg'] * inventory_analysis['lead_time_days'] +\n    inventory_analysis['safety_stock']\n)\n\n# Flag items needing reorder\ninventory_analysis['needs_reorder'] = (\n    inventory_analysis['available_inventory'] < inventory_analysis['reorder_point']\n).astype(int)\n\n# Calculate optimal order quantity (EOQ approximation)\ninventory_analysis['holding_cost_annual'] = inventory_analysis['unit_cost'] * 0.25\ninventory_analysis['order_cost'] = 50  # Fixed ordering cost\ninventory_analysis['annual_demand'] = inventory_analysis['demand_30d_avg'] * 12 * 30\n\ninventory_analysis['economic_order_qty'] = np.sqrt(\n    2 * inventory_analysis['annual_demand'] * inventory_analysis['order_cost'] /\n    inventory_analysis['holding_cost_annual'].replace(0, 1)\n)\n\n# Calculate shipping optimization\nshipping_analysis = pd.merge(\n    inventory_analysis,\n    warehouse_locations[['warehouse_id', 'latitude', 'longitude', 'capacity_units']],\n    on='warehouse_id',\n    how='left'\n)\n\n# Add shipping costs\nshipping_analysis = pd.merge(\n    shipping_analysis,\n    shipping_costs,\n    on='warehouse_id',\n    how='left'\n)\n\n# Calculate total landed cost\nshipping_analysis['landed_cost'] = (\n    shipping_analysis['unit_cost'] +\n    shipping_analysis['shipping_cost_per_unit'] +\n    shipping_analysis['handling_cost']\n)\n\n# Identify stock-out risk\nshipping_analysis['stockout_risk'] = np.where(\n    shipping_analysis['days_of_supply'] < shipping_analysis['lead_time_days'],\n    'High',\n    np.where(\n        shipping_analysis['days_of_supply'] < shipping_analysis['lead_time_days'] * 1.5,\n        'Medium',\n        'Low'\n    )\n)\n\n# Save outputs\ndaily_demand.to_csv('demand_forecast_features.csv', index=False)\ninventory_analysis.to_csv('inventory_optimization.csv', index=False)\nshipping_analysis.to_csv('supply_chain_analysis.csv', index=False)\nmonthly_patterns.to_csv('seasonality_patterns.csv', index=False)\n] PASSED [ 26%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineVisualizations::test_svg_visualization[marketing_attribution-\nimport pandas as pd\nimport numpy as np\n\n# Load marketing data\ntouchpoints = pd.read_csv('touchpoints.csv')\nconversions = pd.read_csv('conversions.csv')\nad_spend = pd.read_csv('ad_spend.csv')\nchannel_costs = pd.read_csv('channel_costs.csv')\ncampaign_metadata = pd.read_csv('campaign_metadata.csv')\ncustomer_journeys = pd.read_csv('customer_journeys.csv')\norganic_traffic = pd.read_csv('organic_traffic.csv')\noffline_media = pd.read_csv('offline_media.csv')\n\n# Parse timestamps\ntouchpoints['touchpoint_time'] = pd.to_datetime(touchpoints['touchpoint_time'])\nconversions['conversion_time'] = pd.to_datetime(conversions['conversion_time'])\n\n# Create customer journey sequences\ntouchpoints_sorted = touchpoints.sort_values(['customer_id', 'touchpoint_time'])\n\n# Assign journey IDs (new journey if gap > 30 days)\ntouchpoints_sorted['time_since_last'] = touchpoints_sorted.groupby('customer_id')['touchpoint_time'].diff()\ntouchpoints_sorted['new_journey'] = (\n    touchpoints_sorted['time_since_last'] > pd.Timedelta(days=30)\n).fillna(True).astype(int)\ntouchpoints_sorted['journey_id'] = touchpoints_sorted.groupby('customer_id')['new_journey'].cumsum()\n\n# Create journey-level aggregations\njourney_touchpoints = touchpoints_sorted.groupby(['customer_id', 'journey_id']).agg({\n    'touchpoint_id': 'count',\n    'channel': lambda x: ' > '.join(x),\n    'campaign_id': lambda x: list(x.unique()),\n    'touchpoint_time': ['min', 'max'],\n    'cost': 'sum'\n}).reset_index()\njourney_touchpoints.columns = ['customer_id', 'journey_id', 'touchpoint_count',\n                                'channel_path', 'campaigns_touched',\n                                'journey_start', 'journey_end', 'journey_cost']\n\n# Calculate journey duration\njourney_touchpoints['journey_duration_days'] = (\n    journey_touchpoints['journey_end'] - journey_touchpoints['journey_start']\n).dt.total_seconds() / 86400\n\n# Join with conversions\njourney_conversions = pd.merge(\n    journey_touchpoints,\n    conversions[['customer_id', 'conversion_time', 'conversion_value', 'product_category']],\n    on='customer_id',\n    how='left'\n)\n\n# Filter to conversions within journey window\njourney_conversions['converted'] = (\n    (journey_conversions['conversion_time'] >= journey_conversions['journey_start']) &\n    (journey_conversions['conversion_time'] <= journey_conversions['journey_end'] + pd.Timedelta(days=7))\n).astype(int)\n\njourney_conversions['conversion_value'] = np.where(\n    journey_conversions['converted'] == 1,\n    journey_conversions['conversion_value'],\n    0\n)\n\n# Calculate attribution models\n\n# 1. First-touch attribution\ntouchpoints_sorted['is_first_touch'] = touchpoints_sorted.groupby(\n    ['customer_id', 'journey_id']\n).cumcount() == 0\n\nfirst_touch = touchpoints_sorted[touchpoints_sorted['is_first_touch']].copy()\nfirst_touch_attribution = first_touch.groupby('channel').agg({\n    'touchpoint_id': 'count',\n    'cost': 'sum'\n}).reset_index()\nfirst_touch_attribution.columns = ['channel', 'first_touch_count', 'first_touch_cost']\n\n# 2. Last-touch attribution\nlast_touch = touchpoints_sorted.groupby(['customer_id', 'journey_id']).last().reset_index()\nlast_touch_attribution = last_touch.groupby('channel').agg({\n    'touchpoint_id': 'count',\n    'cost': 'sum'\n}).reset_index()\nlast_touch_attribution.columns = ['channel', 'last_touch_count', 'last_touch_cost']\n\n# 3. Linear attribution (equal credit to all touchpoints)\njourney_conversions_exploded = journey_conversions.explode('campaigns_touched')\njourney_conversions_exploded['linear_credit'] = (\n    journey_conversions_exploded['conversion_value'] /\n    journey_conversions_exploded['touchpoint_count']\n)\n\n# 4. Time-decay attribution\ntouchpoints_with_conversion = pd.merge(\n    touchpoints_sorted,\n    journey_conversions[['customer_id', 'journey_id', 'conversion_time', 'conversion_value', 'converted']],\n    on=['customer_id', 'journey_id'],\n    how='left'\n)\n\ntouchpoints_with_conversion['days_to_conversion'] = (\n    touchpoints_with_conversion['conversion_time'] - touchpoints_with_conversion['touchpoint_time']\n).dt.total_seconds() / 86400\n\n# Exponential decay weight (half-life = 7 days)\ntouchpoints_with_conversion['decay_weight'] = np.exp(\n    -touchpoints_with_conversion['days_to_conversion'] / 7\n)\n\n# Normalize weights within journey\ntouchpoints_with_conversion['weight_sum'] = touchpoints_with_conversion.groupby(\n    ['customer_id', 'journey_id']\n)['decay_weight'].transform('sum')\n\ntouchpoints_with_conversion['time_decay_credit'] = (\n    touchpoints_with_conversion['decay_weight'] /\n    touchpoints_with_conversion['weight_sum'].replace(0, 1) *\n    touchpoints_with_conversion['conversion_value']\n)\n\n# 5. Position-based attribution (40% first, 40% last, 20% middle)\ntouchpoints_with_conversion['position'] = touchpoints_with_conversion.groupby(\n    ['customer_id', 'journey_id']\n).cumcount() + 1\n\ntouchpoints_with_conversion['total_positions'] = touchpoints_with_conversion.groupby(\n    ['customer_id', 'journey_id']\n)['position'].transform('max')\n\ntouchpoints_with_conversion['position_weight'] = np.where(\n    touchpoints_with_conversion['position'] == 1,\n    0.4,\n    np.where(\n        touchpoints_with_conversion['position'] == touchpoints_with_conversion['total_positions'],\n        0.4,\n        0.2 / (touchpoints_with_conversion['total_positions'] - 2).replace(0, 1)\n    )\n)\n\ntouchpoints_with_conversion['position_based_credit'] = (\n    touchpoints_with_conversion['position_weight'] *\n    touchpoints_with_conversion['conversion_value']\n)\n\n# Aggregate attribution by channel\nchannel_attribution = touchpoints_with_conversion.groupby('channel').agg({\n    'touchpoint_id': 'count',\n    'cost': 'sum',\n    'linear_credit': 'sum',\n    'time_decay_credit': 'sum',\n    'position_based_credit': 'sum'\n}).reset_index()\n\nchannel_attribution = pd.merge(channel_attribution, first_touch_attribution, on='channel', how='left')\nchannel_attribution = pd.merge(channel_attribution, last_touch_attribution, on='channel', how='left')\n\n# Calculate ROI by attribution model\nchannel_attribution['roi_linear'] = (\n    channel_attribution['linear_credit'] - channel_attribution['cost']\n) / channel_attribution['cost'].replace(0, 1)\n\nchannel_attribution['roi_time_decay'] = (\n    channel_attribution['time_decay_credit'] - channel_attribution['cost']\n) / channel_attribution['cost'].replace(0, 1)\n\nchannel_attribution['roi_position'] = (\n    channel_attribution['position_based_credit'] - channel_attribution['cost']\n) / channel_attribution['cost'].replace(0, 1)\n\n# Marketing mix modeling aggregations\ndaily_spend = ad_spend.groupby(['date', 'channel']).agg({\n    'spend': 'sum',\n    'impressions': 'sum',\n    'clicks': 'sum'\n}).reset_index()\n\ndaily_spend['cpm'] = daily_spend['spend'] / daily_spend['impressions'] * 1000\ndaily_spend['cpc'] = daily_spend['spend'] / daily_spend['clicks'].replace(0, 1)\ndaily_spend['ctr'] = daily_spend['clicks'] / daily_spend['impressions'].replace(0, 1)\n\n# Add adstock transformation (carryover effect)\ndaily_spend = daily_spend.sort_values(['channel', 'date'])\ndaily_spend['spend_adstock'] = daily_spend.groupby('channel')['spend'].transform(\n    lambda x: x.ewm(halflife=7).mean()\n)\n\n# Save outputs\njourney_touchpoints.to_csv('customer_journeys_analyzed.csv', index=False)\nchannel_attribution.to_csv('channel_attribution_models.csv', index=False)\ntouchpoints_with_conversion.to_csv('touchpoint_attribution_detail.csv', index=False)\ndaily_spend.to_csv('marketing_mix_features.csv', index=False)\n] PASSED [ 26%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineVisualizations::test_svg_visualization[iot_predictive_maintenance-\nimport pandas as pd\nimport numpy as np\n\n# Load IoT sensor data\nsensor_readings = pd.read_csv('sensor_readings.csv')\nequipment_registry = pd.read_csv('equipment_registry.csv')\nmaintenance_history = pd.read_csv('maintenance_history.csv')\nfailure_logs = pd.read_csv('failure_logs.csv')\noperating_conditions = pd.read_csv('operating_conditions.csv')\nparts_inventory = pd.read_csv('parts_inventory.csv')\ntechnician_schedules = pd.read_csv('technician_schedules.csv')\n\n# Parse timestamps\nsensor_readings['reading_time'] = pd.to_datetime(sensor_readings['reading_time'])\nmaintenance_history['maintenance_date'] = pd.to_datetime(maintenance_history['maintenance_date'])\nfailure_logs['failure_time'] = pd.to_datetime(failure_logs['failure_time'])\n\n# Clean sensor data - remove outliers\nsensor_readings = sensor_readings.sort_values(['equipment_id', 'sensor_type', 'reading_time'])\n\n# Calculate rolling statistics per sensor\nsensor_readings['value_1h_mean'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].transform(lambda x: x.rolling('1H', on=sensor_readings['reading_time']).mean())\n\nsensor_readings['value_1h_std'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].transform(lambda x: x.rolling('1H', on=sensor_readings['reading_time']).std())\n\nsensor_readings['value_24h_mean'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].transform(lambda x: x.rolling('24H', on=sensor_readings['reading_time']).mean())\n\nsensor_readings['value_24h_max'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].transform(lambda x: x.rolling('24H', on=sensor_readings['reading_time']).max())\n\nsensor_readings['value_24h_min'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].transform(lambda x: x.rolling('24H', on=sensor_readings['reading_time']).min())\n\n# Calculate rate of change\nsensor_readings['value_diff'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].diff()\n\nsensor_readings['value_pct_change'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].pct_change()\n\n# Detect anomalies using z-score\nsensor_readings['z_score'] = (\n    (sensor_readings['sensor_value'] - sensor_readings['value_24h_mean']) /\n    sensor_readings['value_1h_std'].replace(0, 1)\n)\nsensor_readings['is_anomaly'] = (abs(sensor_readings['z_score']) > 3).astype(int)\n\n# Pivot sensor types to create feature vectors\nsensor_features = sensor_readings.pivot_table(\n    index=['equipment_id', 'reading_time'],\n    columns='sensor_type',\n    values=['sensor_value', 'value_1h_mean', 'value_24h_max', 'z_score', 'is_anomaly'],\n    aggfunc='first'\n).reset_index()\n\n# Flatten column names\nsensor_features.columns = ['_'.join(str(c) for c in col).strip('_')\n                           for col in sensor_features.columns]\n\n# Add equipment metadata\nsensor_features = pd.merge(\n    sensor_features,\n    equipment_registry[['equipment_id', 'equipment_type', 'manufacturer',\n                        'installation_date', 'rated_capacity', 'location']],\n    on='equipment_id',\n    how='left'\n)\n\n# Calculate equipment age\nsensor_features['installation_date'] = pd.to_datetime(sensor_features['installation_date'])\nsensor_features['equipment_age_days'] = (\n    sensor_features['reading_time'] - sensor_features['installation_date']\n).dt.days\n\n# Add operating conditions\noperating_conditions['condition_time'] = pd.to_datetime(operating_conditions['condition_time'])\nsensor_features = pd.merge_asof(\n    sensor_features.sort_values('reading_time'),\n    operating_conditions.sort_values('condition_time'),\n    left_on='reading_time',\n    right_on='condition_time',\n    by='equipment_id',\n    direction='backward'\n)\n\n# Calculate maintenance history features\nmaintenance_agg = maintenance_history.groupby('equipment_id').agg({\n    'maintenance_id': 'count',\n    'maintenance_date': 'max',\n    'maintenance_cost': 'sum',\n    'downtime_hours': 'sum'\n}).reset_index()\nmaintenance_agg.columns = ['equipment_id', 'total_maintenance_count',\n                           'last_maintenance_date', 'total_maintenance_cost',\n                           'total_downtime_hours']\n\nsensor_features = pd.merge(\n    sensor_features,\n    maintenance_agg,\n    on='equipment_id',\n    how='left'\n)\n\n# Calculate days since last maintenance\nsensor_features['last_maintenance_date'] = pd.to_datetime(sensor_features['last_maintenance_date'])\nsensor_features['days_since_maintenance'] = (\n    sensor_features['reading_time'] - sensor_features['last_maintenance_date']\n).dt.days\n\n# Calculate failure history features\nfailure_agg = failure_logs.groupby('equipment_id').agg({\n    'failure_id': 'count',\n    'failure_time': 'max',\n    'failure_severity': 'mean',\n    'repair_time_hours': 'sum'\n}).reset_index()\nfailure_agg.columns = ['equipment_id', 'total_failures', 'last_failure_date',\n                       'avg_failure_severity', 'total_repair_hours']\n\nsensor_features = pd.merge(\n    sensor_features,\n    failure_agg,\n    on='equipment_id',\n    how='left'\n)\n\n# Create target variable: failure within next N days\nfailure_logs_sorted = failure_logs.sort_values(['equipment_id', 'failure_time'])\nsensor_features = pd.merge_asof(\n    sensor_features.sort_values('reading_time'),\n    failure_logs_sorted[['equipment_id', 'failure_time']].rename(\n        columns={'failure_time': 'next_failure_time'}\n    ),\n    left_on='reading_time',\n    right_on='next_failure_time',\n    by='equipment_id',\n    direction='forward'\n)\n\nsensor_features['days_to_failure'] = (\n    sensor_features['next_failure_time'] - sensor_features['reading_time']\n).dt.total_seconds() / 86400\n\nsensor_features['failure_within_7d'] = (sensor_features['days_to_failure'] <= 7).astype(int)\nsensor_features['failure_within_30d'] = (sensor_features['days_to_failure'] <= 30).astype(int)\n\n# Calculate health score\nsensor_features['anomaly_count_24h'] = sensor_features.groupby('equipment_id')['is_anomaly_temperature'].transform(\n    lambda x: x.rolling('24H', on=sensor_features['reading_time']).sum()\n)\n\nsensor_features['health_score'] = 100 - (\n    sensor_features['anomaly_count_24h'] * 5 +\n    sensor_features['total_failures'].fillna(0) * 2 +\n    np.minimum(sensor_features['days_since_maintenance'].fillna(0) / 365, 1) * 20\n).clip(0, 100)\n\n# Prioritize maintenance\nsensor_features['maintenance_priority'] = np.where(\n    sensor_features['health_score'] < 50, 'Critical',\n    np.where(sensor_features['health_score'] < 70, 'High',\n    np.where(sensor_features['health_score'] < 85, 'Medium', 'Low'))\n)\n\n# Join parts availability\nparts_available = parts_inventory.groupby('equipment_type').agg({\n    'part_id': 'count',\n    'quantity_available': 'sum',\n    'lead_time_days': 'mean'\n}).reset_index()\nparts_available.columns = ['equipment_type', 'part_types_available',\n                           'total_parts_stock', 'avg_part_lead_time']\n\nsensor_features = pd.merge(\n    sensor_features,\n    parts_available,\n    on='equipment_type',\n    how='left'\n)\n\n# Save outputs\nsensor_features.to_csv('predictive_maintenance_features.csv', index=False)\n\n# Create maintenance schedule recommendations\nmaintenance_schedule = sensor_features[sensor_features['maintenance_priority'].isin(['Critical', 'High'])].groupby(\n    'equipment_id'\n).agg({\n    'health_score': 'min',\n    'maintenance_priority': 'first',\n    'days_since_maintenance': 'max',\n    'reading_time': 'max'\n}).reset_index()\n\nmaintenance_schedule.columns = ['equipment_id', 'current_health', 'priority',\n                                'days_since_last_maintenance', 'last_reading']\nmaintenance_schedule.to_csv('maintenance_schedule_recommendations.csv', index=False)\n] PASSED [ 26%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineVisualizations::test_svg_visualization[genomic_analysis-\nimport pandas as pd\nimport numpy as np\n\n# Load genomic data\nvariants = pd.read_csv('variants.csv')\nsamples = pd.read_csv('samples.csv')\ngene_annotations = pd.read_csv('gene_annotations.csv')\nclinical_data = pd.read_csv('clinical_data.csv')\npathway_mappings = pd.read_csv('pathway_mappings.csv')\npopulation_frequencies = pd.read_csv('population_frequencies.csv')\nfunctional_predictions = pd.read_csv('functional_predictions.csv')\ndisease_associations = pd.read_csv('disease_associations.csv')\n\n# Standardize variant identifiers\nvariants['variant_id'] = (\n    variants['chromosome'].astype(str) + ':' +\n    variants['position'].astype(str) + ':' +\n    variants['reference'] + '>' + variants['alternate']\n)\n\n# Calculate variant quality metrics\nvariants['quality_pass'] = (\n    (variants['quality_score'] >= 30) &\n    (variants['read_depth'] >= 10) &\n    (variants['allele_frequency'] >= 0.2)\n).astype(int)\n\n# Filter high-quality variants\nhigh_quality_variants = variants[variants['quality_pass'] == 1].copy()\n\n# Annotate with gene information\nhigh_quality_variants = pd.merge(\n    high_quality_variants,\n    gene_annotations[['chromosome', 'start_position', 'end_position',\n                      'gene_symbol', 'gene_type', 'strand']],\n    on='chromosome',\n    how='left'\n)\n\n# Filter to variants within gene boundaries\nhigh_quality_variants = high_quality_variants[\n    (high_quality_variants['position'] >= high_quality_variants['start_position']) &\n    (high_quality_variants['position'] <= high_quality_variants['end_position'])\n]\n\n# Add functional impact predictions\nhigh_quality_variants = pd.merge(\n    high_quality_variants,\n    functional_predictions[['variant_id', 'sift_score', 'polyphen_score',\n                            'cadd_score', 'predicted_impact']],\n    on='variant_id',\n    how='left'\n)\n\n# Classify variant impact\nhigh_quality_variants['impact_category'] = np.where(\n    (high_quality_variants['sift_score'] < 0.05) | (high_quality_variants['polyphen_score'] > 0.85),\n    'Damaging',\n    np.where(\n        (high_quality_variants['sift_score'] < 0.1) | (high_quality_variants['polyphen_score'] > 0.5),\n        'Possibly_Damaging',\n        'Benign'\n    )\n)\n\n# Add population allele frequencies\nhigh_quality_variants = pd.merge(\n    high_quality_variants,\n    population_frequencies[['variant_id', 'gnomad_af', 'gnomad_af_eas',\n                            'gnomad_af_eur', 'gnomad_af_afr']],\n    on='variant_id',\n    how='left'\n)\n\n# Flag rare variants (MAF < 1%)\nhigh_quality_variants['is_rare'] = (high_quality_variants['gnomad_af'] < 0.01).astype(int)\nhigh_quality_variants['is_ultra_rare'] = (high_quality_variants['gnomad_af'] < 0.001).astype(int)\n\n# Add disease associations\nhigh_quality_variants = pd.merge(\n    high_quality_variants,\n    disease_associations[['gene_symbol', 'disease_name', 'inheritance_pattern',\n                          'clinical_significance']],\n    on='gene_symbol',\n    how='left'\n)\n\n# Calculate per-sample variant burden\nsample_burden = high_quality_variants.groupby('sample_id').agg({\n    'variant_id': 'count',\n    'is_rare': 'sum',\n    'impact_category': lambda x: (x == 'Damaging').sum()\n}).reset_index()\nsample_burden.columns = ['sample_id', 'total_variants', 'rare_variants', 'damaging_variants']\n\n# Add clinical phenotype data\nsample_burden = pd.merge(\n    sample_burden,\n    samples[['sample_id', 'patient_id', 'tissue_type', 'collection_date']],\n    on='sample_id',\n    how='left'\n)\n\nsample_burden = pd.merge(\n    sample_burden,\n    clinical_data[['patient_id', 'diagnosis', 'age_at_diagnosis',\n                   'sex', 'ethnicity', 'family_history']],\n    on='patient_id',\n    how='left'\n)\n\n# Calculate gene-level burden\ngene_burden = high_quality_variants.groupby(['sample_id', 'gene_symbol']).agg({\n    'variant_id': 'count',\n    'is_rare': 'sum',\n    'impact_category': lambda x: (x == 'Damaging').sum(),\n    'cadd_score': 'max'\n}).reset_index()\ngene_burden.columns = ['sample_id', 'gene_symbol', 'variants_in_gene',\n                       'rare_in_gene', 'damaging_in_gene', 'max_cadd']\n\n# Add pathway information\ngene_burden = pd.merge(\n    gene_burden,\n    pathway_mappings[['gene_symbol', 'pathway_id', 'pathway_name']],\n    on='gene_symbol',\n    how='left'\n)\n\n# Calculate pathway-level burden\npathway_burden = gene_burden.groupby(['sample_id', 'pathway_id', 'pathway_name']).agg({\n    'gene_symbol': 'nunique',\n    'variants_in_gene': 'sum',\n    'damaging_in_gene': 'sum',\n    'max_cadd': 'max'\n}).reset_index()\npathway_burden.columns = ['sample_id', 'pathway_id', 'pathway_name',\n                          'genes_affected', 'pathway_variants',\n                          'pathway_damaging', 'pathway_max_cadd']\n\n# Identify potentially pathogenic variants\npathogenic_candidates = high_quality_variants[\n    (high_quality_variants['impact_category'] == 'Damaging') &\n    (high_quality_variants['is_rare'] == 1) &\n    (high_quality_variants['clinical_significance'].isin(['Pathogenic', 'Likely_Pathogenic']).fillna(False))\n].copy()\n\n# Create variant report\nvariant_report = pathogenic_candidates.groupby(['sample_id', 'gene_symbol']).agg({\n    'variant_id': lambda x: '; '.join(x),\n    'disease_name': 'first',\n    'inheritance_pattern': 'first',\n    'cadd_score': 'max'\n}).reset_index()\n\n# Save outputs\nhigh_quality_variants.to_csv('annotated_variants.csv', index=False)\nsample_burden.to_csv('sample_variant_burden.csv', index=False)\ngene_burden.to_csv('gene_level_burden.csv', index=False)\npathway_burden.to_csv('pathway_analysis.csv', index=False)\nvariant_report.to_csv('pathogenic_variant_report.csv', index=False)\n] PASSED [ 27%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineVisualizations::test_svg_visualization[clickstream_analysis-\nimport pandas as pd\nimport numpy as np\n\n# Load clickstream data\npage_views = pd.read_csv('page_views.csv')\nclick_events = pd.read_csv('click_events.csv')\nform_submissions = pd.read_csv('form_submissions.csv')\nsearch_queries = pd.read_csv('search_queries.csv')\nproduct_impressions = pd.read_csv('product_impressions.csv')\ncart_events = pd.read_csv('cart_events.csv')\nuser_agents = pd.read_csv('user_agents.csv')\ngeo_locations = pd.read_csv('geo_locations.csv')\n\n# Parse timestamps\npage_views['timestamp'] = pd.to_datetime(page_views['timestamp'])\nclick_events['timestamp'] = pd.to_datetime(click_events['timestamp'])\n\n# Combine all events\npage_views['event_type'] = 'page_view'\nclick_events['event_type'] = 'click'\nform_submissions['event_type'] = 'form_submit'\nsearch_queries['event_type'] = 'search'\ncart_events['event_type'] = cart_events['cart_action']\n\nall_events = pd.concat([\n    page_views[['session_id', 'user_id', 'timestamp', 'event_type', 'page_url', 'referrer']],\n    click_events[['session_id', 'user_id', 'timestamp', 'event_type', 'element_id', 'page_url']],\n    form_submissions[['session_id', 'user_id', 'timestamp', 'event_type', 'form_id', 'page_url']],\n    search_queries[['session_id', 'user_id', 'timestamp', 'event_type', 'query', 'results_count']],\n    cart_events[['session_id', 'user_id', 'timestamp', 'event_type', 'product_id', 'quantity']]\n], ignore_index=True, sort=False)\n\nall_events = all_events.sort_values(['session_id', 'timestamp'])\n\n# Session-level aggregations\nsession_metrics = all_events.groupby('session_id').agg({\n    'user_id': 'first',\n    'timestamp': ['min', 'max', 'count'],\n    'page_url': 'nunique',\n    'event_type': lambda x: x.value_counts().to_dict()\n}).reset_index()\n\nsession_metrics.columns = ['session_id', 'user_id', 'session_start',\n                            'session_end', 'total_events', 'unique_pages', 'event_breakdown']\n\n# Calculate session duration\nsession_metrics['session_duration_seconds'] = (\n    session_metrics['session_end'] - session_metrics['session_start']\n).dt.total_seconds()\n\n# Extract event counts\nsession_metrics['page_views'] = session_metrics['event_breakdown'].apply(\n    lambda x: x.get('page_view', 0) if isinstance(x, dict) else 0\n)\nsession_metrics['clicks'] = session_metrics['event_breakdown'].apply(\n    lambda x: x.get('click', 0) if isinstance(x, dict) else 0\n)\nsession_metrics['searches'] = session_metrics['event_breakdown'].apply(\n    lambda x: x.get('search', 0) if isinstance(x, dict) else 0\n)\nsession_metrics['cart_adds'] = session_metrics['event_breakdown'].apply(\n    lambda x: x.get('add_to_cart', 0) if isinstance(x, dict) else 0\n)\n\n# Calculate engagement metrics\nsession_metrics['pages_per_minute'] = (\n    session_metrics['unique_pages'] /\n    (session_metrics['session_duration_seconds'] / 60).replace(0, 1)\n)\nsession_metrics['events_per_page'] = (\n    session_metrics['total_events'] / session_metrics['unique_pages'].replace(0, 1)\n)\n\n# Identify bounce sessions\nsession_metrics['is_bounce'] = (\n    (session_metrics['unique_pages'] == 1) &\n    (session_metrics['session_duration_seconds'] < 10)\n).astype(int)\n\n# Create page flow sequences\npage_sequences = all_events[all_events['event_type'] == 'page_view'].copy()\npage_sequences['page_order'] = page_sequences.groupby('session_id').cumcount() + 1\npage_sequences['next_page'] = page_sequences.groupby('session_id')['page_url'].shift(-1)\n\n# Calculate page transitions\npage_transitions = page_sequences.groupby(['page_url', 'next_page']).size().reset_index(name='transition_count')\npage_transitions = page_transitions[page_transitions['next_page'].notna()]\n\n# Calculate exit rates per page\npage_stats = page_sequences.groupby('page_url').agg({\n    'session_id': 'count',\n    'next_page': lambda x: x.isna().sum()\n}).reset_index()\npage_stats.columns = ['page_url', 'total_views', 'exits']\npage_stats['exit_rate'] = page_stats['exits'] / page_stats['total_views']\n\n# Analyze search behavior\nsearch_analysis = search_queries.groupby('session_id').agg({\n    'query': ['count', lambda x: ' | '.join(x)],\n    'results_count': ['mean', 'min'],\n    'clicked_result': 'sum'\n}).reset_index()\nsearch_analysis.columns = ['session_id', 'search_count', 'search_queries',\n                            'avg_results', 'min_results', 'result_clicks']\nsearch_analysis['search_success_rate'] = (\n    search_analysis['result_clicks'] / search_analysis['search_count'].replace(0, 1)\n)\n\n# Analyze cart behavior\ncart_analysis = cart_events.groupby('session_id').agg({\n    'product_id': 'nunique',\n    'quantity': 'sum',\n    'cart_action': lambda x: (x == 'add_to_cart').sum()\n}).reset_index()\ncart_analysis.columns = ['session_id', 'unique_cart_products',\n                          'total_cart_quantity', 'add_to_cart_events']\n\n# Add purchase flag\npurchases = cart_events[cart_events['cart_action'] == 'purchase'].groupby('session_id').agg({\n    'order_value': 'sum'\n}).reset_index()\n\ncart_analysis = pd.merge(cart_analysis, purchases, on='session_id', how='left')\ncart_analysis['converted'] = cart_analysis['order_value'].notna().astype(int)\ncart_analysis['order_value'] = cart_analysis['order_value'].fillna(0)\n\n# Merge all session data\nsession_complete = session_metrics.copy()\nsession_complete = pd.merge(session_complete, search_analysis, on='session_id', how='left')\nsession_complete = pd.merge(session_complete, cart_analysis, on='session_id', how='left')\n\n# Add user agent info\nsession_complete = pd.merge(\n    session_complete,\n    user_agents[['session_id', 'device_type', 'browser', 'os', 'is_mobile']],\n    on='session_id',\n    how='left'\n)\n\n# Add geo location\nsession_complete = pd.merge(\n    session_complete,\n    geo_locations[['session_id', 'country', 'region', 'city', 'timezone']],\n    on='session_id',\n    how='left'\n)\n\n# Calculate user-level aggregations\nuser_metrics = session_complete.groupby('user_id').agg({\n    'session_id': 'count',\n    'session_duration_seconds': ['mean', 'sum'],\n    'page_views': ['mean', 'sum'],\n    'converted': 'sum',\n    'order_value': 'sum',\n    'session_start': ['min', 'max']\n}).reset_index()\n\nuser_metrics.columns = ['user_id', 'total_sessions', 'avg_session_duration',\n                        'total_time_spent', 'avg_pages_per_session', 'total_pages_viewed',\n                        'total_conversions', 'total_revenue', 'first_visit', 'last_visit']\n\nuser_metrics['conversion_rate'] = (\n    user_metrics['total_conversions'] / user_metrics['total_sessions']\n)\nuser_metrics['avg_order_value'] = (\n    user_metrics['total_revenue'] / user_metrics['total_conversions'].replace(0, 1)\n)\n\n# Segment users by engagement\nuser_metrics['engagement_segment'] = pd.cut(\n    user_metrics['total_time_spent'] / 3600,\n    bins=[-np.inf, 0.1, 1, 5, 20, np.inf],\n    labels=['Minimal', 'Light', 'Medium', 'Heavy', 'Power']\n)\n\n# Save outputs\nsession_complete.to_csv('session_analytics.csv', index=False)\nuser_metrics.to_csv('user_engagement_metrics.csv', index=False)\npage_stats.to_csv('page_performance.csv', index=False)\npage_transitions.to_csv('page_flow_analysis.csv', index=False)\n] PASSED [ 27%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineVisualizations::test_svg_visualization[portfolio_risk-\nimport pandas as pd\nimport numpy as np\n\n# Load financial data\npositions = pd.read_csv('positions.csv')\nprice_history = pd.read_csv('price_history.csv')\ninstruments = pd.read_csv('instruments.csv')\nmarket_data = pd.read_csv('market_data.csv')\nfx_rates = pd.read_csv('fx_rates.csv')\nbenchmark_returns = pd.read_csv('benchmark_returns.csv')\nfactor_exposures = pd.read_csv('factor_exposures.csv')\ncredit_ratings = pd.read_csv('credit_ratings.csv')\n\n# Parse dates\nprice_history['date'] = pd.to_datetime(price_history['date'])\nmarket_data['date'] = pd.to_datetime(market_data['date'])\nfx_rates['date'] = pd.to_datetime(fx_rates['date'])\n\n# Calculate daily returns\nprice_history = price_history.sort_values(['instrument_id', 'date'])\nprice_history['daily_return'] = price_history.groupby('instrument_id')['close_price'].pct_change()\nprice_history['log_return'] = np.log(\n    price_history['close_price'] / price_history.groupby('instrument_id')['close_price'].shift(1)\n)\n\n# Calculate rolling volatility\nprice_history['volatility_20d'] = price_history.groupby('instrument_id')['daily_return'].transform(\n    lambda x: x.rolling(20).std() * np.sqrt(252)\n)\nprice_history['volatility_60d'] = price_history.groupby('instrument_id')['daily_return'].transform(\n    lambda x: x.rolling(60).std() * np.sqrt(252)\n)\n\n# Calculate rolling beta to market\nmarket_returns = market_data[market_data['index_id'] == 'SPX'][['date', 'daily_return']].copy()\nmarket_returns.columns = ['date', 'market_return']\n\nprice_with_market = pd.merge(price_history, market_returns, on='date', how='left')\n\ndef calculate_beta(group):\n    if len(group) < 60:\n        return np.nan\n    cov = group['daily_return'].cov(group['market_return'])\n    var = group['market_return'].var()\n    return cov / var if var != 0 else np.nan\n\nprice_with_market['beta'] = price_with_market.groupby('instrument_id').apply(\n    lambda x: x['daily_return'].rolling(60).cov(x['market_return']) /\n              x['market_return'].rolling(60).var()\n).reset_index(level=0, drop=True)\n\n# Enrich positions with instrument details\npositions_enriched = pd.merge(\n    positions,\n    instruments[['instrument_id', 'instrument_type', 'currency', 'sector',\n                 'country', 'maturity_date', 'coupon_rate', 'issuer']],\n    on='instrument_id',\n    how='left'\n)\n\n# Get latest prices and metrics\nlatest_prices = price_with_market.groupby('instrument_id').last().reset_index()\npositions_enriched = pd.merge(\n    positions_enriched,\n    latest_prices[['instrument_id', 'close_price', 'daily_return',\n                   'volatility_20d', 'volatility_60d', 'beta']],\n    on='instrument_id',\n    how='left'\n)\n\n# Convert to base currency (USD)\nfx_latest = fx_rates.groupby('currency_pair').last().reset_index()\nfx_latest['to_currency'] = fx_latest['currency_pair'].str[-3:]\nfx_latest['from_currency'] = fx_latest['currency_pair'].str[:3]\n\npositions_enriched = pd.merge(\n    positions_enriched,\n    fx_latest[['from_currency', 'rate']].rename(columns={'from_currency': 'currency', 'rate': 'fx_rate'}),\n    on='currency',\n    how='left'\n)\npositions_enriched['fx_rate'] = positions_enriched['fx_rate'].fillna(1.0)\n\n# Calculate market values\npositions_enriched['market_value_local'] = positions_enriched['quantity'] * positions_enriched['close_price']\npositions_enriched['market_value_usd'] = positions_enriched['market_value_local'] / positions_enriched['fx_rate']\n\n# Calculate position-level risk metrics\npositions_enriched['position_volatility'] = (\n    positions_enriched['market_value_usd'] * positions_enriched['volatility_20d']\n)\n\n# Portfolio-level calculations\nportfolio_value = positions_enriched.groupby('portfolio_id')['market_value_usd'].sum().reset_index()\nportfolio_value.columns = ['portfolio_id', 'total_nav']\n\npositions_enriched = pd.merge(positions_enriched, portfolio_value, on='portfolio_id', how='left')\npositions_enriched['weight'] = positions_enriched['market_value_usd'] / positions_enriched['total_nav']\n\n# Calculate weighted portfolio beta\nportfolio_beta = positions_enriched.groupby('portfolio_id').apply(\n    lambda x: (x['weight'] * x['beta']).sum()\n).reset_index(name='portfolio_beta')\n\n# Calculate portfolio volatility (simplified - assuming no correlation)\nportfolio_vol_contrib = positions_enriched.groupby('portfolio_id').apply(\n    lambda x: np.sqrt((x['weight']**2 * x['volatility_20d']**2).sum())\n).reset_index(name='portfolio_volatility')\n\n# Sector concentration\nsector_concentration = positions_enriched.groupby(['portfolio_id', 'sector']).agg({\n    'market_value_usd': 'sum',\n    'weight': 'sum'\n}).reset_index()\nsector_concentration.columns = ['portfolio_id', 'sector', 'sector_value', 'sector_weight']\n\n# Country concentration\ncountry_concentration = positions_enriched.groupby(['portfolio_id', 'country']).agg({\n    'market_value_usd': 'sum',\n    'weight': 'sum'\n}).reset_index()\n\n# Add credit risk for fixed income\npositions_enriched = pd.merge(\n    positions_enriched,\n    credit_ratings[['issuer', 'rating', 'rating_numeric', 'default_probability']],\n    on='issuer',\n    how='left'\n)\n\n# Calculate credit VaR contribution\npositions_enriched['credit_var_contrib'] = (\n    positions_enriched['market_value_usd'] *\n    positions_enriched['default_probability'].fillna(0) *\n    (1 - 0.4)  # Assuming 40% recovery rate\n)\n\n# Calculate VaR (parametric approach)\nconfidence_level = 0.99\nz_score = 2.326  # 99% confidence\n\npositions_enriched['var_1d'] = (\n    positions_enriched['market_value_usd'] *\n    positions_enriched['volatility_20d'] / np.sqrt(252) *\n    z_score\n)\n\nportfolio_var = positions_enriched.groupby('portfolio_id')['var_1d'].sum().reset_index()\nportfolio_var.columns = ['portfolio_id', 'total_var_1d']\n\n# Historical VaR calculation\nreturns_pivot = price_history.pivot_table(\n    index='date',\n    columns='instrument_id',\n    values='daily_return'\n).fillna(0)\n\n# Calculate portfolio returns for each portfolio\nportfolio_returns = []\nfor portfolio_id in positions_enriched['portfolio_id'].unique():\n    port_positions = positions_enriched[positions_enriched['portfolio_id'] == portfolio_id]\n    weights = port_positions.set_index('instrument_id')['weight'].to_dict()\n\n    port_return = returns_pivot[list(weights.keys())].mul(\n        pd.Series(weights)\n    ).sum(axis=1)\n\n    portfolio_returns.append(pd.DataFrame({\n        'portfolio_id': portfolio_id,\n        'date': returns_pivot.index,\n        'portfolio_return': port_return\n    }))\n\nportfolio_returns_df = pd.concat(portfolio_returns, ignore_index=True)\n\n# Calculate historical VaR\nhistorical_var = portfolio_returns_df.groupby('portfolio_id')['portfolio_return'].apply(\n    lambda x: x.quantile(1 - confidence_level)\n).reset_index(name='historical_var_1d')\n\n# Calculate Sharpe ratio\nrisk_free_rate = 0.05 / 252  # Daily risk-free rate\n\nportfolio_sharpe = portfolio_returns_df.groupby('portfolio_id').agg({\n    'portfolio_return': ['mean', 'std']\n}).reset_index()\nportfolio_sharpe.columns = ['portfolio_id', 'avg_return', 'return_std']\nportfolio_sharpe['sharpe_ratio'] = (\n    (portfolio_sharpe['avg_return'] - risk_free_rate) / portfolio_sharpe['return_std']\n) * np.sqrt(252)\n\n# Compile portfolio risk summary\nportfolio_risk = portfolio_value.copy()\nportfolio_risk = pd.merge(portfolio_risk, portfolio_beta, on='portfolio_id', how='left')\nportfolio_risk = pd.merge(portfolio_risk, portfolio_vol_contrib, on='portfolio_id', how='left')\nportfolio_risk = pd.merge(portfolio_risk, portfolio_var, on='portfolio_id', how='left')\nportfolio_risk = pd.merge(portfolio_risk, historical_var, on='portfolio_id', how='left')\nportfolio_risk = pd.merge(portfolio_risk, portfolio_sharpe, on='portfolio_id', how='left')\n\n# Save outputs\npositions_enriched.to_csv('positions_with_risk.csv', index=False)\nportfolio_risk.to_csv('portfolio_risk_summary.csv', index=False)\nsector_concentration.to_csv('sector_concentration.csv', index=False)\ncountry_concentration.to_csv('country_concentration.csv', index=False)\nportfolio_returns_df.to_csv('portfolio_returns_history.csv', index=False)\n] PASSED [ 27%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineVisualizations::test_ascii_visualization[fraud_detection-\nimport pandas as pd\nimport numpy as np\n\n# Load transaction streams\ntransactions = pd.read_csv('transactions.csv')\nuser_profiles = pd.read_csv('user_profiles.csv')\ndevice_fingerprints = pd.read_csv('device_fingerprints.csv')\nmerchant_risk_scores = pd.read_csv('merchant_risk_scores.csv')\nhistorical_fraud = pd.read_csv('historical_fraud.csv')\nip_geolocation = pd.read_csv('ip_geolocation.csv')\n\n# Parse timestamps\ntransactions['timestamp'] = pd.to_datetime(transactions['timestamp'])\ntransactions['date'] = transactions['timestamp'].dt.date\ntransactions['hour'] = transactions['timestamp'].dt.hour\ntransactions['minute'] = transactions['timestamp'].dt.minute\ntransactions['day_of_week'] = transactions['timestamp'].dt.dayofweek\ntransactions['is_weekend'] = transactions['day_of_week'].isin([5, 6]).astype(int)\ntransactions['is_night'] = transactions['hour'].between(0, 6).astype(int)\n\n# Enrich with user profile data\ntransactions_enriched = pd.merge(\n    transactions,\n    user_profiles[['user_id', 'account_age_days', 'verification_level',\n                   'avg_transaction_amount', 'typical_login_hour',\n                   'home_country', 'risk_tier']],\n    on='user_id',\n    how='left'\n)\n\n# Add device fingerprint risk\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    device_fingerprints[['device_id', 'device_trust_score', 'is_known_device',\n                         'browser_anomaly_score', 'last_seen_country']],\n    on='device_id',\n    how='left'\n)\n\n# Add merchant risk scores\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    merchant_risk_scores[['merchant_id', 'merchant_risk_score',\n                          'chargeback_rate', 'merchant_category']],\n    on='merchant_id',\n    how='left'\n)\n\n# Add IP geolocation\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    ip_geolocation[['ip_address', 'ip_country', 'ip_city',\n                    'is_vpn', 'is_tor', 'ip_risk_score']],\n    on='ip_address',\n    how='left'\n)\n\n# Calculate velocity features - transactions per user in last N minutes\ntransactions_enriched = transactions_enriched.sort_values(['user_id', 'timestamp'])\n\n# Rolling window aggregations per user\nuser_velocity = transactions_enriched.groupby('user_id').agg({\n    'transaction_id': 'count',\n    'amount': ['sum', 'mean', 'std', 'max'],\n    'merchant_id': 'nunique',\n    'ip_address': 'nunique',\n    'device_id': 'nunique'\n}).reset_index()\nuser_velocity.columns = ['user_id', 'tx_count_session', 'total_amount_session',\n                         'avg_amount_session', 'std_amount_session', 'max_amount_session',\n                         'unique_merchants', 'unique_ips', 'unique_devices']\n\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    user_velocity,\n    on='user_id',\n    how='left'\n)\n\n# Calculate deviation from user's typical behavior\ntransactions_enriched['amount_deviation'] = (\n    (transactions_enriched['amount'] - transactions_enriched['avg_transaction_amount']) /\n    transactions_enriched['avg_transaction_amount'].replace(0, 1)\n)\n\ntransactions_enriched['hour_deviation'] = abs(\n    transactions_enriched['hour'] - transactions_enriched['typical_login_hour']\n)\n\n# Geographic anomaly detection\ntransactions_enriched['country_mismatch'] = (\n    transactions_enriched['ip_country'] != transactions_enriched['home_country']\n).astype(int)\n\ntransactions_enriched['device_country_mismatch'] = (\n    transactions_enriched['ip_country'] != transactions_enriched['last_seen_country']\n).astype(int)\n\n# Composite risk score calculation\ntransactions_enriched['composite_risk_score'] = (\n    transactions_enriched['ip_risk_score'] * 0.2 +\n    transactions_enriched['merchant_risk_score'] * 0.15 +\n    (1 - transactions_enriched['device_trust_score']) * 0.2 +\n    transactions_enriched['amount_deviation'].clip(0, 5) * 0.15 +\n    transactions_enriched['country_mismatch'] * 0.15 +\n    transactions_enriched['is_vpn'] * 0.1 +\n    transactions_enriched['is_tor'] * 0.05\n)\n\n# Flag high-risk transactions\ntransactions_enriched['is_high_risk'] = (\n    transactions_enriched['composite_risk_score'] > 0.7\n).astype(int)\n\n# Join historical fraud patterns\nfraud_patterns = historical_fraud.groupby('user_id').agg({\n    'fraud_flag': 'sum',\n    'transaction_id': 'count'\n}).reset_index()\nfraud_patterns.columns = ['user_id', 'historical_fraud_count', 'historical_tx_count']\nfraud_patterns['historical_fraud_rate'] = (\n    fraud_patterns['historical_fraud_count'] / fraud_patterns['historical_tx_count']\n)\n\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    fraud_patterns[['user_id', 'historical_fraud_rate']],\n    on='user_id',\n    how='left'\n)\ntransactions_enriched['historical_fraud_rate'] = transactions_enriched['historical_fraud_rate'].fillna(0)\n\n# Final feature selection for ML model\nml_features = transactions_enriched[[\n    'transaction_id', 'user_id', 'amount', 'amount_deviation',\n    'hour_deviation', 'is_weekend', 'is_night',\n    'device_trust_score', 'is_known_device', 'browser_anomaly_score',\n    'merchant_risk_score', 'chargeback_rate',\n    'ip_risk_score', 'is_vpn', 'is_tor',\n    'country_mismatch', 'device_country_mismatch',\n    'tx_count_session', 'unique_merchants', 'unique_ips', 'unique_devices',\n    'composite_risk_score', 'historical_fraud_rate', 'is_high_risk'\n]].copy()\n\nml_features.to_csv('fraud_detection_features.csv', index=False)\ntransactions_enriched.to_csv('transactions_enriched_full.csv', index=False)\n] PASSED [ 27%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineVisualizations::test_ascii_visualization[customer_360-\nimport pandas as pd\nimport numpy as np\n\n# Load data from multiple source systems\ncrm_customers = pd.read_csv('crm_customers.csv')\necommerce_users = pd.read_csv('ecommerce_users.csv')\nmobile_app_users = pd.read_csv('mobile_app_users.csv')\ncall_center_interactions = pd.read_csv('call_center_interactions.csv')\nemail_campaigns = pd.read_csv('email_campaigns.csv')\nweb_analytics = pd.read_csv('web_analytics.csv')\nsocial_media = pd.read_csv('social_media.csv')\nloyalty_program = pd.read_csv('loyalty_program.csv')\nsupport_tickets = pd.read_csv('support_tickets.csv')\nnps_surveys = pd.read_csv('nps_surveys.csv')\n\n# Standardize customer identifiers across systems\ncrm_customers['source'] = 'crm'\ncrm_customers['master_id'] = crm_customers['crm_id']\n\necommerce_users['source'] = 'ecommerce'\necommerce_users = pd.merge(\n    ecommerce_users,\n    crm_customers[['email', 'crm_id']],\n    on='email',\n    how='left'\n)\necommerce_users['master_id'] = ecommerce_users['crm_id'].fillna(\n    'ECO_' + ecommerce_users['ecommerce_user_id'].astype(str)\n)\n\nmobile_app_users['source'] = 'mobile'\nmobile_app_users = pd.merge(\n    mobile_app_users,\n    crm_customers[['phone', 'crm_id']],\n    on='phone',\n    how='left'\n)\nmobile_app_users['master_id'] = mobile_app_users['crm_id'].fillna(\n    'MOB_' + mobile_app_users['app_user_id'].astype(str)\n)\n\n# Create unified customer profile\ncustomer_base = crm_customers[['master_id', 'email', 'phone', 'first_name',\n                               'last_name', 'date_of_birth', 'gender',\n                               'address', 'city', 'state', 'country',\n                               'customer_since']].copy()\n\n# Aggregate e-commerce behavior\necommerce_metrics = ecommerce_users.groupby('master_id').agg({\n    'order_id': 'count',\n    'order_total': ['sum', 'mean', 'max'],\n    'items_purchased': 'sum',\n    'last_order_date': 'max',\n    'cart_abandonment_count': 'sum'\n}).reset_index()\necommerce_metrics.columns = ['master_id', 'ecom_order_count', 'ecom_total_spend',\n                              'ecom_avg_order', 'ecom_max_order', 'ecom_items_total',\n                              'ecom_last_order', 'ecom_cart_abandonments']\n\n# Aggregate mobile app engagement\nmobile_metrics = mobile_app_users.groupby('master_id').agg({\n    'session_count': 'sum',\n    'total_time_minutes': 'sum',\n    'push_notifications_clicked': 'sum',\n    'app_crashes': 'sum',\n    'features_used': lambda x: len(set(','.join(x.dropna()).split(','))),\n    'last_active_date': 'max'\n}).reset_index()\nmobile_metrics.columns = ['master_id', 'app_sessions', 'app_time_minutes',\n                          'push_clicks', 'app_crashes', 'features_used_count',\n                          'app_last_active']\n\n# Aggregate call center interactions\ncall_metrics = call_center_interactions.groupby('master_id').agg({\n    'call_id': 'count',\n    'call_duration_seconds': ['sum', 'mean'],\n    'issue_resolved': 'mean',\n    'escalated': 'sum',\n    'sentiment_score': 'mean'\n}).reset_index()\ncall_metrics.columns = ['master_id', 'call_count', 'total_call_duration',\n                        'avg_call_duration', 'resolution_rate',\n                        'escalation_count', 'call_sentiment']\n\n# Aggregate email engagement\nemail_metrics = email_campaigns.groupby('master_id').agg({\n    'email_sent': 'sum',\n    'email_opened': 'sum',\n    'email_clicked': 'sum',\n    'unsubscribed': 'max'\n}).reset_index()\nemail_metrics['email_open_rate'] = email_metrics['email_opened'] / email_metrics['email_sent']\nemail_metrics['email_click_rate'] = email_metrics['email_clicked'] / email_metrics['email_opened'].replace(0, 1)\n\n# Aggregate web analytics\nweb_metrics = web_analytics.groupby('master_id').agg({\n    'page_views': 'sum',\n    'unique_sessions': 'sum',\n    'bounce_rate': 'mean',\n    'avg_session_duration': 'mean',\n    'conversion_events': 'sum'\n}).reset_index()\n\n# Aggregate loyalty program data\nloyalty_metrics = loyalty_program.groupby('master_id').agg({\n    'points_earned': 'sum',\n    'points_redeemed': 'sum',\n    'tier_level': 'max',\n    'referrals_made': 'sum'\n}).reset_index()\nloyalty_metrics['points_balance'] = loyalty_metrics['points_earned'] - loyalty_metrics['points_redeemed']\n\n# Aggregate support tickets\nsupport_metrics = support_tickets.groupby('master_id').agg({\n    'ticket_id': 'count',\n    'resolution_time_hours': 'mean',\n    'satisfaction_rating': 'mean',\n    'ticket_reopened': 'sum'\n}).reset_index()\nsupport_metrics.columns = ['master_id', 'ticket_count', 'avg_resolution_time',\n                           'support_satisfaction', 'tickets_reopened']\n\n# Aggregate NPS surveys\nnps_metrics = nps_surveys.groupby('master_id').agg({\n    'nps_score': 'mean',\n    'survey_id': 'count',\n    'would_recommend': 'mean'\n}).reset_index()\nnps_metrics.columns = ['master_id', 'avg_nps_score', 'surveys_completed', 'recommend_rate']\n\n# Merge all metrics into unified customer 360 view\ncustomer_360 = customer_base.copy()\n\nfor metrics_df in [ecommerce_metrics, mobile_metrics, call_metrics,\n                   email_metrics, web_metrics, loyalty_metrics,\n                   support_metrics, nps_metrics]:\n    customer_360 = pd.merge(customer_360, metrics_df, on='master_id', how='left')\n\n# Fill missing values appropriately\nnumeric_cols = customer_360.select_dtypes(include=[np.number]).columns\ncustomer_360[numeric_cols] = customer_360[numeric_cols].fillna(0)\n\n# Calculate derived metrics\ncustomer_360['total_interactions'] = (\n    customer_360['ecom_order_count'] +\n    customer_360['app_sessions'] +\n    customer_360['call_count'] +\n    customer_360['ticket_count']\n)\n\ncustomer_360['digital_engagement_score'] = (\n    customer_360['app_sessions'] * 0.3 +\n    customer_360['email_open_rate'] * 20 +\n    customer_360['page_views'] * 0.1 +\n    customer_360['push_clicks'] * 0.5\n).clip(0, 100)\n\ncustomer_360['customer_health_score'] = (\n    customer_360['avg_nps_score'] / 10 * 25 +\n    customer_360['resolution_rate'] * 25 +\n    customer_360['support_satisfaction'] / 5 * 25 +\n    (1 - customer_360['escalation_count'].clip(0, 5) / 5) * 25\n)\n\n# Customer segmentation\ncustomer_360['value_segment'] = pd.cut(\n    customer_360['ecom_total_spend'],\n    bins=[-np.inf, 100, 500, 2000, 10000, np.inf],\n    labels=['Dormant', 'Bronze', 'Silver', 'Gold', 'Platinum']\n)\n\ncustomer_360['engagement_segment'] = pd.cut(\n    customer_360['digital_engagement_score'],\n    bins=[-np.inf, 20, 40, 60, 80, np.inf],\n    labels=['Inactive', 'Low', 'Medium', 'High', 'Power User']\n)\n\n# Calculate churn risk\ncustomer_360['days_since_activity'] = (\n    pd.to_datetime('today') - pd.to_datetime(customer_360['ecom_last_order'])\n).dt.days.fillna(999)\n\ncustomer_360['churn_risk_score'] = (\n    customer_360['days_since_activity'] / 365 * 30 +\n    (1 - customer_360['email_open_rate'].fillna(0)) * 20 +\n    customer_360['tickets_reopened'] * 10 +\n    (10 - customer_360['avg_nps_score'].fillna(5)) * 4\n).clip(0, 100)\n\n# Save outputs\ncustomer_360.to_csv('customer_360_unified.csv', index=False)\n] PASSED [ 27%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineVisualizations::test_ascii_visualization[supply_chain-\nimport pandas as pd\nimport numpy as np\n\n# Load supply chain data\nsales_history = pd.read_csv('sales_history.csv')\ninventory_levels = pd.read_csv('inventory_levels.csv')\nsupplier_data = pd.read_csv('supplier_data.csv')\nwarehouse_locations = pd.read_csv('warehouse_locations.csv')\nshipping_costs = pd.read_csv('shipping_costs.csv')\nproduct_catalog = pd.read_csv('product_catalog.csv')\npromotions_calendar = pd.read_csv('promotions_calendar.csv')\nweather_data = pd.read_csv('weather_data.csv')\neconomic_indicators = pd.read_csv('economic_indicators.csv')\n\n# Parse dates\nsales_history['sale_date'] = pd.to_datetime(sales_history['sale_date'])\nsales_history['year'] = sales_history['sale_date'].dt.year\nsales_history['month'] = sales_history['sale_date'].dt.month\nsales_history['week'] = sales_history['sale_date'].dt.isocalendar().week\nsales_history['day_of_week'] = sales_history['sale_date'].dt.dayofweek\nsales_history['is_weekend'] = sales_history['day_of_week'].isin([5, 6]).astype(int)\n\n# Enrich sales with product info\nsales_enriched = pd.merge(\n    sales_history,\n    product_catalog[['product_id', 'category', 'subcategory', 'brand',\n                     'unit_cost', 'weight_kg', 'is_perishable', 'shelf_life_days']],\n    on='product_id',\n    how='left'\n)\n\n# Add promotion flags\npromotions_calendar['promo_date'] = pd.to_datetime(promotions_calendar['promo_date'])\nsales_enriched = pd.merge(\n    sales_enriched,\n    promotions_calendar[['product_id', 'promo_date', 'discount_pct', 'promo_type']],\n    left_on=['product_id', 'sale_date'],\n    right_on=['product_id', 'promo_date'],\n    how='left'\n)\nsales_enriched['is_promotion'] = sales_enriched['discount_pct'].notna().astype(int)\nsales_enriched['discount_pct'] = sales_enriched['discount_pct'].fillna(0)\n\n# Add weather impact\nweather_data['date'] = pd.to_datetime(weather_data['date'])\nsales_enriched = pd.merge(\n    sales_enriched,\n    weather_data[['date', 'region', 'temperature', 'precipitation', 'weather_condition']],\n    left_on=['sale_date', 'region'],\n    right_on=['date', 'region'],\n    how='left'\n)\n\n# Calculate time-based aggregations for demand patterns\ndaily_demand = sales_enriched.groupby(['product_id', 'warehouse_id', 'sale_date']).agg({\n    'quantity': 'sum',\n    'revenue': 'sum',\n    'is_promotion': 'max',\n    'temperature': 'mean'\n}).reset_index()\n\n# Calculate rolling averages for demand forecasting\ndaily_demand = daily_demand.sort_values(['product_id', 'warehouse_id', 'sale_date'])\ndaily_demand['demand_7d_avg'] = daily_demand.groupby(['product_id', 'warehouse_id'])['quantity'].transform(\n    lambda x: x.rolling(7, min_periods=1).mean()\n)\ndaily_demand['demand_30d_avg'] = daily_demand.groupby(['product_id', 'warehouse_id'])['quantity'].transform(\n    lambda x: x.rolling(30, min_periods=1).mean()\n)\ndaily_demand['demand_90d_avg'] = daily_demand.groupby(['product_id', 'warehouse_id'])['quantity'].transform(\n    lambda x: x.rolling(90, min_periods=1).mean()\n)\n\n# Calculate demand volatility\ndaily_demand['demand_7d_std'] = daily_demand.groupby(['product_id', 'warehouse_id'])['quantity'].transform(\n    lambda x: x.rolling(7, min_periods=1).std()\n)\ndaily_demand['demand_coefficient_variation'] = (\n    daily_demand['demand_7d_std'] / daily_demand['demand_7d_avg'].replace(0, 1)\n)\n\n# Seasonality detection\nmonthly_patterns = sales_enriched.groupby(['product_id', 'month']).agg({\n    'quantity': 'mean'\n}).reset_index()\nmonthly_patterns.columns = ['product_id', 'month', 'monthly_avg_demand']\n\nyearly_avg = monthly_patterns.groupby('product_id')['monthly_avg_demand'].transform('mean')\nmonthly_patterns['seasonality_index'] = monthly_patterns['monthly_avg_demand'] / yearly_avg\n\n# Join current inventory levels\ninventory_current = inventory_levels.groupby(['product_id', 'warehouse_id']).agg({\n    'quantity_on_hand': 'sum',\n    'quantity_reserved': 'sum',\n    'quantity_in_transit': 'sum',\n    'last_restock_date': 'max'\n}).reset_index()\ninventory_current['available_inventory'] = (\n    inventory_current['quantity_on_hand'] - inventory_current['quantity_reserved']\n)\n\n# Calculate days of supply\ninventory_analysis = pd.merge(\n    inventory_current,\n    daily_demand.groupby(['product_id', 'warehouse_id']).agg({\n        'demand_30d_avg': 'last'\n    }).reset_index(),\n    on=['product_id', 'warehouse_id'],\n    how='left'\n)\ninventory_analysis['days_of_supply'] = (\n    inventory_analysis['available_inventory'] /\n    inventory_analysis['demand_30d_avg'].replace(0, 0.1)\n)\n\n# Add supplier lead times\ninventory_analysis = pd.merge(\n    inventory_analysis,\n    supplier_data[['product_id', 'supplier_id', 'lead_time_days',\n                   'min_order_qty', 'unit_cost', 'reliability_score']],\n    on='product_id',\n    how='left'\n)\n\n# Calculate reorder points with safety stock\ninventory_analysis['safety_stock'] = (\n    inventory_analysis['demand_30d_avg'] *\n    inventory_analysis['lead_time_days'] * 0.5  # Safety factor\n)\ninventory_analysis['reorder_point'] = (\n    inventory_analysis['demand_30d_avg'] * inventory_analysis['lead_time_days'] +\n    inventory_analysis['safety_stock']\n)\n\n# Flag items needing reorder\ninventory_analysis['needs_reorder'] = (\n    inventory_analysis['available_inventory'] < inventory_analysis['reorder_point']\n).astype(int)\n\n# Calculate optimal order quantity (EOQ approximation)\ninventory_analysis['holding_cost_annual'] = inventory_analysis['unit_cost'] * 0.25\ninventory_analysis['order_cost'] = 50  # Fixed ordering cost\ninventory_analysis['annual_demand'] = inventory_analysis['demand_30d_avg'] * 12 * 30\n\ninventory_analysis['economic_order_qty'] = np.sqrt(\n    2 * inventory_analysis['annual_demand'] * inventory_analysis['order_cost'] /\n    inventory_analysis['holding_cost_annual'].replace(0, 1)\n)\n\n# Calculate shipping optimization\nshipping_analysis = pd.merge(\n    inventory_analysis,\n    warehouse_locations[['warehouse_id', 'latitude', 'longitude', 'capacity_units']],\n    on='warehouse_id',\n    how='left'\n)\n\n# Add shipping costs\nshipping_analysis = pd.merge(\n    shipping_analysis,\n    shipping_costs,\n    on='warehouse_id',\n    how='left'\n)\n\n# Calculate total landed cost\nshipping_analysis['landed_cost'] = (\n    shipping_analysis['unit_cost'] +\n    shipping_analysis['shipping_cost_per_unit'] +\n    shipping_analysis['handling_cost']\n)\n\n# Identify stock-out risk\nshipping_analysis['stockout_risk'] = np.where(\n    shipping_analysis['days_of_supply'] < shipping_analysis['lead_time_days'],\n    'High',\n    np.where(\n        shipping_analysis['days_of_supply'] < shipping_analysis['lead_time_days'] * 1.5,\n        'Medium',\n        'Low'\n    )\n)\n\n# Save outputs\ndaily_demand.to_csv('demand_forecast_features.csv', index=False)\ninventory_analysis.to_csv('inventory_optimization.csv', index=False)\nshipping_analysis.to_csv('supply_chain_analysis.csv', index=False)\nmonthly_patterns.to_csv('seasonality_patterns.csv', index=False)\n] PASSED [ 27%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineVisualizations::test_ascii_visualization[marketing_attribution-\nimport pandas as pd\nimport numpy as np\n\n# Load marketing data\ntouchpoints = pd.read_csv('touchpoints.csv')\nconversions = pd.read_csv('conversions.csv')\nad_spend = pd.read_csv('ad_spend.csv')\nchannel_costs = pd.read_csv('channel_costs.csv')\ncampaign_metadata = pd.read_csv('campaign_metadata.csv')\ncustomer_journeys = pd.read_csv('customer_journeys.csv')\norganic_traffic = pd.read_csv('organic_traffic.csv')\noffline_media = pd.read_csv('offline_media.csv')\n\n# Parse timestamps\ntouchpoints['touchpoint_time'] = pd.to_datetime(touchpoints['touchpoint_time'])\nconversions['conversion_time'] = pd.to_datetime(conversions['conversion_time'])\n\n# Create customer journey sequences\ntouchpoints_sorted = touchpoints.sort_values(['customer_id', 'touchpoint_time'])\n\n# Assign journey IDs (new journey if gap > 30 days)\ntouchpoints_sorted['time_since_last'] = touchpoints_sorted.groupby('customer_id')['touchpoint_time'].diff()\ntouchpoints_sorted['new_journey'] = (\n    touchpoints_sorted['time_since_last'] > pd.Timedelta(days=30)\n).fillna(True).astype(int)\ntouchpoints_sorted['journey_id'] = touchpoints_sorted.groupby('customer_id')['new_journey'].cumsum()\n\n# Create journey-level aggregations\njourney_touchpoints = touchpoints_sorted.groupby(['customer_id', 'journey_id']).agg({\n    'touchpoint_id': 'count',\n    'channel': lambda x: ' > '.join(x),\n    'campaign_id': lambda x: list(x.unique()),\n    'touchpoint_time': ['min', 'max'],\n    'cost': 'sum'\n}).reset_index()\njourney_touchpoints.columns = ['customer_id', 'journey_id', 'touchpoint_count',\n                                'channel_path', 'campaigns_touched',\n                                'journey_start', 'journey_end', 'journey_cost']\n\n# Calculate journey duration\njourney_touchpoints['journey_duration_days'] = (\n    journey_touchpoints['journey_end'] - journey_touchpoints['journey_start']\n).dt.total_seconds() / 86400\n\n# Join with conversions\njourney_conversions = pd.merge(\n    journey_touchpoints,\n    conversions[['customer_id', 'conversion_time', 'conversion_value', 'product_category']],\n    on='customer_id',\n    how='left'\n)\n\n# Filter to conversions within journey window\njourney_conversions['converted'] = (\n    (journey_conversions['conversion_time'] >= journey_conversions['journey_start']) &\n    (journey_conversions['conversion_time'] <= journey_conversions['journey_end'] + pd.Timedelta(days=7))\n).astype(int)\n\njourney_conversions['conversion_value'] = np.where(\n    journey_conversions['converted'] == 1,\n    journey_conversions['conversion_value'],\n    0\n)\n\n# Calculate attribution models\n\n# 1. First-touch attribution\ntouchpoints_sorted['is_first_touch'] = touchpoints_sorted.groupby(\n    ['customer_id', 'journey_id']\n).cumcount() == 0\n\nfirst_touch = touchpoints_sorted[touchpoints_sorted['is_first_touch']].copy()\nfirst_touch_attribution = first_touch.groupby('channel').agg({\n    'touchpoint_id': 'count',\n    'cost': 'sum'\n}).reset_index()\nfirst_touch_attribution.columns = ['channel', 'first_touch_count', 'first_touch_cost']\n\n# 2. Last-touch attribution\nlast_touch = touchpoints_sorted.groupby(['customer_id', 'journey_id']).last().reset_index()\nlast_touch_attribution = last_touch.groupby('channel').agg({\n    'touchpoint_id': 'count',\n    'cost': 'sum'\n}).reset_index()\nlast_touch_attribution.columns = ['channel', 'last_touch_count', 'last_touch_cost']\n\n# 3. Linear attribution (equal credit to all touchpoints)\njourney_conversions_exploded = journey_conversions.explode('campaigns_touched')\njourney_conversions_exploded['linear_credit'] = (\n    journey_conversions_exploded['conversion_value'] /\n    journey_conversions_exploded['touchpoint_count']\n)\n\n# 4. Time-decay attribution\ntouchpoints_with_conversion = pd.merge(\n    touchpoints_sorted,\n    journey_conversions[['customer_id', 'journey_id', 'conversion_time', 'conversion_value', 'converted']],\n    on=['customer_id', 'journey_id'],\n    how='left'\n)\n\ntouchpoints_with_conversion['days_to_conversion'] = (\n    touchpoints_with_conversion['conversion_time'] - touchpoints_with_conversion['touchpoint_time']\n).dt.total_seconds() / 86400\n\n# Exponential decay weight (half-life = 7 days)\ntouchpoints_with_conversion['decay_weight'] = np.exp(\n    -touchpoints_with_conversion['days_to_conversion'] / 7\n)\n\n# Normalize weights within journey\ntouchpoints_with_conversion['weight_sum'] = touchpoints_with_conversion.groupby(\n    ['customer_id', 'journey_id']\n)['decay_weight'].transform('sum')\n\ntouchpoints_with_conversion['time_decay_credit'] = (\n    touchpoints_with_conversion['decay_weight'] /\n    touchpoints_with_conversion['weight_sum'].replace(0, 1) *\n    touchpoints_with_conversion['conversion_value']\n)\n\n# 5. Position-based attribution (40% first, 40% last, 20% middle)\ntouchpoints_with_conversion['position'] = touchpoints_with_conversion.groupby(\n    ['customer_id', 'journey_id']\n).cumcount() + 1\n\ntouchpoints_with_conversion['total_positions'] = touchpoints_with_conversion.groupby(\n    ['customer_id', 'journey_id']\n)['position'].transform('max')\n\ntouchpoints_with_conversion['position_weight'] = np.where(\n    touchpoints_with_conversion['position'] == 1,\n    0.4,\n    np.where(\n        touchpoints_with_conversion['position'] == touchpoints_with_conversion['total_positions'],\n        0.4,\n        0.2 / (touchpoints_with_conversion['total_positions'] - 2).replace(0, 1)\n    )\n)\n\ntouchpoints_with_conversion['position_based_credit'] = (\n    touchpoints_with_conversion['position_weight'] *\n    touchpoints_with_conversion['conversion_value']\n)\n\n# Aggregate attribution by channel\nchannel_attribution = touchpoints_with_conversion.groupby('channel').agg({\n    'touchpoint_id': 'count',\n    'cost': 'sum',\n    'linear_credit': 'sum',\n    'time_decay_credit': 'sum',\n    'position_based_credit': 'sum'\n}).reset_index()\n\nchannel_attribution = pd.merge(channel_attribution, first_touch_attribution, on='channel', how='left')\nchannel_attribution = pd.merge(channel_attribution, last_touch_attribution, on='channel', how='left')\n\n# Calculate ROI by attribution model\nchannel_attribution['roi_linear'] = (\n    channel_attribution['linear_credit'] - channel_attribution['cost']\n) / channel_attribution['cost'].replace(0, 1)\n\nchannel_attribution['roi_time_decay'] = (\n    channel_attribution['time_decay_credit'] - channel_attribution['cost']\n) / channel_attribution['cost'].replace(0, 1)\n\nchannel_attribution['roi_position'] = (\n    channel_attribution['position_based_credit'] - channel_attribution['cost']\n) / channel_attribution['cost'].replace(0, 1)\n\n# Marketing mix modeling aggregations\ndaily_spend = ad_spend.groupby(['date', 'channel']).agg({\n    'spend': 'sum',\n    'impressions': 'sum',\n    'clicks': 'sum'\n}).reset_index()\n\ndaily_spend['cpm'] = daily_spend['spend'] / daily_spend['impressions'] * 1000\ndaily_spend['cpc'] = daily_spend['spend'] / daily_spend['clicks'].replace(0, 1)\ndaily_spend['ctr'] = daily_spend['clicks'] / daily_spend['impressions'].replace(0, 1)\n\n# Add adstock transformation (carryover effect)\ndaily_spend = daily_spend.sort_values(['channel', 'date'])\ndaily_spend['spend_adstock'] = daily_spend.groupby('channel')['spend'].transform(\n    lambda x: x.ewm(halflife=7).mean()\n)\n\n# Save outputs\njourney_touchpoints.to_csv('customer_journeys_analyzed.csv', index=False)\nchannel_attribution.to_csv('channel_attribution_models.csv', index=False)\ntouchpoints_with_conversion.to_csv('touchpoint_attribution_detail.csv', index=False)\ndaily_spend.to_csv('marketing_mix_features.csv', index=False)\n] PASSED [ 27%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineVisualizations::test_ascii_visualization[iot_predictive_maintenance-\nimport pandas as pd\nimport numpy as np\n\n# Load IoT sensor data\nsensor_readings = pd.read_csv('sensor_readings.csv')\nequipment_registry = pd.read_csv('equipment_registry.csv')\nmaintenance_history = pd.read_csv('maintenance_history.csv')\nfailure_logs = pd.read_csv('failure_logs.csv')\noperating_conditions = pd.read_csv('operating_conditions.csv')\nparts_inventory = pd.read_csv('parts_inventory.csv')\ntechnician_schedules = pd.read_csv('technician_schedules.csv')\n\n# Parse timestamps\nsensor_readings['reading_time'] = pd.to_datetime(sensor_readings['reading_time'])\nmaintenance_history['maintenance_date'] = pd.to_datetime(maintenance_history['maintenance_date'])\nfailure_logs['failure_time'] = pd.to_datetime(failure_logs['failure_time'])\n\n# Clean sensor data - remove outliers\nsensor_readings = sensor_readings.sort_values(['equipment_id', 'sensor_type', 'reading_time'])\n\n# Calculate rolling statistics per sensor\nsensor_readings['value_1h_mean'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].transform(lambda x: x.rolling('1H', on=sensor_readings['reading_time']).mean())\n\nsensor_readings['value_1h_std'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].transform(lambda x: x.rolling('1H', on=sensor_readings['reading_time']).std())\n\nsensor_readings['value_24h_mean'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].transform(lambda x: x.rolling('24H', on=sensor_readings['reading_time']).mean())\n\nsensor_readings['value_24h_max'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].transform(lambda x: x.rolling('24H', on=sensor_readings['reading_time']).max())\n\nsensor_readings['value_24h_min'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].transform(lambda x: x.rolling('24H', on=sensor_readings['reading_time']).min())\n\n# Calculate rate of change\nsensor_readings['value_diff'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].diff()\n\nsensor_readings['value_pct_change'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].pct_change()\n\n# Detect anomalies using z-score\nsensor_readings['z_score'] = (\n    (sensor_readings['sensor_value'] - sensor_readings['value_24h_mean']) /\n    sensor_readings['value_1h_std'].replace(0, 1)\n)\nsensor_readings['is_anomaly'] = (abs(sensor_readings['z_score']) > 3).astype(int)\n\n# Pivot sensor types to create feature vectors\nsensor_features = sensor_readings.pivot_table(\n    index=['equipment_id', 'reading_time'],\n    columns='sensor_type',\n    values=['sensor_value', 'value_1h_mean', 'value_24h_max', 'z_score', 'is_anomaly'],\n    aggfunc='first'\n).reset_index()\n\n# Flatten column names\nsensor_features.columns = ['_'.join(str(c) for c in col).strip('_')\n                           for col in sensor_features.columns]\n\n# Add equipment metadata\nsensor_features = pd.merge(\n    sensor_features,\n    equipment_registry[['equipment_id', 'equipment_type', 'manufacturer',\n                        'installation_date', 'rated_capacity', 'location']],\n    on='equipment_id',\n    how='left'\n)\n\n# Calculate equipment age\nsensor_features['installation_date'] = pd.to_datetime(sensor_features['installation_date'])\nsensor_features['equipment_age_days'] = (\n    sensor_features['reading_time'] - sensor_features['installation_date']\n).dt.days\n\n# Add operating conditions\noperating_conditions['condition_time'] = pd.to_datetime(operating_conditions['condition_time'])\nsensor_features = pd.merge_asof(\n    sensor_features.sort_values('reading_time'),\n    operating_conditions.sort_values('condition_time'),\n    left_on='reading_time',\n    right_on='condition_time',\n    by='equipment_id',\n    direction='backward'\n)\n\n# Calculate maintenance history features\nmaintenance_agg = maintenance_history.groupby('equipment_id').agg({\n    'maintenance_id': 'count',\n    'maintenance_date': 'max',\n    'maintenance_cost': 'sum',\n    'downtime_hours': 'sum'\n}).reset_index()\nmaintenance_agg.columns = ['equipment_id', 'total_maintenance_count',\n                           'last_maintenance_date', 'total_maintenance_cost',\n                           'total_downtime_hours']\n\nsensor_features = pd.merge(\n    sensor_features,\n    maintenance_agg,\n    on='equipment_id',\n    how='left'\n)\n\n# Calculate days since last maintenance\nsensor_features['last_maintenance_date'] = pd.to_datetime(sensor_features['last_maintenance_date'])\nsensor_features['days_since_maintenance'] = (\n    sensor_features['reading_time'] - sensor_features['last_maintenance_date']\n).dt.days\n\n# Calculate failure history features\nfailure_agg = failure_logs.groupby('equipment_id').agg({\n    'failure_id': 'count',\n    'failure_time': 'max',\n    'failure_severity': 'mean',\n    'repair_time_hours': 'sum'\n}).reset_index()\nfailure_agg.columns = ['equipment_id', 'total_failures', 'last_failure_date',\n                       'avg_failure_severity', 'total_repair_hours']\n\nsensor_features = pd.merge(\n    sensor_features,\n    failure_agg,\n    on='equipment_id',\n    how='left'\n)\n\n# Create target variable: failure within next N days\nfailure_logs_sorted = failure_logs.sort_values(['equipment_id', 'failure_time'])\nsensor_features = pd.merge_asof(\n    sensor_features.sort_values('reading_time'),\n    failure_logs_sorted[['equipment_id', 'failure_time']].rename(\n        columns={'failure_time': 'next_failure_time'}\n    ),\n    left_on='reading_time',\n    right_on='next_failure_time',\n    by='equipment_id',\n    direction='forward'\n)\n\nsensor_features['days_to_failure'] = (\n    sensor_features['next_failure_time'] - sensor_features['reading_time']\n).dt.total_seconds() / 86400\n\nsensor_features['failure_within_7d'] = (sensor_features['days_to_failure'] <= 7).astype(int)\nsensor_features['failure_within_30d'] = (sensor_features['days_to_failure'] <= 30).astype(int)\n\n# Calculate health score\nsensor_features['anomaly_count_24h'] = sensor_features.groupby('equipment_id')['is_anomaly_temperature'].transform(\n    lambda x: x.rolling('24H', on=sensor_features['reading_time']).sum()\n)\n\nsensor_features['health_score'] = 100 - (\n    sensor_features['anomaly_count_24h'] * 5 +\n    sensor_features['total_failures'].fillna(0) * 2 +\n    np.minimum(sensor_features['days_since_maintenance'].fillna(0) / 365, 1) * 20\n).clip(0, 100)\n\n# Prioritize maintenance\nsensor_features['maintenance_priority'] = np.where(\n    sensor_features['health_score'] < 50, 'Critical',\n    np.where(sensor_features['health_score'] < 70, 'High',\n    np.where(sensor_features['health_score'] < 85, 'Medium', 'Low'))\n)\n\n# Join parts availability\nparts_available = parts_inventory.groupby('equipment_type').agg({\n    'part_id': 'count',\n    'quantity_available': 'sum',\n    'lead_time_days': 'mean'\n}).reset_index()\nparts_available.columns = ['equipment_type', 'part_types_available',\n                           'total_parts_stock', 'avg_part_lead_time']\n\nsensor_features = pd.merge(\n    sensor_features,\n    parts_available,\n    on='equipment_type',\n    how='left'\n)\n\n# Save outputs\nsensor_features.to_csv('predictive_maintenance_features.csv', index=False)\n\n# Create maintenance schedule recommendations\nmaintenance_schedule = sensor_features[sensor_features['maintenance_priority'].isin(['Critical', 'High'])].groupby(\n    'equipment_id'\n).agg({\n    'health_score': 'min',\n    'maintenance_priority': 'first',\n    'days_since_maintenance': 'max',\n    'reading_time': 'max'\n}).reset_index()\n\nmaintenance_schedule.columns = ['equipment_id', 'current_health', 'priority',\n                                'days_since_last_maintenance', 'last_reading']\nmaintenance_schedule.to_csv('maintenance_schedule_recommendations.csv', index=False)\n] PASSED [ 27%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineVisualizations::test_ascii_visualization[genomic_analysis-\nimport pandas as pd\nimport numpy as np\n\n# Load genomic data\nvariants = pd.read_csv('variants.csv')\nsamples = pd.read_csv('samples.csv')\ngene_annotations = pd.read_csv('gene_annotations.csv')\nclinical_data = pd.read_csv('clinical_data.csv')\npathway_mappings = pd.read_csv('pathway_mappings.csv')\npopulation_frequencies = pd.read_csv('population_frequencies.csv')\nfunctional_predictions = pd.read_csv('functional_predictions.csv')\ndisease_associations = pd.read_csv('disease_associations.csv')\n\n# Standardize variant identifiers\nvariants['variant_id'] = (\n    variants['chromosome'].astype(str) + ':' +\n    variants['position'].astype(str) + ':' +\n    variants['reference'] + '>' + variants['alternate']\n)\n\n# Calculate variant quality metrics\nvariants['quality_pass'] = (\n    (variants['quality_score'] >= 30) &\n    (variants['read_depth'] >= 10) &\n    (variants['allele_frequency'] >= 0.2)\n).astype(int)\n\n# Filter high-quality variants\nhigh_quality_variants = variants[variants['quality_pass'] == 1].copy()\n\n# Annotate with gene information\nhigh_quality_variants = pd.merge(\n    high_quality_variants,\n    gene_annotations[['chromosome', 'start_position', 'end_position',\n                      'gene_symbol', 'gene_type', 'strand']],\n    on='chromosome',\n    how='left'\n)\n\n# Filter to variants within gene boundaries\nhigh_quality_variants = high_quality_variants[\n    (high_quality_variants['position'] >= high_quality_variants['start_position']) &\n    (high_quality_variants['position'] <= high_quality_variants['end_position'])\n]\n\n# Add functional impact predictions\nhigh_quality_variants = pd.merge(\n    high_quality_variants,\n    functional_predictions[['variant_id', 'sift_score', 'polyphen_score',\n                            'cadd_score', 'predicted_impact']],\n    on='variant_id',\n    how='left'\n)\n\n# Classify variant impact\nhigh_quality_variants['impact_category'] = np.where(\n    (high_quality_variants['sift_score'] < 0.05) | (high_quality_variants['polyphen_score'] > 0.85),\n    'Damaging',\n    np.where(\n        (high_quality_variants['sift_score'] < 0.1) | (high_quality_variants['polyphen_score'] > 0.5),\n        'Possibly_Damaging',\n        'Benign'\n    )\n)\n\n# Add population allele frequencies\nhigh_quality_variants = pd.merge(\n    high_quality_variants,\n    population_frequencies[['variant_id', 'gnomad_af', 'gnomad_af_eas',\n                            'gnomad_af_eur', 'gnomad_af_afr']],\n    on='variant_id',\n    how='left'\n)\n\n# Flag rare variants (MAF < 1%)\nhigh_quality_variants['is_rare'] = (high_quality_variants['gnomad_af'] < 0.01).astype(int)\nhigh_quality_variants['is_ultra_rare'] = (high_quality_variants['gnomad_af'] < 0.001).astype(int)\n\n# Add disease associations\nhigh_quality_variants = pd.merge(\n    high_quality_variants,\n    disease_associations[['gene_symbol', 'disease_name', 'inheritance_pattern',\n                          'clinical_significance']],\n    on='gene_symbol',\n    how='left'\n)\n\n# Calculate per-sample variant burden\nsample_burden = high_quality_variants.groupby('sample_id').agg({\n    'variant_id': 'count',\n    'is_rare': 'sum',\n    'impact_category': lambda x: (x == 'Damaging').sum()\n}).reset_index()\nsample_burden.columns = ['sample_id', 'total_variants', 'rare_variants', 'damaging_variants']\n\n# Add clinical phenotype data\nsample_burden = pd.merge(\n    sample_burden,\n    samples[['sample_id', 'patient_id', 'tissue_type', 'collection_date']],\n    on='sample_id',\n    how='left'\n)\n\nsample_burden = pd.merge(\n    sample_burden,\n    clinical_data[['patient_id', 'diagnosis', 'age_at_diagnosis',\n                   'sex', 'ethnicity', 'family_history']],\n    on='patient_id',\n    how='left'\n)\n\n# Calculate gene-level burden\ngene_burden = high_quality_variants.groupby(['sample_id', 'gene_symbol']).agg({\n    'variant_id': 'count',\n    'is_rare': 'sum',\n    'impact_category': lambda x: (x == 'Damaging').sum(),\n    'cadd_score': 'max'\n}).reset_index()\ngene_burden.columns = ['sample_id', 'gene_symbol', 'variants_in_gene',\n                       'rare_in_gene', 'damaging_in_gene', 'max_cadd']\n\n# Add pathway information\ngene_burden = pd.merge(\n    gene_burden,\n    pathway_mappings[['gene_symbol', 'pathway_id', 'pathway_name']],\n    on='gene_symbol',\n    how='left'\n)\n\n# Calculate pathway-level burden\npathway_burden = gene_burden.groupby(['sample_id', 'pathway_id', 'pathway_name']).agg({\n    'gene_symbol': 'nunique',\n    'variants_in_gene': 'sum',\n    'damaging_in_gene': 'sum',\n    'max_cadd': 'max'\n}).reset_index()\npathway_burden.columns = ['sample_id', 'pathway_id', 'pathway_name',\n                          'genes_affected', 'pathway_variants',\n                          'pathway_damaging', 'pathway_max_cadd']\n\n# Identify potentially pathogenic variants\npathogenic_candidates = high_quality_variants[\n    (high_quality_variants['impact_category'] == 'Damaging') &\n    (high_quality_variants['is_rare'] == 1) &\n    (high_quality_variants['clinical_significance'].isin(['Pathogenic', 'Likely_Pathogenic']).fillna(False))\n].copy()\n\n# Create variant report\nvariant_report = pathogenic_candidates.groupby(['sample_id', 'gene_symbol']).agg({\n    'variant_id': lambda x: '; '.join(x),\n    'disease_name': 'first',\n    'inheritance_pattern': 'first',\n    'cadd_score': 'max'\n}).reset_index()\n\n# Save outputs\nhigh_quality_variants.to_csv('annotated_variants.csv', index=False)\nsample_burden.to_csv('sample_variant_burden.csv', index=False)\ngene_burden.to_csv('gene_level_burden.csv', index=False)\npathway_burden.to_csv('pathway_analysis.csv', index=False)\nvariant_report.to_csv('pathogenic_variant_report.csv', index=False)\n] PASSED [ 27%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineVisualizations::test_ascii_visualization[clickstream_analysis-\nimport pandas as pd\nimport numpy as np\n\n# Load clickstream data\npage_views = pd.read_csv('page_views.csv')\nclick_events = pd.read_csv('click_events.csv')\nform_submissions = pd.read_csv('form_submissions.csv')\nsearch_queries = pd.read_csv('search_queries.csv')\nproduct_impressions = pd.read_csv('product_impressions.csv')\ncart_events = pd.read_csv('cart_events.csv')\nuser_agents = pd.read_csv('user_agents.csv')\ngeo_locations = pd.read_csv('geo_locations.csv')\n\n# Parse timestamps\npage_views['timestamp'] = pd.to_datetime(page_views['timestamp'])\nclick_events['timestamp'] = pd.to_datetime(click_events['timestamp'])\n\n# Combine all events\npage_views['event_type'] = 'page_view'\nclick_events['event_type'] = 'click'\nform_submissions['event_type'] = 'form_submit'\nsearch_queries['event_type'] = 'search'\ncart_events['event_type'] = cart_events['cart_action']\n\nall_events = pd.concat([\n    page_views[['session_id', 'user_id', 'timestamp', 'event_type', 'page_url', 'referrer']],\n    click_events[['session_id', 'user_id', 'timestamp', 'event_type', 'element_id', 'page_url']],\n    form_submissions[['session_id', 'user_id', 'timestamp', 'event_type', 'form_id', 'page_url']],\n    search_queries[['session_id', 'user_id', 'timestamp', 'event_type', 'query', 'results_count']],\n    cart_events[['session_id', 'user_id', 'timestamp', 'event_type', 'product_id', 'quantity']]\n], ignore_index=True, sort=False)\n\nall_events = all_events.sort_values(['session_id', 'timestamp'])\n\n# Session-level aggregations\nsession_metrics = all_events.groupby('session_id').agg({\n    'user_id': 'first',\n    'timestamp': ['min', 'max', 'count'],\n    'page_url': 'nunique',\n    'event_type': lambda x: x.value_counts().to_dict()\n}).reset_index()\n\nsession_metrics.columns = ['session_id', 'user_id', 'session_start',\n                            'session_end', 'total_events', 'unique_pages', 'event_breakdown']\n\n# Calculate session duration\nsession_metrics['session_duration_seconds'] = (\n    session_metrics['session_end'] - session_metrics['session_start']\n).dt.total_seconds()\n\n# Extract event counts\nsession_metrics['page_views'] = session_metrics['event_breakdown'].apply(\n    lambda x: x.get('page_view', 0) if isinstance(x, dict) else 0\n)\nsession_metrics['clicks'] = session_metrics['event_breakdown'].apply(\n    lambda x: x.get('click', 0) if isinstance(x, dict) else 0\n)\nsession_metrics['searches'] = session_metrics['event_breakdown'].apply(\n    lambda x: x.get('search', 0) if isinstance(x, dict) else 0\n)\nsession_metrics['cart_adds'] = session_metrics['event_breakdown'].apply(\n    lambda x: x.get('add_to_cart', 0) if isinstance(x, dict) else 0\n)\n\n# Calculate engagement metrics\nsession_metrics['pages_per_minute'] = (\n    session_metrics['unique_pages'] /\n    (session_metrics['session_duration_seconds'] / 60).replace(0, 1)\n)\nsession_metrics['events_per_page'] = (\n    session_metrics['total_events'] / session_metrics['unique_pages'].replace(0, 1)\n)\n\n# Identify bounce sessions\nsession_metrics['is_bounce'] = (\n    (session_metrics['unique_pages'] == 1) &\n    (session_metrics['session_duration_seconds'] < 10)\n).astype(int)\n\n# Create page flow sequences\npage_sequences = all_events[all_events['event_type'] == 'page_view'].copy()\npage_sequences['page_order'] = page_sequences.groupby('session_id').cumcount() + 1\npage_sequences['next_page'] = page_sequences.groupby('session_id')['page_url'].shift(-1)\n\n# Calculate page transitions\npage_transitions = page_sequences.groupby(['page_url', 'next_page']).size().reset_index(name='transition_count')\npage_transitions = page_transitions[page_transitions['next_page'].notna()]\n\n# Calculate exit rates per page\npage_stats = page_sequences.groupby('page_url').agg({\n    'session_id': 'count',\n    'next_page': lambda x: x.isna().sum()\n}).reset_index()\npage_stats.columns = ['page_url', 'total_views', 'exits']\npage_stats['exit_rate'] = page_stats['exits'] / page_stats['total_views']\n\n# Analyze search behavior\nsearch_analysis = search_queries.groupby('session_id').agg({\n    'query': ['count', lambda x: ' | '.join(x)],\n    'results_count': ['mean', 'min'],\n    'clicked_result': 'sum'\n}).reset_index()\nsearch_analysis.columns = ['session_id', 'search_count', 'search_queries',\n                            'avg_results', 'min_results', 'result_clicks']\nsearch_analysis['search_success_rate'] = (\n    search_analysis['result_clicks'] / search_analysis['search_count'].replace(0, 1)\n)\n\n# Analyze cart behavior\ncart_analysis = cart_events.groupby('session_id').agg({\n    'product_id': 'nunique',\n    'quantity': 'sum',\n    'cart_action': lambda x: (x == 'add_to_cart').sum()\n}).reset_index()\ncart_analysis.columns = ['session_id', 'unique_cart_products',\n                          'total_cart_quantity', 'add_to_cart_events']\n\n# Add purchase flag\npurchases = cart_events[cart_events['cart_action'] == 'purchase'].groupby('session_id').agg({\n    'order_value': 'sum'\n}).reset_index()\n\ncart_analysis = pd.merge(cart_analysis, purchases, on='session_id', how='left')\ncart_analysis['converted'] = cart_analysis['order_value'].notna().astype(int)\ncart_analysis['order_value'] = cart_analysis['order_value'].fillna(0)\n\n# Merge all session data\nsession_complete = session_metrics.copy()\nsession_complete = pd.merge(session_complete, search_analysis, on='session_id', how='left')\nsession_complete = pd.merge(session_complete, cart_analysis, on='session_id', how='left')\n\n# Add user agent info\nsession_complete = pd.merge(\n    session_complete,\n    user_agents[['session_id', 'device_type', 'browser', 'os', 'is_mobile']],\n    on='session_id',\n    how='left'\n)\n\n# Add geo location\nsession_complete = pd.merge(\n    session_complete,\n    geo_locations[['session_id', 'country', 'region', 'city', 'timezone']],\n    on='session_id',\n    how='left'\n)\n\n# Calculate user-level aggregations\nuser_metrics = session_complete.groupby('user_id').agg({\n    'session_id': 'count',\n    'session_duration_seconds': ['mean', 'sum'],\n    'page_views': ['mean', 'sum'],\n    'converted': 'sum',\n    'order_value': 'sum',\n    'session_start': ['min', 'max']\n}).reset_index()\n\nuser_metrics.columns = ['user_id', 'total_sessions', 'avg_session_duration',\n                        'total_time_spent', 'avg_pages_per_session', 'total_pages_viewed',\n                        'total_conversions', 'total_revenue', 'first_visit', 'last_visit']\n\nuser_metrics['conversion_rate'] = (\n    user_metrics['total_conversions'] / user_metrics['total_sessions']\n)\nuser_metrics['avg_order_value'] = (\n    user_metrics['total_revenue'] / user_metrics['total_conversions'].replace(0, 1)\n)\n\n# Segment users by engagement\nuser_metrics['engagement_segment'] = pd.cut(\n    user_metrics['total_time_spent'] / 3600,\n    bins=[-np.inf, 0.1, 1, 5, 20, np.inf],\n    labels=['Minimal', 'Light', 'Medium', 'Heavy', 'Power']\n)\n\n# Save outputs\nsession_complete.to_csv('session_analytics.csv', index=False)\nuser_metrics.to_csv('user_engagement_metrics.csv', index=False)\npage_stats.to_csv('page_performance.csv', index=False)\npage_transitions.to_csv('page_flow_analysis.csv', index=False)\n] PASSED [ 28%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineVisualizations::test_ascii_visualization[portfolio_risk-\nimport pandas as pd\nimport numpy as np\n\n# Load financial data\npositions = pd.read_csv('positions.csv')\nprice_history = pd.read_csv('price_history.csv')\ninstruments = pd.read_csv('instruments.csv')\nmarket_data = pd.read_csv('market_data.csv')\nfx_rates = pd.read_csv('fx_rates.csv')\nbenchmark_returns = pd.read_csv('benchmark_returns.csv')\nfactor_exposures = pd.read_csv('factor_exposures.csv')\ncredit_ratings = pd.read_csv('credit_ratings.csv')\n\n# Parse dates\nprice_history['date'] = pd.to_datetime(price_history['date'])\nmarket_data['date'] = pd.to_datetime(market_data['date'])\nfx_rates['date'] = pd.to_datetime(fx_rates['date'])\n\n# Calculate daily returns\nprice_history = price_history.sort_values(['instrument_id', 'date'])\nprice_history['daily_return'] = price_history.groupby('instrument_id')['close_price'].pct_change()\nprice_history['log_return'] = np.log(\n    price_history['close_price'] / price_history.groupby('instrument_id')['close_price'].shift(1)\n)\n\n# Calculate rolling volatility\nprice_history['volatility_20d'] = price_history.groupby('instrument_id')['daily_return'].transform(\n    lambda x: x.rolling(20).std() * np.sqrt(252)\n)\nprice_history['volatility_60d'] = price_history.groupby('instrument_id')['daily_return'].transform(\n    lambda x: x.rolling(60).std() * np.sqrt(252)\n)\n\n# Calculate rolling beta to market\nmarket_returns = market_data[market_data['index_id'] == 'SPX'][['date', 'daily_return']].copy()\nmarket_returns.columns = ['date', 'market_return']\n\nprice_with_market = pd.merge(price_history, market_returns, on='date', how='left')\n\ndef calculate_beta(group):\n    if len(group) < 60:\n        return np.nan\n    cov = group['daily_return'].cov(group['market_return'])\n    var = group['market_return'].var()\n    return cov / var if var != 0 else np.nan\n\nprice_with_market['beta'] = price_with_market.groupby('instrument_id').apply(\n    lambda x: x['daily_return'].rolling(60).cov(x['market_return']) /\n              x['market_return'].rolling(60).var()\n).reset_index(level=0, drop=True)\n\n# Enrich positions with instrument details\npositions_enriched = pd.merge(\n    positions,\n    instruments[['instrument_id', 'instrument_type', 'currency', 'sector',\n                 'country', 'maturity_date', 'coupon_rate', 'issuer']],\n    on='instrument_id',\n    how='left'\n)\n\n# Get latest prices and metrics\nlatest_prices = price_with_market.groupby('instrument_id').last().reset_index()\npositions_enriched = pd.merge(\n    positions_enriched,\n    latest_prices[['instrument_id', 'close_price', 'daily_return',\n                   'volatility_20d', 'volatility_60d', 'beta']],\n    on='instrument_id',\n    how='left'\n)\n\n# Convert to base currency (USD)\nfx_latest = fx_rates.groupby('currency_pair').last().reset_index()\nfx_latest['to_currency'] = fx_latest['currency_pair'].str[-3:]\nfx_latest['from_currency'] = fx_latest['currency_pair'].str[:3]\n\npositions_enriched = pd.merge(\n    positions_enriched,\n    fx_latest[['from_currency', 'rate']].rename(columns={'from_currency': 'currency', 'rate': 'fx_rate'}),\n    on='currency',\n    how='left'\n)\npositions_enriched['fx_rate'] = positions_enriched['fx_rate'].fillna(1.0)\n\n# Calculate market values\npositions_enriched['market_value_local'] = positions_enriched['quantity'] * positions_enriched['close_price']\npositions_enriched['market_value_usd'] = positions_enriched['market_value_local'] / positions_enriched['fx_rate']\n\n# Calculate position-level risk metrics\npositions_enriched['position_volatility'] = (\n    positions_enriched['market_value_usd'] * positions_enriched['volatility_20d']\n)\n\n# Portfolio-level calculations\nportfolio_value = positions_enriched.groupby('portfolio_id')['market_value_usd'].sum().reset_index()\nportfolio_value.columns = ['portfolio_id', 'total_nav']\n\npositions_enriched = pd.merge(positions_enriched, portfolio_value, on='portfolio_id', how='left')\npositions_enriched['weight'] = positions_enriched['market_value_usd'] / positions_enriched['total_nav']\n\n# Calculate weighted portfolio beta\nportfolio_beta = positions_enriched.groupby('portfolio_id').apply(\n    lambda x: (x['weight'] * x['beta']).sum()\n).reset_index(name='portfolio_beta')\n\n# Calculate portfolio volatility (simplified - assuming no correlation)\nportfolio_vol_contrib = positions_enriched.groupby('portfolio_id').apply(\n    lambda x: np.sqrt((x['weight']**2 * x['volatility_20d']**2).sum())\n).reset_index(name='portfolio_volatility')\n\n# Sector concentration\nsector_concentration = positions_enriched.groupby(['portfolio_id', 'sector']).agg({\n    'market_value_usd': 'sum',\n    'weight': 'sum'\n}).reset_index()\nsector_concentration.columns = ['portfolio_id', 'sector', 'sector_value', 'sector_weight']\n\n# Country concentration\ncountry_concentration = positions_enriched.groupby(['portfolio_id', 'country']).agg({\n    'market_value_usd': 'sum',\n    'weight': 'sum'\n}).reset_index()\n\n# Add credit risk for fixed income\npositions_enriched = pd.merge(\n    positions_enriched,\n    credit_ratings[['issuer', 'rating', 'rating_numeric', 'default_probability']],\n    on='issuer',\n    how='left'\n)\n\n# Calculate credit VaR contribution\npositions_enriched['credit_var_contrib'] = (\n    positions_enriched['market_value_usd'] *\n    positions_enriched['default_probability'].fillna(0) *\n    (1 - 0.4)  # Assuming 40% recovery rate\n)\n\n# Calculate VaR (parametric approach)\nconfidence_level = 0.99\nz_score = 2.326  # 99% confidence\n\npositions_enriched['var_1d'] = (\n    positions_enriched['market_value_usd'] *\n    positions_enriched['volatility_20d'] / np.sqrt(252) *\n    z_score\n)\n\nportfolio_var = positions_enriched.groupby('portfolio_id')['var_1d'].sum().reset_index()\nportfolio_var.columns = ['portfolio_id', 'total_var_1d']\n\n# Historical VaR calculation\nreturns_pivot = price_history.pivot_table(\n    index='date',\n    columns='instrument_id',\n    values='daily_return'\n).fillna(0)\n\n# Calculate portfolio returns for each portfolio\nportfolio_returns = []\nfor portfolio_id in positions_enriched['portfolio_id'].unique():\n    port_positions = positions_enriched[positions_enriched['portfolio_id'] == portfolio_id]\n    weights = port_positions.set_index('instrument_id')['weight'].to_dict()\n\n    port_return = returns_pivot[list(weights.keys())].mul(\n        pd.Series(weights)\n    ).sum(axis=1)\n\n    portfolio_returns.append(pd.DataFrame({\n        'portfolio_id': portfolio_id,\n        'date': returns_pivot.index,\n        'portfolio_return': port_return\n    }))\n\nportfolio_returns_df = pd.concat(portfolio_returns, ignore_index=True)\n\n# Calculate historical VaR\nhistorical_var = portfolio_returns_df.groupby('portfolio_id')['portfolio_return'].apply(\n    lambda x: x.quantile(1 - confidence_level)\n).reset_index(name='historical_var_1d')\n\n# Calculate Sharpe ratio\nrisk_free_rate = 0.05 / 252  # Daily risk-free rate\n\nportfolio_sharpe = portfolio_returns_df.groupby('portfolio_id').agg({\n    'portfolio_return': ['mean', 'std']\n}).reset_index()\nportfolio_sharpe.columns = ['portfolio_id', 'avg_return', 'return_std']\nportfolio_sharpe['sharpe_ratio'] = (\n    (portfolio_sharpe['avg_return'] - risk_free_rate) / portfolio_sharpe['return_std']\n) * np.sqrt(252)\n\n# Compile portfolio risk summary\nportfolio_risk = portfolio_value.copy()\nportfolio_risk = pd.merge(portfolio_risk, portfolio_beta, on='portfolio_id', how='left')\nportfolio_risk = pd.merge(portfolio_risk, portfolio_vol_contrib, on='portfolio_id', how='left')\nportfolio_risk = pd.merge(portfolio_risk, portfolio_var, on='portfolio_id', how='left')\nportfolio_risk = pd.merge(portfolio_risk, historical_var, on='portfolio_id', how='left')\nportfolio_risk = pd.merge(portfolio_risk, portfolio_sharpe, on='portfolio_id', how='left')\n\n# Save outputs\npositions_enriched.to_csv('positions_with_risk.csv', index=False)\nportfolio_risk.to_csv('portfolio_risk_summary.csv', index=False)\nsector_concentration.to_csv('sector_concentration.csv', index=False)\ncountry_concentration.to_csv('country_concentration.csv', index=False)\nportfolio_returns_df.to_csv('portfolio_returns_history.csv', index=False)\n] PASSED [ 28%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineVisualizations::test_mermaid_visualization[fraud_detection-\nimport pandas as pd\nimport numpy as np\n\n# Load transaction streams\ntransactions = pd.read_csv('transactions.csv')\nuser_profiles = pd.read_csv('user_profiles.csv')\ndevice_fingerprints = pd.read_csv('device_fingerprints.csv')\nmerchant_risk_scores = pd.read_csv('merchant_risk_scores.csv')\nhistorical_fraud = pd.read_csv('historical_fraud.csv')\nip_geolocation = pd.read_csv('ip_geolocation.csv')\n\n# Parse timestamps\ntransactions['timestamp'] = pd.to_datetime(transactions['timestamp'])\ntransactions['date'] = transactions['timestamp'].dt.date\ntransactions['hour'] = transactions['timestamp'].dt.hour\ntransactions['minute'] = transactions['timestamp'].dt.minute\ntransactions['day_of_week'] = transactions['timestamp'].dt.dayofweek\ntransactions['is_weekend'] = transactions['day_of_week'].isin([5, 6]).astype(int)\ntransactions['is_night'] = transactions['hour'].between(0, 6).astype(int)\n\n# Enrich with user profile data\ntransactions_enriched = pd.merge(\n    transactions,\n    user_profiles[['user_id', 'account_age_days', 'verification_level',\n                   'avg_transaction_amount', 'typical_login_hour',\n                   'home_country', 'risk_tier']],\n    on='user_id',\n    how='left'\n)\n\n# Add device fingerprint risk\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    device_fingerprints[['device_id', 'device_trust_score', 'is_known_device',\n                         'browser_anomaly_score', 'last_seen_country']],\n    on='device_id',\n    how='left'\n)\n\n# Add merchant risk scores\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    merchant_risk_scores[['merchant_id', 'merchant_risk_score',\n                          'chargeback_rate', 'merchant_category']],\n    on='merchant_id',\n    how='left'\n)\n\n# Add IP geolocation\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    ip_geolocation[['ip_address', 'ip_country', 'ip_city',\n                    'is_vpn', 'is_tor', 'ip_risk_score']],\n    on='ip_address',\n    how='left'\n)\n\n# Calculate velocity features - transactions per user in last N minutes\ntransactions_enriched = transactions_enriched.sort_values(['user_id', 'timestamp'])\n\n# Rolling window aggregations per user\nuser_velocity = transactions_enriched.groupby('user_id').agg({\n    'transaction_id': 'count',\n    'amount': ['sum', 'mean', 'std', 'max'],\n    'merchant_id': 'nunique',\n    'ip_address': 'nunique',\n    'device_id': 'nunique'\n}).reset_index()\nuser_velocity.columns = ['user_id', 'tx_count_session', 'total_amount_session',\n                         'avg_amount_session', 'std_amount_session', 'max_amount_session',\n                         'unique_merchants', 'unique_ips', 'unique_devices']\n\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    user_velocity,\n    on='user_id',\n    how='left'\n)\n\n# Calculate deviation from user's typical behavior\ntransactions_enriched['amount_deviation'] = (\n    (transactions_enriched['amount'] - transactions_enriched['avg_transaction_amount']) /\n    transactions_enriched['avg_transaction_amount'].replace(0, 1)\n)\n\ntransactions_enriched['hour_deviation'] = abs(\n    transactions_enriched['hour'] - transactions_enriched['typical_login_hour']\n)\n\n# Geographic anomaly detection\ntransactions_enriched['country_mismatch'] = (\n    transactions_enriched['ip_country'] != transactions_enriched['home_country']\n).astype(int)\n\ntransactions_enriched['device_country_mismatch'] = (\n    transactions_enriched['ip_country'] != transactions_enriched['last_seen_country']\n).astype(int)\n\n# Composite risk score calculation\ntransactions_enriched['composite_risk_score'] = (\n    transactions_enriched['ip_risk_score'] * 0.2 +\n    transactions_enriched['merchant_risk_score'] * 0.15 +\n    (1 - transactions_enriched['device_trust_score']) * 0.2 +\n    transactions_enriched['amount_deviation'].clip(0, 5) * 0.15 +\n    transactions_enriched['country_mismatch'] * 0.15 +\n    transactions_enriched['is_vpn'] * 0.1 +\n    transactions_enriched['is_tor'] * 0.05\n)\n\n# Flag high-risk transactions\ntransactions_enriched['is_high_risk'] = (\n    transactions_enriched['composite_risk_score'] > 0.7\n).astype(int)\n\n# Join historical fraud patterns\nfraud_patterns = historical_fraud.groupby('user_id').agg({\n    'fraud_flag': 'sum',\n    'transaction_id': 'count'\n}).reset_index()\nfraud_patterns.columns = ['user_id', 'historical_fraud_count', 'historical_tx_count']\nfraud_patterns['historical_fraud_rate'] = (\n    fraud_patterns['historical_fraud_count'] / fraud_patterns['historical_tx_count']\n)\n\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    fraud_patterns[['user_id', 'historical_fraud_rate']],\n    on='user_id',\n    how='left'\n)\ntransactions_enriched['historical_fraud_rate'] = transactions_enriched['historical_fraud_rate'].fillna(0)\n\n# Final feature selection for ML model\nml_features = transactions_enriched[[\n    'transaction_id', 'user_id', 'amount', 'amount_deviation',\n    'hour_deviation', 'is_weekend', 'is_night',\n    'device_trust_score', 'is_known_device', 'browser_anomaly_score',\n    'merchant_risk_score', 'chargeback_rate',\n    'ip_risk_score', 'is_vpn', 'is_tor',\n    'country_mismatch', 'device_country_mismatch',\n    'tx_count_session', 'unique_merchants', 'unique_ips', 'unique_devices',\n    'composite_risk_score', 'historical_fraud_rate', 'is_high_risk'\n]].copy()\n\nml_features.to_csv('fraud_detection_features.csv', index=False)\ntransactions_enriched.to_csv('transactions_enriched_full.csv', index=False)\n] PASSED [ 28%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineVisualizations::test_mermaid_visualization[customer_360-\nimport pandas as pd\nimport numpy as np\n\n# Load data from multiple source systems\ncrm_customers = pd.read_csv('crm_customers.csv')\necommerce_users = pd.read_csv('ecommerce_users.csv')\nmobile_app_users = pd.read_csv('mobile_app_users.csv')\ncall_center_interactions = pd.read_csv('call_center_interactions.csv')\nemail_campaigns = pd.read_csv('email_campaigns.csv')\nweb_analytics = pd.read_csv('web_analytics.csv')\nsocial_media = pd.read_csv('social_media.csv')\nloyalty_program = pd.read_csv('loyalty_program.csv')\nsupport_tickets = pd.read_csv('support_tickets.csv')\nnps_surveys = pd.read_csv('nps_surveys.csv')\n\n# Standardize customer identifiers across systems\ncrm_customers['source'] = 'crm'\ncrm_customers['master_id'] = crm_customers['crm_id']\n\necommerce_users['source'] = 'ecommerce'\necommerce_users = pd.merge(\n    ecommerce_users,\n    crm_customers[['email', 'crm_id']],\n    on='email',\n    how='left'\n)\necommerce_users['master_id'] = ecommerce_users['crm_id'].fillna(\n    'ECO_' + ecommerce_users['ecommerce_user_id'].astype(str)\n)\n\nmobile_app_users['source'] = 'mobile'\nmobile_app_users = pd.merge(\n    mobile_app_users,\n    crm_customers[['phone', 'crm_id']],\n    on='phone',\n    how='left'\n)\nmobile_app_users['master_id'] = mobile_app_users['crm_id'].fillna(\n    'MOB_' + mobile_app_users['app_user_id'].astype(str)\n)\n\n# Create unified customer profile\ncustomer_base = crm_customers[['master_id', 'email', 'phone', 'first_name',\n                               'last_name', 'date_of_birth', 'gender',\n                               'address', 'city', 'state', 'country',\n                               'customer_since']].copy()\n\n# Aggregate e-commerce behavior\necommerce_metrics = ecommerce_users.groupby('master_id').agg({\n    'order_id': 'count',\n    'order_total': ['sum', 'mean', 'max'],\n    'items_purchased': 'sum',\n    'last_order_date': 'max',\n    'cart_abandonment_count': 'sum'\n}).reset_index()\necommerce_metrics.columns = ['master_id', 'ecom_order_count', 'ecom_total_spend',\n                              'ecom_avg_order', 'ecom_max_order', 'ecom_items_total',\n                              'ecom_last_order', 'ecom_cart_abandonments']\n\n# Aggregate mobile app engagement\nmobile_metrics = mobile_app_users.groupby('master_id').agg({\n    'session_count': 'sum',\n    'total_time_minutes': 'sum',\n    'push_notifications_clicked': 'sum',\n    'app_crashes': 'sum',\n    'features_used': lambda x: len(set(','.join(x.dropna()).split(','))),\n    'last_active_date': 'max'\n}).reset_index()\nmobile_metrics.columns = ['master_id', 'app_sessions', 'app_time_minutes',\n                          'push_clicks', 'app_crashes', 'features_used_count',\n                          'app_last_active']\n\n# Aggregate call center interactions\ncall_metrics = call_center_interactions.groupby('master_id').agg({\n    'call_id': 'count',\n    'call_duration_seconds': ['sum', 'mean'],\n    'issue_resolved': 'mean',\n    'escalated': 'sum',\n    'sentiment_score': 'mean'\n}).reset_index()\ncall_metrics.columns = ['master_id', 'call_count', 'total_call_duration',\n                        'avg_call_duration', 'resolution_rate',\n                        'escalation_count', 'call_sentiment']\n\n# Aggregate email engagement\nemail_metrics = email_campaigns.groupby('master_id').agg({\n    'email_sent': 'sum',\n    'email_opened': 'sum',\n    'email_clicked': 'sum',\n    'unsubscribed': 'max'\n}).reset_index()\nemail_metrics['email_open_rate'] = email_metrics['email_opened'] / email_metrics['email_sent']\nemail_metrics['email_click_rate'] = email_metrics['email_clicked'] / email_metrics['email_opened'].replace(0, 1)\n\n# Aggregate web analytics\nweb_metrics = web_analytics.groupby('master_id').agg({\n    'page_views': 'sum',\n    'unique_sessions': 'sum',\n    'bounce_rate': 'mean',\n    'avg_session_duration': 'mean',\n    'conversion_events': 'sum'\n}).reset_index()\n\n# Aggregate loyalty program data\nloyalty_metrics = loyalty_program.groupby('master_id').agg({\n    'points_earned': 'sum',\n    'points_redeemed': 'sum',\n    'tier_level': 'max',\n    'referrals_made': 'sum'\n}).reset_index()\nloyalty_metrics['points_balance'] = loyalty_metrics['points_earned'] - loyalty_metrics['points_redeemed']\n\n# Aggregate support tickets\nsupport_metrics = support_tickets.groupby('master_id').agg({\n    'ticket_id': 'count',\n    'resolution_time_hours': 'mean',\n    'satisfaction_rating': 'mean',\n    'ticket_reopened': 'sum'\n}).reset_index()\nsupport_metrics.columns = ['master_id', 'ticket_count', 'avg_resolution_time',\n                           'support_satisfaction', 'tickets_reopened']\n\n# Aggregate NPS surveys\nnps_metrics = nps_surveys.groupby('master_id').agg({\n    'nps_score': 'mean',\n    'survey_id': 'count',\n    'would_recommend': 'mean'\n}).reset_index()\nnps_metrics.columns = ['master_id', 'avg_nps_score', 'surveys_completed', 'recommend_rate']\n\n# Merge all metrics into unified customer 360 view\ncustomer_360 = customer_base.copy()\n\nfor metrics_df in [ecommerce_metrics, mobile_metrics, call_metrics,\n                   email_metrics, web_metrics, loyalty_metrics,\n                   support_metrics, nps_metrics]:\n    customer_360 = pd.merge(customer_360, metrics_df, on='master_id', how='left')\n\n# Fill missing values appropriately\nnumeric_cols = customer_360.select_dtypes(include=[np.number]).columns\ncustomer_360[numeric_cols] = customer_360[numeric_cols].fillna(0)\n\n# Calculate derived metrics\ncustomer_360['total_interactions'] = (\n    customer_360['ecom_order_count'] +\n    customer_360['app_sessions'] +\n    customer_360['call_count'] +\n    customer_360['ticket_count']\n)\n\ncustomer_360['digital_engagement_score'] = (\n    customer_360['app_sessions'] * 0.3 +\n    customer_360['email_open_rate'] * 20 +\n    customer_360['page_views'] * 0.1 +\n    customer_360['push_clicks'] * 0.5\n).clip(0, 100)\n\ncustomer_360['customer_health_score'] = (\n    customer_360['avg_nps_score'] / 10 * 25 +\n    customer_360['resolution_rate'] * 25 +\n    customer_360['support_satisfaction'] / 5 * 25 +\n    (1 - customer_360['escalation_count'].clip(0, 5) / 5) * 25\n)\n\n# Customer segmentation\ncustomer_360['value_segment'] = pd.cut(\n    customer_360['ecom_total_spend'],\n    bins=[-np.inf, 100, 500, 2000, 10000, np.inf],\n    labels=['Dormant', 'Bronze', 'Silver', 'Gold', 'Platinum']\n)\n\ncustomer_360['engagement_segment'] = pd.cut(\n    customer_360['digital_engagement_score'],\n    bins=[-np.inf, 20, 40, 60, 80, np.inf],\n    labels=['Inactive', 'Low', 'Medium', 'High', 'Power User']\n)\n\n# Calculate churn risk\ncustomer_360['days_since_activity'] = (\n    pd.to_datetime('today') - pd.to_datetime(customer_360['ecom_last_order'])\n).dt.days.fillna(999)\n\ncustomer_360['churn_risk_score'] = (\n    customer_360['days_since_activity'] / 365 * 30 +\n    (1 - customer_360['email_open_rate'].fillna(0)) * 20 +\n    customer_360['tickets_reopened'] * 10 +\n    (10 - customer_360['avg_nps_score'].fillna(5)) * 4\n).clip(0, 100)\n\n# Save outputs\ncustomer_360.to_csv('customer_360_unified.csv', index=False)\n] PASSED [ 28%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineVisualizations::test_mermaid_visualization[supply_chain-\nimport pandas as pd\nimport numpy as np\n\n# Load supply chain data\nsales_history = pd.read_csv('sales_history.csv')\ninventory_levels = pd.read_csv('inventory_levels.csv')\nsupplier_data = pd.read_csv('supplier_data.csv')\nwarehouse_locations = pd.read_csv('warehouse_locations.csv')\nshipping_costs = pd.read_csv('shipping_costs.csv')\nproduct_catalog = pd.read_csv('product_catalog.csv')\npromotions_calendar = pd.read_csv('promotions_calendar.csv')\nweather_data = pd.read_csv('weather_data.csv')\neconomic_indicators = pd.read_csv('economic_indicators.csv')\n\n# Parse dates\nsales_history['sale_date'] = pd.to_datetime(sales_history['sale_date'])\nsales_history['year'] = sales_history['sale_date'].dt.year\nsales_history['month'] = sales_history['sale_date'].dt.month\nsales_history['week'] = sales_history['sale_date'].dt.isocalendar().week\nsales_history['day_of_week'] = sales_history['sale_date'].dt.dayofweek\nsales_history['is_weekend'] = sales_history['day_of_week'].isin([5, 6]).astype(int)\n\n# Enrich sales with product info\nsales_enriched = pd.merge(\n    sales_history,\n    product_catalog[['product_id', 'category', 'subcategory', 'brand',\n                     'unit_cost', 'weight_kg', 'is_perishable', 'shelf_life_days']],\n    on='product_id',\n    how='left'\n)\n\n# Add promotion flags\npromotions_calendar['promo_date'] = pd.to_datetime(promotions_calendar['promo_date'])\nsales_enriched = pd.merge(\n    sales_enriched,\n    promotions_calendar[['product_id', 'promo_date', 'discount_pct', 'promo_type']],\n    left_on=['product_id', 'sale_date'],\n    right_on=['product_id', 'promo_date'],\n    how='left'\n)\nsales_enriched['is_promotion'] = sales_enriched['discount_pct'].notna().astype(int)\nsales_enriched['discount_pct'] = sales_enriched['discount_pct'].fillna(0)\n\n# Add weather impact\nweather_data['date'] = pd.to_datetime(weather_data['date'])\nsales_enriched = pd.merge(\n    sales_enriched,\n    weather_data[['date', 'region', 'temperature', 'precipitation', 'weather_condition']],\n    left_on=['sale_date', 'region'],\n    right_on=['date', 'region'],\n    how='left'\n)\n\n# Calculate time-based aggregations for demand patterns\ndaily_demand = sales_enriched.groupby(['product_id', 'warehouse_id', 'sale_date']).agg({\n    'quantity': 'sum',\n    'revenue': 'sum',\n    'is_promotion': 'max',\n    'temperature': 'mean'\n}).reset_index()\n\n# Calculate rolling averages for demand forecasting\ndaily_demand = daily_demand.sort_values(['product_id', 'warehouse_id', 'sale_date'])\ndaily_demand['demand_7d_avg'] = daily_demand.groupby(['product_id', 'warehouse_id'])['quantity'].transform(\n    lambda x: x.rolling(7, min_periods=1).mean()\n)\ndaily_demand['demand_30d_avg'] = daily_demand.groupby(['product_id', 'warehouse_id'])['quantity'].transform(\n    lambda x: x.rolling(30, min_periods=1).mean()\n)\ndaily_demand['demand_90d_avg'] = daily_demand.groupby(['product_id', 'warehouse_id'])['quantity'].transform(\n    lambda x: x.rolling(90, min_periods=1).mean()\n)\n\n# Calculate demand volatility\ndaily_demand['demand_7d_std'] = daily_demand.groupby(['product_id', 'warehouse_id'])['quantity'].transform(\n    lambda x: x.rolling(7, min_periods=1).std()\n)\ndaily_demand['demand_coefficient_variation'] = (\n    daily_demand['demand_7d_std'] / daily_demand['demand_7d_avg'].replace(0, 1)\n)\n\n# Seasonality detection\nmonthly_patterns = sales_enriched.groupby(['product_id', 'month']).agg({\n    'quantity': 'mean'\n}).reset_index()\nmonthly_patterns.columns = ['product_id', 'month', 'monthly_avg_demand']\n\nyearly_avg = monthly_patterns.groupby('product_id')['monthly_avg_demand'].transform('mean')\nmonthly_patterns['seasonality_index'] = monthly_patterns['monthly_avg_demand'] / yearly_avg\n\n# Join current inventory levels\ninventory_current = inventory_levels.groupby(['product_id', 'warehouse_id']).agg({\n    'quantity_on_hand': 'sum',\n    'quantity_reserved': 'sum',\n    'quantity_in_transit': 'sum',\n    'last_restock_date': 'max'\n}).reset_index()\ninventory_current['available_inventory'] = (\n    inventory_current['quantity_on_hand'] - inventory_current['quantity_reserved']\n)\n\n# Calculate days of supply\ninventory_analysis = pd.merge(\n    inventory_current,\n    daily_demand.groupby(['product_id', 'warehouse_id']).agg({\n        'demand_30d_avg': 'last'\n    }).reset_index(),\n    on=['product_id', 'warehouse_id'],\n    how='left'\n)\ninventory_analysis['days_of_supply'] = (\n    inventory_analysis['available_inventory'] /\n    inventory_analysis['demand_30d_avg'].replace(0, 0.1)\n)\n\n# Add supplier lead times\ninventory_analysis = pd.merge(\n    inventory_analysis,\n    supplier_data[['product_id', 'supplier_id', 'lead_time_days',\n                   'min_order_qty', 'unit_cost', 'reliability_score']],\n    on='product_id',\n    how='left'\n)\n\n# Calculate reorder points with safety stock\ninventory_analysis['safety_stock'] = (\n    inventory_analysis['demand_30d_avg'] *\n    inventory_analysis['lead_time_days'] * 0.5  # Safety factor\n)\ninventory_analysis['reorder_point'] = (\n    inventory_analysis['demand_30d_avg'] * inventory_analysis['lead_time_days'] +\n    inventory_analysis['safety_stock']\n)\n\n# Flag items needing reorder\ninventory_analysis['needs_reorder'] = (\n    inventory_analysis['available_inventory'] < inventory_analysis['reorder_point']\n).astype(int)\n\n# Calculate optimal order quantity (EOQ approximation)\ninventory_analysis['holding_cost_annual'] = inventory_analysis['unit_cost'] * 0.25\ninventory_analysis['order_cost'] = 50  # Fixed ordering cost\ninventory_analysis['annual_demand'] = inventory_analysis['demand_30d_avg'] * 12 * 30\n\ninventory_analysis['economic_order_qty'] = np.sqrt(\n    2 * inventory_analysis['annual_demand'] * inventory_analysis['order_cost'] /\n    inventory_analysis['holding_cost_annual'].replace(0, 1)\n)\n\n# Calculate shipping optimization\nshipping_analysis = pd.merge(\n    inventory_analysis,\n    warehouse_locations[['warehouse_id', 'latitude', 'longitude', 'capacity_units']],\n    on='warehouse_id',\n    how='left'\n)\n\n# Add shipping costs\nshipping_analysis = pd.merge(\n    shipping_analysis,\n    shipping_costs,\n    on='warehouse_id',\n    how='left'\n)\n\n# Calculate total landed cost\nshipping_analysis['landed_cost'] = (\n    shipping_analysis['unit_cost'] +\n    shipping_analysis['shipping_cost_per_unit'] +\n    shipping_analysis['handling_cost']\n)\n\n# Identify stock-out risk\nshipping_analysis['stockout_risk'] = np.where(\n    shipping_analysis['days_of_supply'] < shipping_analysis['lead_time_days'],\n    'High',\n    np.where(\n        shipping_analysis['days_of_supply'] < shipping_analysis['lead_time_days'] * 1.5,\n        'Medium',\n        'Low'\n    )\n)\n\n# Save outputs\ndaily_demand.to_csv('demand_forecast_features.csv', index=False)\ninventory_analysis.to_csv('inventory_optimization.csv', index=False)\nshipping_analysis.to_csv('supply_chain_analysis.csv', index=False)\nmonthly_patterns.to_csv('seasonality_patterns.csv', index=False)\n] PASSED [ 28%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineVisualizations::test_mermaid_visualization[marketing_attribution-\nimport pandas as pd\nimport numpy as np\n\n# Load marketing data\ntouchpoints = pd.read_csv('touchpoints.csv')\nconversions = pd.read_csv('conversions.csv')\nad_spend = pd.read_csv('ad_spend.csv')\nchannel_costs = pd.read_csv('channel_costs.csv')\ncampaign_metadata = pd.read_csv('campaign_metadata.csv')\ncustomer_journeys = pd.read_csv('customer_journeys.csv')\norganic_traffic = pd.read_csv('organic_traffic.csv')\noffline_media = pd.read_csv('offline_media.csv')\n\n# Parse timestamps\ntouchpoints['touchpoint_time'] = pd.to_datetime(touchpoints['touchpoint_time'])\nconversions['conversion_time'] = pd.to_datetime(conversions['conversion_time'])\n\n# Create customer journey sequences\ntouchpoints_sorted = touchpoints.sort_values(['customer_id', 'touchpoint_time'])\n\n# Assign journey IDs (new journey if gap > 30 days)\ntouchpoints_sorted['time_since_last'] = touchpoints_sorted.groupby('customer_id')['touchpoint_time'].diff()\ntouchpoints_sorted['new_journey'] = (\n    touchpoints_sorted['time_since_last'] > pd.Timedelta(days=30)\n).fillna(True).astype(int)\ntouchpoints_sorted['journey_id'] = touchpoints_sorted.groupby('customer_id')['new_journey'].cumsum()\n\n# Create journey-level aggregations\njourney_touchpoints = touchpoints_sorted.groupby(['customer_id', 'journey_id']).agg({\n    'touchpoint_id': 'count',\n    'channel': lambda x: ' > '.join(x),\n    'campaign_id': lambda x: list(x.unique()),\n    'touchpoint_time': ['min', 'max'],\n    'cost': 'sum'\n}).reset_index()\njourney_touchpoints.columns = ['customer_id', 'journey_id', 'touchpoint_count',\n                                'channel_path', 'campaigns_touched',\n                                'journey_start', 'journey_end', 'journey_cost']\n\n# Calculate journey duration\njourney_touchpoints['journey_duration_days'] = (\n    journey_touchpoints['journey_end'] - journey_touchpoints['journey_start']\n).dt.total_seconds() / 86400\n\n# Join with conversions\njourney_conversions = pd.merge(\n    journey_touchpoints,\n    conversions[['customer_id', 'conversion_time', 'conversion_value', 'product_category']],\n    on='customer_id',\n    how='left'\n)\n\n# Filter to conversions within journey window\njourney_conversions['converted'] = (\n    (journey_conversions['conversion_time'] >= journey_conversions['journey_start']) &\n    (journey_conversions['conversion_time'] <= journey_conversions['journey_end'] + pd.Timedelta(days=7))\n).astype(int)\n\njourney_conversions['conversion_value'] = np.where(\n    journey_conversions['converted'] == 1,\n    journey_conversions['conversion_value'],\n    0\n)\n\n# Calculate attribution models\n\n# 1. First-touch attribution\ntouchpoints_sorted['is_first_touch'] = touchpoints_sorted.groupby(\n    ['customer_id', 'journey_id']\n).cumcount() == 0\n\nfirst_touch = touchpoints_sorted[touchpoints_sorted['is_first_touch']].copy()\nfirst_touch_attribution = first_touch.groupby('channel').agg({\n    'touchpoint_id': 'count',\n    'cost': 'sum'\n}).reset_index()\nfirst_touch_attribution.columns = ['channel', 'first_touch_count', 'first_touch_cost']\n\n# 2. Last-touch attribution\nlast_touch = touchpoints_sorted.groupby(['customer_id', 'journey_id']).last().reset_index()\nlast_touch_attribution = last_touch.groupby('channel').agg({\n    'touchpoint_id': 'count',\n    'cost': 'sum'\n}).reset_index()\nlast_touch_attribution.columns = ['channel', 'last_touch_count', 'last_touch_cost']\n\n# 3. Linear attribution (equal credit to all touchpoints)\njourney_conversions_exploded = journey_conversions.explode('campaigns_touched')\njourney_conversions_exploded['linear_credit'] = (\n    journey_conversions_exploded['conversion_value'] /\n    journey_conversions_exploded['touchpoint_count']\n)\n\n# 4. Time-decay attribution\ntouchpoints_with_conversion = pd.merge(\n    touchpoints_sorted,\n    journey_conversions[['customer_id', 'journey_id', 'conversion_time', 'conversion_value', 'converted']],\n    on=['customer_id', 'journey_id'],\n    how='left'\n)\n\ntouchpoints_with_conversion['days_to_conversion'] = (\n    touchpoints_with_conversion['conversion_time'] - touchpoints_with_conversion['touchpoint_time']\n).dt.total_seconds() / 86400\n\n# Exponential decay weight (half-life = 7 days)\ntouchpoints_with_conversion['decay_weight'] = np.exp(\n    -touchpoints_with_conversion['days_to_conversion'] / 7\n)\n\n# Normalize weights within journey\ntouchpoints_with_conversion['weight_sum'] = touchpoints_with_conversion.groupby(\n    ['customer_id', 'journey_id']\n)['decay_weight'].transform('sum')\n\ntouchpoints_with_conversion['time_decay_credit'] = (\n    touchpoints_with_conversion['decay_weight'] /\n    touchpoints_with_conversion['weight_sum'].replace(0, 1) *\n    touchpoints_with_conversion['conversion_value']\n)\n\n# 5. Position-based attribution (40% first, 40% last, 20% middle)\ntouchpoints_with_conversion['position'] = touchpoints_with_conversion.groupby(\n    ['customer_id', 'journey_id']\n).cumcount() + 1\n\ntouchpoints_with_conversion['total_positions'] = touchpoints_with_conversion.groupby(\n    ['customer_id', 'journey_id']\n)['position'].transform('max')\n\ntouchpoints_with_conversion['position_weight'] = np.where(\n    touchpoints_with_conversion['position'] == 1,\n    0.4,\n    np.where(\n        touchpoints_with_conversion['position'] == touchpoints_with_conversion['total_positions'],\n        0.4,\n        0.2 / (touchpoints_with_conversion['total_positions'] - 2).replace(0, 1)\n    )\n)\n\ntouchpoints_with_conversion['position_based_credit'] = (\n    touchpoints_with_conversion['position_weight'] *\n    touchpoints_with_conversion['conversion_value']\n)\n\n# Aggregate attribution by channel\nchannel_attribution = touchpoints_with_conversion.groupby('channel').agg({\n    'touchpoint_id': 'count',\n    'cost': 'sum',\n    'linear_credit': 'sum',\n    'time_decay_credit': 'sum',\n    'position_based_credit': 'sum'\n}).reset_index()\n\nchannel_attribution = pd.merge(channel_attribution, first_touch_attribution, on='channel', how='left')\nchannel_attribution = pd.merge(channel_attribution, last_touch_attribution, on='channel', how='left')\n\n# Calculate ROI by attribution model\nchannel_attribution['roi_linear'] = (\n    channel_attribution['linear_credit'] - channel_attribution['cost']\n) / channel_attribution['cost'].replace(0, 1)\n\nchannel_attribution['roi_time_decay'] = (\n    channel_attribution['time_decay_credit'] - channel_attribution['cost']\n) / channel_attribution['cost'].replace(0, 1)\n\nchannel_attribution['roi_position'] = (\n    channel_attribution['position_based_credit'] - channel_attribution['cost']\n) / channel_attribution['cost'].replace(0, 1)\n\n# Marketing mix modeling aggregations\ndaily_spend = ad_spend.groupby(['date', 'channel']).agg({\n    'spend': 'sum',\n    'impressions': 'sum',\n    'clicks': 'sum'\n}).reset_index()\n\ndaily_spend['cpm'] = daily_spend['spend'] / daily_spend['impressions'] * 1000\ndaily_spend['cpc'] = daily_spend['spend'] / daily_spend['clicks'].replace(0, 1)\ndaily_spend['ctr'] = daily_spend['clicks'] / daily_spend['impressions'].replace(0, 1)\n\n# Add adstock transformation (carryover effect)\ndaily_spend = daily_spend.sort_values(['channel', 'date'])\ndaily_spend['spend_adstock'] = daily_spend.groupby('channel')['spend'].transform(\n    lambda x: x.ewm(halflife=7).mean()\n)\n\n# Save outputs\njourney_touchpoints.to_csv('customer_journeys_analyzed.csv', index=False)\nchannel_attribution.to_csv('channel_attribution_models.csv', index=False)\ntouchpoints_with_conversion.to_csv('touchpoint_attribution_detail.csv', index=False)\ndaily_spend.to_csv('marketing_mix_features.csv', index=False)\n] PASSED [ 28%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineVisualizations::test_mermaid_visualization[iot_predictive_maintenance-\nimport pandas as pd\nimport numpy as np\n\n# Load IoT sensor data\nsensor_readings = pd.read_csv('sensor_readings.csv')\nequipment_registry = pd.read_csv('equipment_registry.csv')\nmaintenance_history = pd.read_csv('maintenance_history.csv')\nfailure_logs = pd.read_csv('failure_logs.csv')\noperating_conditions = pd.read_csv('operating_conditions.csv')\nparts_inventory = pd.read_csv('parts_inventory.csv')\ntechnician_schedules = pd.read_csv('technician_schedules.csv')\n\n# Parse timestamps\nsensor_readings['reading_time'] = pd.to_datetime(sensor_readings['reading_time'])\nmaintenance_history['maintenance_date'] = pd.to_datetime(maintenance_history['maintenance_date'])\nfailure_logs['failure_time'] = pd.to_datetime(failure_logs['failure_time'])\n\n# Clean sensor data - remove outliers\nsensor_readings = sensor_readings.sort_values(['equipment_id', 'sensor_type', 'reading_time'])\n\n# Calculate rolling statistics per sensor\nsensor_readings['value_1h_mean'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].transform(lambda x: x.rolling('1H', on=sensor_readings['reading_time']).mean())\n\nsensor_readings['value_1h_std'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].transform(lambda x: x.rolling('1H', on=sensor_readings['reading_time']).std())\n\nsensor_readings['value_24h_mean'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].transform(lambda x: x.rolling('24H', on=sensor_readings['reading_time']).mean())\n\nsensor_readings['value_24h_max'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].transform(lambda x: x.rolling('24H', on=sensor_readings['reading_time']).max())\n\nsensor_readings['value_24h_min'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].transform(lambda x: x.rolling('24H', on=sensor_readings['reading_time']).min())\n\n# Calculate rate of change\nsensor_readings['value_diff'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].diff()\n\nsensor_readings['value_pct_change'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].pct_change()\n\n# Detect anomalies using z-score\nsensor_readings['z_score'] = (\n    (sensor_readings['sensor_value'] - sensor_readings['value_24h_mean']) /\n    sensor_readings['value_1h_std'].replace(0, 1)\n)\nsensor_readings['is_anomaly'] = (abs(sensor_readings['z_score']) > 3).astype(int)\n\n# Pivot sensor types to create feature vectors\nsensor_features = sensor_readings.pivot_table(\n    index=['equipment_id', 'reading_time'],\n    columns='sensor_type',\n    values=['sensor_value', 'value_1h_mean', 'value_24h_max', 'z_score', 'is_anomaly'],\n    aggfunc='first'\n).reset_index()\n\n# Flatten column names\nsensor_features.columns = ['_'.join(str(c) for c in col).strip('_')\n                           for col in sensor_features.columns]\n\n# Add equipment metadata\nsensor_features = pd.merge(\n    sensor_features,\n    equipment_registry[['equipment_id', 'equipment_type', 'manufacturer',\n                        'installation_date', 'rated_capacity', 'location']],\n    on='equipment_id',\n    how='left'\n)\n\n# Calculate equipment age\nsensor_features['installation_date'] = pd.to_datetime(sensor_features['installation_date'])\nsensor_features['equipment_age_days'] = (\n    sensor_features['reading_time'] - sensor_features['installation_date']\n).dt.days\n\n# Add operating conditions\noperating_conditions['condition_time'] = pd.to_datetime(operating_conditions['condition_time'])\nsensor_features = pd.merge_asof(\n    sensor_features.sort_values('reading_time'),\n    operating_conditions.sort_values('condition_time'),\n    left_on='reading_time',\n    right_on='condition_time',\n    by='equipment_id',\n    direction='backward'\n)\n\n# Calculate maintenance history features\nmaintenance_agg = maintenance_history.groupby('equipment_id').agg({\n    'maintenance_id': 'count',\n    'maintenance_date': 'max',\n    'maintenance_cost': 'sum',\n    'downtime_hours': 'sum'\n}).reset_index()\nmaintenance_agg.columns = ['equipment_id', 'total_maintenance_count',\n                           'last_maintenance_date', 'total_maintenance_cost',\n                           'total_downtime_hours']\n\nsensor_features = pd.merge(\n    sensor_features,\n    maintenance_agg,\n    on='equipment_id',\n    how='left'\n)\n\n# Calculate days since last maintenance\nsensor_features['last_maintenance_date'] = pd.to_datetime(sensor_features['last_maintenance_date'])\nsensor_features['days_since_maintenance'] = (\n    sensor_features['reading_time'] - sensor_features['last_maintenance_date']\n).dt.days\n\n# Calculate failure history features\nfailure_agg = failure_logs.groupby('equipment_id').agg({\n    'failure_id': 'count',\n    'failure_time': 'max',\n    'failure_severity': 'mean',\n    'repair_time_hours': 'sum'\n}).reset_index()\nfailure_agg.columns = ['equipment_id', 'total_failures', 'last_failure_date',\n                       'avg_failure_severity', 'total_repair_hours']\n\nsensor_features = pd.merge(\n    sensor_features,\n    failure_agg,\n    on='equipment_id',\n    how='left'\n)\n\n# Create target variable: failure within next N days\nfailure_logs_sorted = failure_logs.sort_values(['equipment_id', 'failure_time'])\nsensor_features = pd.merge_asof(\n    sensor_features.sort_values('reading_time'),\n    failure_logs_sorted[['equipment_id', 'failure_time']].rename(\n        columns={'failure_time': 'next_failure_time'}\n    ),\n    left_on='reading_time',\n    right_on='next_failure_time',\n    by='equipment_id',\n    direction='forward'\n)\n\nsensor_features['days_to_failure'] = (\n    sensor_features['next_failure_time'] - sensor_features['reading_time']\n).dt.total_seconds() / 86400\n\nsensor_features['failure_within_7d'] = (sensor_features['days_to_failure'] <= 7).astype(int)\nsensor_features['failure_within_30d'] = (sensor_features['days_to_failure'] <= 30).astype(int)\n\n# Calculate health score\nsensor_features['anomaly_count_24h'] = sensor_features.groupby('equipment_id')['is_anomaly_temperature'].transform(\n    lambda x: x.rolling('24H', on=sensor_features['reading_time']).sum()\n)\n\nsensor_features['health_score'] = 100 - (\n    sensor_features['anomaly_count_24h'] * 5 +\n    sensor_features['total_failures'].fillna(0) * 2 +\n    np.minimum(sensor_features['days_since_maintenance'].fillna(0) / 365, 1) * 20\n).clip(0, 100)\n\n# Prioritize maintenance\nsensor_features['maintenance_priority'] = np.where(\n    sensor_features['health_score'] < 50, 'Critical',\n    np.where(sensor_features['health_score'] < 70, 'High',\n    np.where(sensor_features['health_score'] < 85, 'Medium', 'Low'))\n)\n\n# Join parts availability\nparts_available = parts_inventory.groupby('equipment_type').agg({\n    'part_id': 'count',\n    'quantity_available': 'sum',\n    'lead_time_days': 'mean'\n}).reset_index()\nparts_available.columns = ['equipment_type', 'part_types_available',\n                           'total_parts_stock', 'avg_part_lead_time']\n\nsensor_features = pd.merge(\n    sensor_features,\n    parts_available,\n    on='equipment_type',\n    how='left'\n)\n\n# Save outputs\nsensor_features.to_csv('predictive_maintenance_features.csv', index=False)\n\n# Create maintenance schedule recommendations\nmaintenance_schedule = sensor_features[sensor_features['maintenance_priority'].isin(['Critical', 'High'])].groupby(\n    'equipment_id'\n).agg({\n    'health_score': 'min',\n    'maintenance_priority': 'first',\n    'days_since_maintenance': 'max',\n    'reading_time': 'max'\n}).reset_index()\n\nmaintenance_schedule.columns = ['equipment_id', 'current_health', 'priority',\n                                'days_since_last_maintenance', 'last_reading']\nmaintenance_schedule.to_csv('maintenance_schedule_recommendations.csv', index=False)\n] PASSED [ 28%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineVisualizations::test_mermaid_visualization[genomic_analysis-\nimport pandas as pd\nimport numpy as np\n\n# Load genomic data\nvariants = pd.read_csv('variants.csv')\nsamples = pd.read_csv('samples.csv')\ngene_annotations = pd.read_csv('gene_annotations.csv')\nclinical_data = pd.read_csv('clinical_data.csv')\npathway_mappings = pd.read_csv('pathway_mappings.csv')\npopulation_frequencies = pd.read_csv('population_frequencies.csv')\nfunctional_predictions = pd.read_csv('functional_predictions.csv')\ndisease_associations = pd.read_csv('disease_associations.csv')\n\n# Standardize variant identifiers\nvariants['variant_id'] = (\n    variants['chromosome'].astype(str) + ':' +\n    variants['position'].astype(str) + ':' +\n    variants['reference'] + '>' + variants['alternate']\n)\n\n# Calculate variant quality metrics\nvariants['quality_pass'] = (\n    (variants['quality_score'] >= 30) &\n    (variants['read_depth'] >= 10) &\n    (variants['allele_frequency'] >= 0.2)\n).astype(int)\n\n# Filter high-quality variants\nhigh_quality_variants = variants[variants['quality_pass'] == 1].copy()\n\n# Annotate with gene information\nhigh_quality_variants = pd.merge(\n    high_quality_variants,\n    gene_annotations[['chromosome', 'start_position', 'end_position',\n                      'gene_symbol', 'gene_type', 'strand']],\n    on='chromosome',\n    how='left'\n)\n\n# Filter to variants within gene boundaries\nhigh_quality_variants = high_quality_variants[\n    (high_quality_variants['position'] >= high_quality_variants['start_position']) &\n    (high_quality_variants['position'] <= high_quality_variants['end_position'])\n]\n\n# Add functional impact predictions\nhigh_quality_variants = pd.merge(\n    high_quality_variants,\n    functional_predictions[['variant_id', 'sift_score', 'polyphen_score',\n                            'cadd_score', 'predicted_impact']],\n    on='variant_id',\n    how='left'\n)\n\n# Classify variant impact\nhigh_quality_variants['impact_category'] = np.where(\n    (high_quality_variants['sift_score'] < 0.05) | (high_quality_variants['polyphen_score'] > 0.85),\n    'Damaging',\n    np.where(\n        (high_quality_variants['sift_score'] < 0.1) | (high_quality_variants['polyphen_score'] > 0.5),\n        'Possibly_Damaging',\n        'Benign'\n    )\n)\n\n# Add population allele frequencies\nhigh_quality_variants = pd.merge(\n    high_quality_variants,\n    population_frequencies[['variant_id', 'gnomad_af', 'gnomad_af_eas',\n                            'gnomad_af_eur', 'gnomad_af_afr']],\n    on='variant_id',\n    how='left'\n)\n\n# Flag rare variants (MAF < 1%)\nhigh_quality_variants['is_rare'] = (high_quality_variants['gnomad_af'] < 0.01).astype(int)\nhigh_quality_variants['is_ultra_rare'] = (high_quality_variants['gnomad_af'] < 0.001).astype(int)\n\n# Add disease associations\nhigh_quality_variants = pd.merge(\n    high_quality_variants,\n    disease_associations[['gene_symbol', 'disease_name', 'inheritance_pattern',\n                          'clinical_significance']],\n    on='gene_symbol',\n    how='left'\n)\n\n# Calculate per-sample variant burden\nsample_burden = high_quality_variants.groupby('sample_id').agg({\n    'variant_id': 'count',\n    'is_rare': 'sum',\n    'impact_category': lambda x: (x == 'Damaging').sum()\n}).reset_index()\nsample_burden.columns = ['sample_id', 'total_variants', 'rare_variants', 'damaging_variants']\n\n# Add clinical phenotype data\nsample_burden = pd.merge(\n    sample_burden,\n    samples[['sample_id', 'patient_id', 'tissue_type', 'collection_date']],\n    on='sample_id',\n    how='left'\n)\n\nsample_burden = pd.merge(\n    sample_burden,\n    clinical_data[['patient_id', 'diagnosis', 'age_at_diagnosis',\n                   'sex', 'ethnicity', 'family_history']],\n    on='patient_id',\n    how='left'\n)\n\n# Calculate gene-level burden\ngene_burden = high_quality_variants.groupby(['sample_id', 'gene_symbol']).agg({\n    'variant_id': 'count',\n    'is_rare': 'sum',\n    'impact_category': lambda x: (x == 'Damaging').sum(),\n    'cadd_score': 'max'\n}).reset_index()\ngene_burden.columns = ['sample_id', 'gene_symbol', 'variants_in_gene',\n                       'rare_in_gene', 'damaging_in_gene', 'max_cadd']\n\n# Add pathway information\ngene_burden = pd.merge(\n    gene_burden,\n    pathway_mappings[['gene_symbol', 'pathway_id', 'pathway_name']],\n    on='gene_symbol',\n    how='left'\n)\n\n# Calculate pathway-level burden\npathway_burden = gene_burden.groupby(['sample_id', 'pathway_id', 'pathway_name']).agg({\n    'gene_symbol': 'nunique',\n    'variants_in_gene': 'sum',\n    'damaging_in_gene': 'sum',\n    'max_cadd': 'max'\n}).reset_index()\npathway_burden.columns = ['sample_id', 'pathway_id', 'pathway_name',\n                          'genes_affected', 'pathway_variants',\n                          'pathway_damaging', 'pathway_max_cadd']\n\n# Identify potentially pathogenic variants\npathogenic_candidates = high_quality_variants[\n    (high_quality_variants['impact_category'] == 'Damaging') &\n    (high_quality_variants['is_rare'] == 1) &\n    (high_quality_variants['clinical_significance'].isin(['Pathogenic', 'Likely_Pathogenic']).fillna(False))\n].copy()\n\n# Create variant report\nvariant_report = pathogenic_candidates.groupby(['sample_id', 'gene_symbol']).agg({\n    'variant_id': lambda x: '; '.join(x),\n    'disease_name': 'first',\n    'inheritance_pattern': 'first',\n    'cadd_score': 'max'\n}).reset_index()\n\n# Save outputs\nhigh_quality_variants.to_csv('annotated_variants.csv', index=False)\nsample_burden.to_csv('sample_variant_burden.csv', index=False)\ngene_burden.to_csv('gene_level_burden.csv', index=False)\npathway_burden.to_csv('pathway_analysis.csv', index=False)\nvariant_report.to_csv('pathogenic_variant_report.csv', index=False)\n] PASSED [ 28%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineVisualizations::test_mermaid_visualization[clickstream_analysis-\nimport pandas as pd\nimport numpy as np\n\n# Load clickstream data\npage_views = pd.read_csv('page_views.csv')\nclick_events = pd.read_csv('click_events.csv')\nform_submissions = pd.read_csv('form_submissions.csv')\nsearch_queries = pd.read_csv('search_queries.csv')\nproduct_impressions = pd.read_csv('product_impressions.csv')\ncart_events = pd.read_csv('cart_events.csv')\nuser_agents = pd.read_csv('user_agents.csv')\ngeo_locations = pd.read_csv('geo_locations.csv')\n\n# Parse timestamps\npage_views['timestamp'] = pd.to_datetime(page_views['timestamp'])\nclick_events['timestamp'] = pd.to_datetime(click_events['timestamp'])\n\n# Combine all events\npage_views['event_type'] = 'page_view'\nclick_events['event_type'] = 'click'\nform_submissions['event_type'] = 'form_submit'\nsearch_queries['event_type'] = 'search'\ncart_events['event_type'] = cart_events['cart_action']\n\nall_events = pd.concat([\n    page_views[['session_id', 'user_id', 'timestamp', 'event_type', 'page_url', 'referrer']],\n    click_events[['session_id', 'user_id', 'timestamp', 'event_type', 'element_id', 'page_url']],\n    form_submissions[['session_id', 'user_id', 'timestamp', 'event_type', 'form_id', 'page_url']],\n    search_queries[['session_id', 'user_id', 'timestamp', 'event_type', 'query', 'results_count']],\n    cart_events[['session_id', 'user_id', 'timestamp', 'event_type', 'product_id', 'quantity']]\n], ignore_index=True, sort=False)\n\nall_events = all_events.sort_values(['session_id', 'timestamp'])\n\n# Session-level aggregations\nsession_metrics = all_events.groupby('session_id').agg({\n    'user_id': 'first',\n    'timestamp': ['min', 'max', 'count'],\n    'page_url': 'nunique',\n    'event_type': lambda x: x.value_counts().to_dict()\n}).reset_index()\n\nsession_metrics.columns = ['session_id', 'user_id', 'session_start',\n                            'session_end', 'total_events', 'unique_pages', 'event_breakdown']\n\n# Calculate session duration\nsession_metrics['session_duration_seconds'] = (\n    session_metrics['session_end'] - session_metrics['session_start']\n).dt.total_seconds()\n\n# Extract event counts\nsession_metrics['page_views'] = session_metrics['event_breakdown'].apply(\n    lambda x: x.get('page_view', 0) if isinstance(x, dict) else 0\n)\nsession_metrics['clicks'] = session_metrics['event_breakdown'].apply(\n    lambda x: x.get('click', 0) if isinstance(x, dict) else 0\n)\nsession_metrics['searches'] = session_metrics['event_breakdown'].apply(\n    lambda x: x.get('search', 0) if isinstance(x, dict) else 0\n)\nsession_metrics['cart_adds'] = session_metrics['event_breakdown'].apply(\n    lambda x: x.get('add_to_cart', 0) if isinstance(x, dict) else 0\n)\n\n# Calculate engagement metrics\nsession_metrics['pages_per_minute'] = (\n    session_metrics['unique_pages'] /\n    (session_metrics['session_duration_seconds'] / 60).replace(0, 1)\n)\nsession_metrics['events_per_page'] = (\n    session_metrics['total_events'] / session_metrics['unique_pages'].replace(0, 1)\n)\n\n# Identify bounce sessions\nsession_metrics['is_bounce'] = (\n    (session_metrics['unique_pages'] == 1) &\n    (session_metrics['session_duration_seconds'] < 10)\n).astype(int)\n\n# Create page flow sequences\npage_sequences = all_events[all_events['event_type'] == 'page_view'].copy()\npage_sequences['page_order'] = page_sequences.groupby('session_id').cumcount() + 1\npage_sequences['next_page'] = page_sequences.groupby('session_id')['page_url'].shift(-1)\n\n# Calculate page transitions\npage_transitions = page_sequences.groupby(['page_url', 'next_page']).size().reset_index(name='transition_count')\npage_transitions = page_transitions[page_transitions['next_page'].notna()]\n\n# Calculate exit rates per page\npage_stats = page_sequences.groupby('page_url').agg({\n    'session_id': 'count',\n    'next_page': lambda x: x.isna().sum()\n}).reset_index()\npage_stats.columns = ['page_url', 'total_views', 'exits']\npage_stats['exit_rate'] = page_stats['exits'] / page_stats['total_views']\n\n# Analyze search behavior\nsearch_analysis = search_queries.groupby('session_id').agg({\n    'query': ['count', lambda x: ' | '.join(x)],\n    'results_count': ['mean', 'min'],\n    'clicked_result': 'sum'\n}).reset_index()\nsearch_analysis.columns = ['session_id', 'search_count', 'search_queries',\n                            'avg_results', 'min_results', 'result_clicks']\nsearch_analysis['search_success_rate'] = (\n    search_analysis['result_clicks'] / search_analysis['search_count'].replace(0, 1)\n)\n\n# Analyze cart behavior\ncart_analysis = cart_events.groupby('session_id').agg({\n    'product_id': 'nunique',\n    'quantity': 'sum',\n    'cart_action': lambda x: (x == 'add_to_cart').sum()\n}).reset_index()\ncart_analysis.columns = ['session_id', 'unique_cart_products',\n                          'total_cart_quantity', 'add_to_cart_events']\n\n# Add purchase flag\npurchases = cart_events[cart_events['cart_action'] == 'purchase'].groupby('session_id').agg({\n    'order_value': 'sum'\n}).reset_index()\n\ncart_analysis = pd.merge(cart_analysis, purchases, on='session_id', how='left')\ncart_analysis['converted'] = cart_analysis['order_value'].notna().astype(int)\ncart_analysis['order_value'] = cart_analysis['order_value'].fillna(0)\n\n# Merge all session data\nsession_complete = session_metrics.copy()\nsession_complete = pd.merge(session_complete, search_analysis, on='session_id', how='left')\nsession_complete = pd.merge(session_complete, cart_analysis, on='session_id', how='left')\n\n# Add user agent info\nsession_complete = pd.merge(\n    session_complete,\n    user_agents[['session_id', 'device_type', 'browser', 'os', 'is_mobile']],\n    on='session_id',\n    how='left'\n)\n\n# Add geo location\nsession_complete = pd.merge(\n    session_complete,\n    geo_locations[['session_id', 'country', 'region', 'city', 'timezone']],\n    on='session_id',\n    how='left'\n)\n\n# Calculate user-level aggregations\nuser_metrics = session_complete.groupby('user_id').agg({\n    'session_id': 'count',\n    'session_duration_seconds': ['mean', 'sum'],\n    'page_views': ['mean', 'sum'],\n    'converted': 'sum',\n    'order_value': 'sum',\n    'session_start': ['min', 'max']\n}).reset_index()\n\nuser_metrics.columns = ['user_id', 'total_sessions', 'avg_session_duration',\n                        'total_time_spent', 'avg_pages_per_session', 'total_pages_viewed',\n                        'total_conversions', 'total_revenue', 'first_visit', 'last_visit']\n\nuser_metrics['conversion_rate'] = (\n    user_metrics['total_conversions'] / user_metrics['total_sessions']\n)\nuser_metrics['avg_order_value'] = (\n    user_metrics['total_revenue'] / user_metrics['total_conversions'].replace(0, 1)\n)\n\n# Segment users by engagement\nuser_metrics['engagement_segment'] = pd.cut(\n    user_metrics['total_time_spent'] / 3600,\n    bins=[-np.inf, 0.1, 1, 5, 20, np.inf],\n    labels=['Minimal', 'Light', 'Medium', 'Heavy', 'Power']\n)\n\n# Save outputs\nsession_complete.to_csv('session_analytics.csv', index=False)\nuser_metrics.to_csv('user_engagement_metrics.csv', index=False)\npage_stats.to_csv('page_performance.csv', index=False)\npage_transitions.to_csv('page_flow_analysis.csv', index=False)\n] PASSED [ 29%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineVisualizations::test_mermaid_visualization[portfolio_risk-\nimport pandas as pd\nimport numpy as np\n\n# Load financial data\npositions = pd.read_csv('positions.csv')\nprice_history = pd.read_csv('price_history.csv')\ninstruments = pd.read_csv('instruments.csv')\nmarket_data = pd.read_csv('market_data.csv')\nfx_rates = pd.read_csv('fx_rates.csv')\nbenchmark_returns = pd.read_csv('benchmark_returns.csv')\nfactor_exposures = pd.read_csv('factor_exposures.csv')\ncredit_ratings = pd.read_csv('credit_ratings.csv')\n\n# Parse dates\nprice_history['date'] = pd.to_datetime(price_history['date'])\nmarket_data['date'] = pd.to_datetime(market_data['date'])\nfx_rates['date'] = pd.to_datetime(fx_rates['date'])\n\n# Calculate daily returns\nprice_history = price_history.sort_values(['instrument_id', 'date'])\nprice_history['daily_return'] = price_history.groupby('instrument_id')['close_price'].pct_change()\nprice_history['log_return'] = np.log(\n    price_history['close_price'] / price_history.groupby('instrument_id')['close_price'].shift(1)\n)\n\n# Calculate rolling volatility\nprice_history['volatility_20d'] = price_history.groupby('instrument_id')['daily_return'].transform(\n    lambda x: x.rolling(20).std() * np.sqrt(252)\n)\nprice_history['volatility_60d'] = price_history.groupby('instrument_id')['daily_return'].transform(\n    lambda x: x.rolling(60).std() * np.sqrt(252)\n)\n\n# Calculate rolling beta to market\nmarket_returns = market_data[market_data['index_id'] == 'SPX'][['date', 'daily_return']].copy()\nmarket_returns.columns = ['date', 'market_return']\n\nprice_with_market = pd.merge(price_history, market_returns, on='date', how='left')\n\ndef calculate_beta(group):\n    if len(group) < 60:\n        return np.nan\n    cov = group['daily_return'].cov(group['market_return'])\n    var = group['market_return'].var()\n    return cov / var if var != 0 else np.nan\n\nprice_with_market['beta'] = price_with_market.groupby('instrument_id').apply(\n    lambda x: x['daily_return'].rolling(60).cov(x['market_return']) /\n              x['market_return'].rolling(60).var()\n).reset_index(level=0, drop=True)\n\n# Enrich positions with instrument details\npositions_enriched = pd.merge(\n    positions,\n    instruments[['instrument_id', 'instrument_type', 'currency', 'sector',\n                 'country', 'maturity_date', 'coupon_rate', 'issuer']],\n    on='instrument_id',\n    how='left'\n)\n\n# Get latest prices and metrics\nlatest_prices = price_with_market.groupby('instrument_id').last().reset_index()\npositions_enriched = pd.merge(\n    positions_enriched,\n    latest_prices[['instrument_id', 'close_price', 'daily_return',\n                   'volatility_20d', 'volatility_60d', 'beta']],\n    on='instrument_id',\n    how='left'\n)\n\n# Convert to base currency (USD)\nfx_latest = fx_rates.groupby('currency_pair').last().reset_index()\nfx_latest['to_currency'] = fx_latest['currency_pair'].str[-3:]\nfx_latest['from_currency'] = fx_latest['currency_pair'].str[:3]\n\npositions_enriched = pd.merge(\n    positions_enriched,\n    fx_latest[['from_currency', 'rate']].rename(columns={'from_currency': 'currency', 'rate': 'fx_rate'}),\n    on='currency',\n    how='left'\n)\npositions_enriched['fx_rate'] = positions_enriched['fx_rate'].fillna(1.0)\n\n# Calculate market values\npositions_enriched['market_value_local'] = positions_enriched['quantity'] * positions_enriched['close_price']\npositions_enriched['market_value_usd'] = positions_enriched['market_value_local'] / positions_enriched['fx_rate']\n\n# Calculate position-level risk metrics\npositions_enriched['position_volatility'] = (\n    positions_enriched['market_value_usd'] * positions_enriched['volatility_20d']\n)\n\n# Portfolio-level calculations\nportfolio_value = positions_enriched.groupby('portfolio_id')['market_value_usd'].sum().reset_index()\nportfolio_value.columns = ['portfolio_id', 'total_nav']\n\npositions_enriched = pd.merge(positions_enriched, portfolio_value, on='portfolio_id', how='left')\npositions_enriched['weight'] = positions_enriched['market_value_usd'] / positions_enriched['total_nav']\n\n# Calculate weighted portfolio beta\nportfolio_beta = positions_enriched.groupby('portfolio_id').apply(\n    lambda x: (x['weight'] * x['beta']).sum()\n).reset_index(name='portfolio_beta')\n\n# Calculate portfolio volatility (simplified - assuming no correlation)\nportfolio_vol_contrib = positions_enriched.groupby('portfolio_id').apply(\n    lambda x: np.sqrt((x['weight']**2 * x['volatility_20d']**2).sum())\n).reset_index(name='portfolio_volatility')\n\n# Sector concentration\nsector_concentration = positions_enriched.groupby(['portfolio_id', 'sector']).agg({\n    'market_value_usd': 'sum',\n    'weight': 'sum'\n}).reset_index()\nsector_concentration.columns = ['portfolio_id', 'sector', 'sector_value', 'sector_weight']\n\n# Country concentration\ncountry_concentration = positions_enriched.groupby(['portfolio_id', 'country']).agg({\n    'market_value_usd': 'sum',\n    'weight': 'sum'\n}).reset_index()\n\n# Add credit risk for fixed income\npositions_enriched = pd.merge(\n    positions_enriched,\n    credit_ratings[['issuer', 'rating', 'rating_numeric', 'default_probability']],\n    on='issuer',\n    how='left'\n)\n\n# Calculate credit VaR contribution\npositions_enriched['credit_var_contrib'] = (\n    positions_enriched['market_value_usd'] *\n    positions_enriched['default_probability'].fillna(0) *\n    (1 - 0.4)  # Assuming 40% recovery rate\n)\n\n# Calculate VaR (parametric approach)\nconfidence_level = 0.99\nz_score = 2.326  # 99% confidence\n\npositions_enriched['var_1d'] = (\n    positions_enriched['market_value_usd'] *\n    positions_enriched['volatility_20d'] / np.sqrt(252) *\n    z_score\n)\n\nportfolio_var = positions_enriched.groupby('portfolio_id')['var_1d'].sum().reset_index()\nportfolio_var.columns = ['portfolio_id', 'total_var_1d']\n\n# Historical VaR calculation\nreturns_pivot = price_history.pivot_table(\n    index='date',\n    columns='instrument_id',\n    values='daily_return'\n).fillna(0)\n\n# Calculate portfolio returns for each portfolio\nportfolio_returns = []\nfor portfolio_id in positions_enriched['portfolio_id'].unique():\n    port_positions = positions_enriched[positions_enriched['portfolio_id'] == portfolio_id]\n    weights = port_positions.set_index('instrument_id')['weight'].to_dict()\n\n    port_return = returns_pivot[list(weights.keys())].mul(\n        pd.Series(weights)\n    ).sum(axis=1)\n\n    portfolio_returns.append(pd.DataFrame({\n        'portfolio_id': portfolio_id,\n        'date': returns_pivot.index,\n        'portfolio_return': port_return\n    }))\n\nportfolio_returns_df = pd.concat(portfolio_returns, ignore_index=True)\n\n# Calculate historical VaR\nhistorical_var = portfolio_returns_df.groupby('portfolio_id')['portfolio_return'].apply(\n    lambda x: x.quantile(1 - confidence_level)\n).reset_index(name='historical_var_1d')\n\n# Calculate Sharpe ratio\nrisk_free_rate = 0.05 / 252  # Daily risk-free rate\n\nportfolio_sharpe = portfolio_returns_df.groupby('portfolio_id').agg({\n    'portfolio_return': ['mean', 'std']\n}).reset_index()\nportfolio_sharpe.columns = ['portfolio_id', 'avg_return', 'return_std']\nportfolio_sharpe['sharpe_ratio'] = (\n    (portfolio_sharpe['avg_return'] - risk_free_rate) / portfolio_sharpe['return_std']\n) * np.sqrt(252)\n\n# Compile portfolio risk summary\nportfolio_risk = portfolio_value.copy()\nportfolio_risk = pd.merge(portfolio_risk, portfolio_beta, on='portfolio_id', how='left')\nportfolio_risk = pd.merge(portfolio_risk, portfolio_vol_contrib, on='portfolio_id', how='left')\nportfolio_risk = pd.merge(portfolio_risk, portfolio_var, on='portfolio_id', how='left')\nportfolio_risk = pd.merge(portfolio_risk, historical_var, on='portfolio_id', how='left')\nportfolio_risk = pd.merge(portfolio_risk, portfolio_sharpe, on='portfolio_id', how='left')\n\n# Save outputs\npositions_enriched.to_csv('positions_with_risk.csv', index=False)\nportfolio_risk.to_csv('portfolio_risk_summary.csv', index=False)\nsector_concentration.to_csv('sector_concentration.csv', index=False)\ncountry_concentration.to_csv('country_concentration.csv', index=False)\nportfolio_returns_df.to_csv('portfolio_returns_history.csv', index=False)\n] PASSED [ 29%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineMetrics::test_pipeline_has_datasets[fraud_detection-\nimport pandas as pd\nimport numpy as np\n\n# Load transaction streams\ntransactions = pd.read_csv('transactions.csv')\nuser_profiles = pd.read_csv('user_profiles.csv')\ndevice_fingerprints = pd.read_csv('device_fingerprints.csv')\nmerchant_risk_scores = pd.read_csv('merchant_risk_scores.csv')\nhistorical_fraud = pd.read_csv('historical_fraud.csv')\nip_geolocation = pd.read_csv('ip_geolocation.csv')\n\n# Parse timestamps\ntransactions['timestamp'] = pd.to_datetime(transactions['timestamp'])\ntransactions['date'] = transactions['timestamp'].dt.date\ntransactions['hour'] = transactions['timestamp'].dt.hour\ntransactions['minute'] = transactions['timestamp'].dt.minute\ntransactions['day_of_week'] = transactions['timestamp'].dt.dayofweek\ntransactions['is_weekend'] = transactions['day_of_week'].isin([5, 6]).astype(int)\ntransactions['is_night'] = transactions['hour'].between(0, 6).astype(int)\n\n# Enrich with user profile data\ntransactions_enriched = pd.merge(\n    transactions,\n    user_profiles[['user_id', 'account_age_days', 'verification_level',\n                   'avg_transaction_amount', 'typical_login_hour',\n                   'home_country', 'risk_tier']],\n    on='user_id',\n    how='left'\n)\n\n# Add device fingerprint risk\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    device_fingerprints[['device_id', 'device_trust_score', 'is_known_device',\n                         'browser_anomaly_score', 'last_seen_country']],\n    on='device_id',\n    how='left'\n)\n\n# Add merchant risk scores\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    merchant_risk_scores[['merchant_id', 'merchant_risk_score',\n                          'chargeback_rate', 'merchant_category']],\n    on='merchant_id',\n    how='left'\n)\n\n# Add IP geolocation\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    ip_geolocation[['ip_address', 'ip_country', 'ip_city',\n                    'is_vpn', 'is_tor', 'ip_risk_score']],\n    on='ip_address',\n    how='left'\n)\n\n# Calculate velocity features - transactions per user in last N minutes\ntransactions_enriched = transactions_enriched.sort_values(['user_id', 'timestamp'])\n\n# Rolling window aggregations per user\nuser_velocity = transactions_enriched.groupby('user_id').agg({\n    'transaction_id': 'count',\n    'amount': ['sum', 'mean', 'std', 'max'],\n    'merchant_id': 'nunique',\n    'ip_address': 'nunique',\n    'device_id': 'nunique'\n}).reset_index()\nuser_velocity.columns = ['user_id', 'tx_count_session', 'total_amount_session',\n                         'avg_amount_session', 'std_amount_session', 'max_amount_session',\n                         'unique_merchants', 'unique_ips', 'unique_devices']\n\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    user_velocity,\n    on='user_id',\n    how='left'\n)\n\n# Calculate deviation from user's typical behavior\ntransactions_enriched['amount_deviation'] = (\n    (transactions_enriched['amount'] - transactions_enriched['avg_transaction_amount']) /\n    transactions_enriched['avg_transaction_amount'].replace(0, 1)\n)\n\ntransactions_enriched['hour_deviation'] = abs(\n    transactions_enriched['hour'] - transactions_enriched['typical_login_hour']\n)\n\n# Geographic anomaly detection\ntransactions_enriched['country_mismatch'] = (\n    transactions_enriched['ip_country'] != transactions_enriched['home_country']\n).astype(int)\n\ntransactions_enriched['device_country_mismatch'] = (\n    transactions_enriched['ip_country'] != transactions_enriched['last_seen_country']\n).astype(int)\n\n# Composite risk score calculation\ntransactions_enriched['composite_risk_score'] = (\n    transactions_enriched['ip_risk_score'] * 0.2 +\n    transactions_enriched['merchant_risk_score'] * 0.15 +\n    (1 - transactions_enriched['device_trust_score']) * 0.2 +\n    transactions_enriched['amount_deviation'].clip(0, 5) * 0.15 +\n    transactions_enriched['country_mismatch'] * 0.15 +\n    transactions_enriched['is_vpn'] * 0.1 +\n    transactions_enriched['is_tor'] * 0.05\n)\n\n# Flag high-risk transactions\ntransactions_enriched['is_high_risk'] = (\n    transactions_enriched['composite_risk_score'] > 0.7\n).astype(int)\n\n# Join historical fraud patterns\nfraud_patterns = historical_fraud.groupby('user_id').agg({\n    'fraud_flag': 'sum',\n    'transaction_id': 'count'\n}).reset_index()\nfraud_patterns.columns = ['user_id', 'historical_fraud_count', 'historical_tx_count']\nfraud_patterns['historical_fraud_rate'] = (\n    fraud_patterns['historical_fraud_count'] / fraud_patterns['historical_tx_count']\n)\n\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    fraud_patterns[['user_id', 'historical_fraud_rate']],\n    on='user_id',\n    how='left'\n)\ntransactions_enriched['historical_fraud_rate'] = transactions_enriched['historical_fraud_rate'].fillna(0)\n\n# Final feature selection for ML model\nml_features = transactions_enriched[[\n    'transaction_id', 'user_id', 'amount', 'amount_deviation',\n    'hour_deviation', 'is_weekend', 'is_night',\n    'device_trust_score', 'is_known_device', 'browser_anomaly_score',\n    'merchant_risk_score', 'chargeback_rate',\n    'ip_risk_score', 'is_vpn', 'is_tor',\n    'country_mismatch', 'device_country_mismatch',\n    'tx_count_session', 'unique_merchants', 'unique_ips', 'unique_devices',\n    'composite_risk_score', 'historical_fraud_rate', 'is_high_risk'\n]].copy()\n\nml_features.to_csv('fraud_detection_features.csv', index=False)\ntransactions_enriched.to_csv('transactions_enriched_full.csv', index=False)\n] PASSED [ 29%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineMetrics::test_pipeline_has_datasets[customer_360-\nimport pandas as pd\nimport numpy as np\n\n# Load data from multiple source systems\ncrm_customers = pd.read_csv('crm_customers.csv')\necommerce_users = pd.read_csv('ecommerce_users.csv')\nmobile_app_users = pd.read_csv('mobile_app_users.csv')\ncall_center_interactions = pd.read_csv('call_center_interactions.csv')\nemail_campaigns = pd.read_csv('email_campaigns.csv')\nweb_analytics = pd.read_csv('web_analytics.csv')\nsocial_media = pd.read_csv('social_media.csv')\nloyalty_program = pd.read_csv('loyalty_program.csv')\nsupport_tickets = pd.read_csv('support_tickets.csv')\nnps_surveys = pd.read_csv('nps_surveys.csv')\n\n# Standardize customer identifiers across systems\ncrm_customers['source'] = 'crm'\ncrm_customers['master_id'] = crm_customers['crm_id']\n\necommerce_users['source'] = 'ecommerce'\necommerce_users = pd.merge(\n    ecommerce_users,\n    crm_customers[['email', 'crm_id']],\n    on='email',\n    how='left'\n)\necommerce_users['master_id'] = ecommerce_users['crm_id'].fillna(\n    'ECO_' + ecommerce_users['ecommerce_user_id'].astype(str)\n)\n\nmobile_app_users['source'] = 'mobile'\nmobile_app_users = pd.merge(\n    mobile_app_users,\n    crm_customers[['phone', 'crm_id']],\n    on='phone',\n    how='left'\n)\nmobile_app_users['master_id'] = mobile_app_users['crm_id'].fillna(\n    'MOB_' + mobile_app_users['app_user_id'].astype(str)\n)\n\n# Create unified customer profile\ncustomer_base = crm_customers[['master_id', 'email', 'phone', 'first_name',\n                               'last_name', 'date_of_birth', 'gender',\n                               'address', 'city', 'state', 'country',\n                               'customer_since']].copy()\n\n# Aggregate e-commerce behavior\necommerce_metrics = ecommerce_users.groupby('master_id').agg({\n    'order_id': 'count',\n    'order_total': ['sum', 'mean', 'max'],\n    'items_purchased': 'sum',\n    'last_order_date': 'max',\n    'cart_abandonment_count': 'sum'\n}).reset_index()\necommerce_metrics.columns = ['master_id', 'ecom_order_count', 'ecom_total_spend',\n                              'ecom_avg_order', 'ecom_max_order', 'ecom_items_total',\n                              'ecom_last_order', 'ecom_cart_abandonments']\n\n# Aggregate mobile app engagement\nmobile_metrics = mobile_app_users.groupby('master_id').agg({\n    'session_count': 'sum',\n    'total_time_minutes': 'sum',\n    'push_notifications_clicked': 'sum',\n    'app_crashes': 'sum',\n    'features_used': lambda x: len(set(','.join(x.dropna()).split(','))),\n    'last_active_date': 'max'\n}).reset_index()\nmobile_metrics.columns = ['master_id', 'app_sessions', 'app_time_minutes',\n                          'push_clicks', 'app_crashes', 'features_used_count',\n                          'app_last_active']\n\n# Aggregate call center interactions\ncall_metrics = call_center_interactions.groupby('master_id').agg({\n    'call_id': 'count',\n    'call_duration_seconds': ['sum', 'mean'],\n    'issue_resolved': 'mean',\n    'escalated': 'sum',\n    'sentiment_score': 'mean'\n}).reset_index()\ncall_metrics.columns = ['master_id', 'call_count', 'total_call_duration',\n                        'avg_call_duration', 'resolution_rate',\n                        'escalation_count', 'call_sentiment']\n\n# Aggregate email engagement\nemail_metrics = email_campaigns.groupby('master_id').agg({\n    'email_sent': 'sum',\n    'email_opened': 'sum',\n    'email_clicked': 'sum',\n    'unsubscribed': 'max'\n}).reset_index()\nemail_metrics['email_open_rate'] = email_metrics['email_opened'] / email_metrics['email_sent']\nemail_metrics['email_click_rate'] = email_metrics['email_clicked'] / email_metrics['email_opened'].replace(0, 1)\n\n# Aggregate web analytics\nweb_metrics = web_analytics.groupby('master_id').agg({\n    'page_views': 'sum',\n    'unique_sessions': 'sum',\n    'bounce_rate': 'mean',\n    'avg_session_duration': 'mean',\n    'conversion_events': 'sum'\n}).reset_index()\n\n# Aggregate loyalty program data\nloyalty_metrics = loyalty_program.groupby('master_id').agg({\n    'points_earned': 'sum',\n    'points_redeemed': 'sum',\n    'tier_level': 'max',\n    'referrals_made': 'sum'\n}).reset_index()\nloyalty_metrics['points_balance'] = loyalty_metrics['points_earned'] - loyalty_metrics['points_redeemed']\n\n# Aggregate support tickets\nsupport_metrics = support_tickets.groupby('master_id').agg({\n    'ticket_id': 'count',\n    'resolution_time_hours': 'mean',\n    'satisfaction_rating': 'mean',\n    'ticket_reopened': 'sum'\n}).reset_index()\nsupport_metrics.columns = ['master_id', 'ticket_count', 'avg_resolution_time',\n                           'support_satisfaction', 'tickets_reopened']\n\n# Aggregate NPS surveys\nnps_metrics = nps_surveys.groupby('master_id').agg({\n    'nps_score': 'mean',\n    'survey_id': 'count',\n    'would_recommend': 'mean'\n}).reset_index()\nnps_metrics.columns = ['master_id', 'avg_nps_score', 'surveys_completed', 'recommend_rate']\n\n# Merge all metrics into unified customer 360 view\ncustomer_360 = customer_base.copy()\n\nfor metrics_df in [ecommerce_metrics, mobile_metrics, call_metrics,\n                   email_metrics, web_metrics, loyalty_metrics,\n                   support_metrics, nps_metrics]:\n    customer_360 = pd.merge(customer_360, metrics_df, on='master_id', how='left')\n\n# Fill missing values appropriately\nnumeric_cols = customer_360.select_dtypes(include=[np.number]).columns\ncustomer_360[numeric_cols] = customer_360[numeric_cols].fillna(0)\n\n# Calculate derived metrics\ncustomer_360['total_interactions'] = (\n    customer_360['ecom_order_count'] +\n    customer_360['app_sessions'] +\n    customer_360['call_count'] +\n    customer_360['ticket_count']\n)\n\ncustomer_360['digital_engagement_score'] = (\n    customer_360['app_sessions'] * 0.3 +\n    customer_360['email_open_rate'] * 20 +\n    customer_360['page_views'] * 0.1 +\n    customer_360['push_clicks'] * 0.5\n).clip(0, 100)\n\ncustomer_360['customer_health_score'] = (\n    customer_360['avg_nps_score'] / 10 * 25 +\n    customer_360['resolution_rate'] * 25 +\n    customer_360['support_satisfaction'] / 5 * 25 +\n    (1 - customer_360['escalation_count'].clip(0, 5) / 5) * 25\n)\n\n# Customer segmentation\ncustomer_360['value_segment'] = pd.cut(\n    customer_360['ecom_total_spend'],\n    bins=[-np.inf, 100, 500, 2000, 10000, np.inf],\n    labels=['Dormant', 'Bronze', 'Silver', 'Gold', 'Platinum']\n)\n\ncustomer_360['engagement_segment'] = pd.cut(\n    customer_360['digital_engagement_score'],\n    bins=[-np.inf, 20, 40, 60, 80, np.inf],\n    labels=['Inactive', 'Low', 'Medium', 'High', 'Power User']\n)\n\n# Calculate churn risk\ncustomer_360['days_since_activity'] = (\n    pd.to_datetime('today') - pd.to_datetime(customer_360['ecom_last_order'])\n).dt.days.fillna(999)\n\ncustomer_360['churn_risk_score'] = (\n    customer_360['days_since_activity'] / 365 * 30 +\n    (1 - customer_360['email_open_rate'].fillna(0)) * 20 +\n    customer_360['tickets_reopened'] * 10 +\n    (10 - customer_360['avg_nps_score'].fillna(5)) * 4\n).clip(0, 100)\n\n# Save outputs\ncustomer_360.to_csv('customer_360_unified.csv', index=False)\n] PASSED [ 29%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineMetrics::test_pipeline_has_datasets[supply_chain-\nimport pandas as pd\nimport numpy as np\n\n# Load supply chain data\nsales_history = pd.read_csv('sales_history.csv')\ninventory_levels = pd.read_csv('inventory_levels.csv')\nsupplier_data = pd.read_csv('supplier_data.csv')\nwarehouse_locations = pd.read_csv('warehouse_locations.csv')\nshipping_costs = pd.read_csv('shipping_costs.csv')\nproduct_catalog = pd.read_csv('product_catalog.csv')\npromotions_calendar = pd.read_csv('promotions_calendar.csv')\nweather_data = pd.read_csv('weather_data.csv')\neconomic_indicators = pd.read_csv('economic_indicators.csv')\n\n# Parse dates\nsales_history['sale_date'] = pd.to_datetime(sales_history['sale_date'])\nsales_history['year'] = sales_history['sale_date'].dt.year\nsales_history['month'] = sales_history['sale_date'].dt.month\nsales_history['week'] = sales_history['sale_date'].dt.isocalendar().week\nsales_history['day_of_week'] = sales_history['sale_date'].dt.dayofweek\nsales_history['is_weekend'] = sales_history['day_of_week'].isin([5, 6]).astype(int)\n\n# Enrich sales with product info\nsales_enriched = pd.merge(\n    sales_history,\n    product_catalog[['product_id', 'category', 'subcategory', 'brand',\n                     'unit_cost', 'weight_kg', 'is_perishable', 'shelf_life_days']],\n    on='product_id',\n    how='left'\n)\n\n# Add promotion flags\npromotions_calendar['promo_date'] = pd.to_datetime(promotions_calendar['promo_date'])\nsales_enriched = pd.merge(\n    sales_enriched,\n    promotions_calendar[['product_id', 'promo_date', 'discount_pct', 'promo_type']],\n    left_on=['product_id', 'sale_date'],\n    right_on=['product_id', 'promo_date'],\n    how='left'\n)\nsales_enriched['is_promotion'] = sales_enriched['discount_pct'].notna().astype(int)\nsales_enriched['discount_pct'] = sales_enriched['discount_pct'].fillna(0)\n\n# Add weather impact\nweather_data['date'] = pd.to_datetime(weather_data['date'])\nsales_enriched = pd.merge(\n    sales_enriched,\n    weather_data[['date', 'region', 'temperature', 'precipitation', 'weather_condition']],\n    left_on=['sale_date', 'region'],\n    right_on=['date', 'region'],\n    how='left'\n)\n\n# Calculate time-based aggregations for demand patterns\ndaily_demand = sales_enriched.groupby(['product_id', 'warehouse_id', 'sale_date']).agg({\n    'quantity': 'sum',\n    'revenue': 'sum',\n    'is_promotion': 'max',\n    'temperature': 'mean'\n}).reset_index()\n\n# Calculate rolling averages for demand forecasting\ndaily_demand = daily_demand.sort_values(['product_id', 'warehouse_id', 'sale_date'])\ndaily_demand['demand_7d_avg'] = daily_demand.groupby(['product_id', 'warehouse_id'])['quantity'].transform(\n    lambda x: x.rolling(7, min_periods=1).mean()\n)\ndaily_demand['demand_30d_avg'] = daily_demand.groupby(['product_id', 'warehouse_id'])['quantity'].transform(\n    lambda x: x.rolling(30, min_periods=1).mean()\n)\ndaily_demand['demand_90d_avg'] = daily_demand.groupby(['product_id', 'warehouse_id'])['quantity'].transform(\n    lambda x: x.rolling(90, min_periods=1).mean()\n)\n\n# Calculate demand volatility\ndaily_demand['demand_7d_std'] = daily_demand.groupby(['product_id', 'warehouse_id'])['quantity'].transform(\n    lambda x: x.rolling(7, min_periods=1).std()\n)\ndaily_demand['demand_coefficient_variation'] = (\n    daily_demand['demand_7d_std'] / daily_demand['demand_7d_avg'].replace(0, 1)\n)\n\n# Seasonality detection\nmonthly_patterns = sales_enriched.groupby(['product_id', 'month']).agg({\n    'quantity': 'mean'\n}).reset_index()\nmonthly_patterns.columns = ['product_id', 'month', 'monthly_avg_demand']\n\nyearly_avg = monthly_patterns.groupby('product_id')['monthly_avg_demand'].transform('mean')\nmonthly_patterns['seasonality_index'] = monthly_patterns['monthly_avg_demand'] / yearly_avg\n\n# Join current inventory levels\ninventory_current = inventory_levels.groupby(['product_id', 'warehouse_id']).agg({\n    'quantity_on_hand': 'sum',\n    'quantity_reserved': 'sum',\n    'quantity_in_transit': 'sum',\n    'last_restock_date': 'max'\n}).reset_index()\ninventory_current['available_inventory'] = (\n    inventory_current['quantity_on_hand'] - inventory_current['quantity_reserved']\n)\n\n# Calculate days of supply\ninventory_analysis = pd.merge(\n    inventory_current,\n    daily_demand.groupby(['product_id', 'warehouse_id']).agg({\n        'demand_30d_avg': 'last'\n    }).reset_index(),\n    on=['product_id', 'warehouse_id'],\n    how='left'\n)\ninventory_analysis['days_of_supply'] = (\n    inventory_analysis['available_inventory'] /\n    inventory_analysis['demand_30d_avg'].replace(0, 0.1)\n)\n\n# Add supplier lead times\ninventory_analysis = pd.merge(\n    inventory_analysis,\n    supplier_data[['product_id', 'supplier_id', 'lead_time_days',\n                   'min_order_qty', 'unit_cost', 'reliability_score']],\n    on='product_id',\n    how='left'\n)\n\n# Calculate reorder points with safety stock\ninventory_analysis['safety_stock'] = (\n    inventory_analysis['demand_30d_avg'] *\n    inventory_analysis['lead_time_days'] * 0.5  # Safety factor\n)\ninventory_analysis['reorder_point'] = (\n    inventory_analysis['demand_30d_avg'] * inventory_analysis['lead_time_days'] +\n    inventory_analysis['safety_stock']\n)\n\n# Flag items needing reorder\ninventory_analysis['needs_reorder'] = (\n    inventory_analysis['available_inventory'] < inventory_analysis['reorder_point']\n).astype(int)\n\n# Calculate optimal order quantity (EOQ approximation)\ninventory_analysis['holding_cost_annual'] = inventory_analysis['unit_cost'] * 0.25\ninventory_analysis['order_cost'] = 50  # Fixed ordering cost\ninventory_analysis['annual_demand'] = inventory_analysis['demand_30d_avg'] * 12 * 30\n\ninventory_analysis['economic_order_qty'] = np.sqrt(\n    2 * inventory_analysis['annual_demand'] * inventory_analysis['order_cost'] /\n    inventory_analysis['holding_cost_annual'].replace(0, 1)\n)\n\n# Calculate shipping optimization\nshipping_analysis = pd.merge(\n    inventory_analysis,\n    warehouse_locations[['warehouse_id', 'latitude', 'longitude', 'capacity_units']],\n    on='warehouse_id',\n    how='left'\n)\n\n# Add shipping costs\nshipping_analysis = pd.merge(\n    shipping_analysis,\n    shipping_costs,\n    on='warehouse_id',\n    how='left'\n)\n\n# Calculate total landed cost\nshipping_analysis['landed_cost'] = (\n    shipping_analysis['unit_cost'] +\n    shipping_analysis['shipping_cost_per_unit'] +\n    shipping_analysis['handling_cost']\n)\n\n# Identify stock-out risk\nshipping_analysis['stockout_risk'] = np.where(\n    shipping_analysis['days_of_supply'] < shipping_analysis['lead_time_days'],\n    'High',\n    np.where(\n        shipping_analysis['days_of_supply'] < shipping_analysis['lead_time_days'] * 1.5,\n        'Medium',\n        'Low'\n    )\n)\n\n# Save outputs\ndaily_demand.to_csv('demand_forecast_features.csv', index=False)\ninventory_analysis.to_csv('inventory_optimization.csv', index=False)\nshipping_analysis.to_csv('supply_chain_analysis.csv', index=False)\nmonthly_patterns.to_csv('seasonality_patterns.csv', index=False)\n] PASSED [ 29%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineMetrics::test_pipeline_has_datasets[marketing_attribution-\nimport pandas as pd\nimport numpy as np\n\n# Load marketing data\ntouchpoints = pd.read_csv('touchpoints.csv')\nconversions = pd.read_csv('conversions.csv')\nad_spend = pd.read_csv('ad_spend.csv')\nchannel_costs = pd.read_csv('channel_costs.csv')\ncampaign_metadata = pd.read_csv('campaign_metadata.csv')\ncustomer_journeys = pd.read_csv('customer_journeys.csv')\norganic_traffic = pd.read_csv('organic_traffic.csv')\noffline_media = pd.read_csv('offline_media.csv')\n\n# Parse timestamps\ntouchpoints['touchpoint_time'] = pd.to_datetime(touchpoints['touchpoint_time'])\nconversions['conversion_time'] = pd.to_datetime(conversions['conversion_time'])\n\n# Create customer journey sequences\ntouchpoints_sorted = touchpoints.sort_values(['customer_id', 'touchpoint_time'])\n\n# Assign journey IDs (new journey if gap > 30 days)\ntouchpoints_sorted['time_since_last'] = touchpoints_sorted.groupby('customer_id')['touchpoint_time'].diff()\ntouchpoints_sorted['new_journey'] = (\n    touchpoints_sorted['time_since_last'] > pd.Timedelta(days=30)\n).fillna(True).astype(int)\ntouchpoints_sorted['journey_id'] = touchpoints_sorted.groupby('customer_id')['new_journey'].cumsum()\n\n# Create journey-level aggregations\njourney_touchpoints = touchpoints_sorted.groupby(['customer_id', 'journey_id']).agg({\n    'touchpoint_id': 'count',\n    'channel': lambda x: ' > '.join(x),\n    'campaign_id': lambda x: list(x.unique()),\n    'touchpoint_time': ['min', 'max'],\n    'cost': 'sum'\n}).reset_index()\njourney_touchpoints.columns = ['customer_id', 'journey_id', 'touchpoint_count',\n                                'channel_path', 'campaigns_touched',\n                                'journey_start', 'journey_end', 'journey_cost']\n\n# Calculate journey duration\njourney_touchpoints['journey_duration_days'] = (\n    journey_touchpoints['journey_end'] - journey_touchpoints['journey_start']\n).dt.total_seconds() / 86400\n\n# Join with conversions\njourney_conversions = pd.merge(\n    journey_touchpoints,\n    conversions[['customer_id', 'conversion_time', 'conversion_value', 'product_category']],\n    on='customer_id',\n    how='left'\n)\n\n# Filter to conversions within journey window\njourney_conversions['converted'] = (\n    (journey_conversions['conversion_time'] >= journey_conversions['journey_start']) &\n    (journey_conversions['conversion_time'] <= journey_conversions['journey_end'] + pd.Timedelta(days=7))\n).astype(int)\n\njourney_conversions['conversion_value'] = np.where(\n    journey_conversions['converted'] == 1,\n    journey_conversions['conversion_value'],\n    0\n)\n\n# Calculate attribution models\n\n# 1. First-touch attribution\ntouchpoints_sorted['is_first_touch'] = touchpoints_sorted.groupby(\n    ['customer_id', 'journey_id']\n).cumcount() == 0\n\nfirst_touch = touchpoints_sorted[touchpoints_sorted['is_first_touch']].copy()\nfirst_touch_attribution = first_touch.groupby('channel').agg({\n    'touchpoint_id': 'count',\n    'cost': 'sum'\n}).reset_index()\nfirst_touch_attribution.columns = ['channel', 'first_touch_count', 'first_touch_cost']\n\n# 2. Last-touch attribution\nlast_touch = touchpoints_sorted.groupby(['customer_id', 'journey_id']).last().reset_index()\nlast_touch_attribution = last_touch.groupby('channel').agg({\n    'touchpoint_id': 'count',\n    'cost': 'sum'\n}).reset_index()\nlast_touch_attribution.columns = ['channel', 'last_touch_count', 'last_touch_cost']\n\n# 3. Linear attribution (equal credit to all touchpoints)\njourney_conversions_exploded = journey_conversions.explode('campaigns_touched')\njourney_conversions_exploded['linear_credit'] = (\n    journey_conversions_exploded['conversion_value'] /\n    journey_conversions_exploded['touchpoint_count']\n)\n\n# 4. Time-decay attribution\ntouchpoints_with_conversion = pd.merge(\n    touchpoints_sorted,\n    journey_conversions[['customer_id', 'journey_id', 'conversion_time', 'conversion_value', 'converted']],\n    on=['customer_id', 'journey_id'],\n    how='left'\n)\n\ntouchpoints_with_conversion['days_to_conversion'] = (\n    touchpoints_with_conversion['conversion_time'] - touchpoints_with_conversion['touchpoint_time']\n).dt.total_seconds() / 86400\n\n# Exponential decay weight (half-life = 7 days)\ntouchpoints_with_conversion['decay_weight'] = np.exp(\n    -touchpoints_with_conversion['days_to_conversion'] / 7\n)\n\n# Normalize weights within journey\ntouchpoints_with_conversion['weight_sum'] = touchpoints_with_conversion.groupby(\n    ['customer_id', 'journey_id']\n)['decay_weight'].transform('sum')\n\ntouchpoints_with_conversion['time_decay_credit'] = (\n    touchpoints_with_conversion['decay_weight'] /\n    touchpoints_with_conversion['weight_sum'].replace(0, 1) *\n    touchpoints_with_conversion['conversion_value']\n)\n\n# 5. Position-based attribution (40% first, 40% last, 20% middle)\ntouchpoints_with_conversion['position'] = touchpoints_with_conversion.groupby(\n    ['customer_id', 'journey_id']\n).cumcount() + 1\n\ntouchpoints_with_conversion['total_positions'] = touchpoints_with_conversion.groupby(\n    ['customer_id', 'journey_id']\n)['position'].transform('max')\n\ntouchpoints_with_conversion['position_weight'] = np.where(\n    touchpoints_with_conversion['position'] == 1,\n    0.4,\n    np.where(\n        touchpoints_with_conversion['position'] == touchpoints_with_conversion['total_positions'],\n        0.4,\n        0.2 / (touchpoints_with_conversion['total_positions'] - 2).replace(0, 1)\n    )\n)\n\ntouchpoints_with_conversion['position_based_credit'] = (\n    touchpoints_with_conversion['position_weight'] *\n    touchpoints_with_conversion['conversion_value']\n)\n\n# Aggregate attribution by channel\nchannel_attribution = touchpoints_with_conversion.groupby('channel').agg({\n    'touchpoint_id': 'count',\n    'cost': 'sum',\n    'linear_credit': 'sum',\n    'time_decay_credit': 'sum',\n    'position_based_credit': 'sum'\n}).reset_index()\n\nchannel_attribution = pd.merge(channel_attribution, first_touch_attribution, on='channel', how='left')\nchannel_attribution = pd.merge(channel_attribution, last_touch_attribution, on='channel', how='left')\n\n# Calculate ROI by attribution model\nchannel_attribution['roi_linear'] = (\n    channel_attribution['linear_credit'] - channel_attribution['cost']\n) / channel_attribution['cost'].replace(0, 1)\n\nchannel_attribution['roi_time_decay'] = (\n    channel_attribution['time_decay_credit'] - channel_attribution['cost']\n) / channel_attribution['cost'].replace(0, 1)\n\nchannel_attribution['roi_position'] = (\n    channel_attribution['position_based_credit'] - channel_attribution['cost']\n) / channel_attribution['cost'].replace(0, 1)\n\n# Marketing mix modeling aggregations\ndaily_spend = ad_spend.groupby(['date', 'channel']).agg({\n    'spend': 'sum',\n    'impressions': 'sum',\n    'clicks': 'sum'\n}).reset_index()\n\ndaily_spend['cpm'] = daily_spend['spend'] / daily_spend['impressions'] * 1000\ndaily_spend['cpc'] = daily_spend['spend'] / daily_spend['clicks'].replace(0, 1)\ndaily_spend['ctr'] = daily_spend['clicks'] / daily_spend['impressions'].replace(0, 1)\n\n# Add adstock transformation (carryover effect)\ndaily_spend = daily_spend.sort_values(['channel', 'date'])\ndaily_spend['spend_adstock'] = daily_spend.groupby('channel')['spend'].transform(\n    lambda x: x.ewm(halflife=7).mean()\n)\n\n# Save outputs\njourney_touchpoints.to_csv('customer_journeys_analyzed.csv', index=False)\nchannel_attribution.to_csv('channel_attribution_models.csv', index=False)\ntouchpoints_with_conversion.to_csv('touchpoint_attribution_detail.csv', index=False)\ndaily_spend.to_csv('marketing_mix_features.csv', index=False)\n] PASSED [ 29%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineMetrics::test_pipeline_has_datasets[iot_predictive_maintenance-\nimport pandas as pd\nimport numpy as np\n\n# Load IoT sensor data\nsensor_readings = pd.read_csv('sensor_readings.csv')\nequipment_registry = pd.read_csv('equipment_registry.csv')\nmaintenance_history = pd.read_csv('maintenance_history.csv')\nfailure_logs = pd.read_csv('failure_logs.csv')\noperating_conditions = pd.read_csv('operating_conditions.csv')\nparts_inventory = pd.read_csv('parts_inventory.csv')\ntechnician_schedules = pd.read_csv('technician_schedules.csv')\n\n# Parse timestamps\nsensor_readings['reading_time'] = pd.to_datetime(sensor_readings['reading_time'])\nmaintenance_history['maintenance_date'] = pd.to_datetime(maintenance_history['maintenance_date'])\nfailure_logs['failure_time'] = pd.to_datetime(failure_logs['failure_time'])\n\n# Clean sensor data - remove outliers\nsensor_readings = sensor_readings.sort_values(['equipment_id', 'sensor_type', 'reading_time'])\n\n# Calculate rolling statistics per sensor\nsensor_readings['value_1h_mean'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].transform(lambda x: x.rolling('1H', on=sensor_readings['reading_time']).mean())\n\nsensor_readings['value_1h_std'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].transform(lambda x: x.rolling('1H', on=sensor_readings['reading_time']).std())\n\nsensor_readings['value_24h_mean'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].transform(lambda x: x.rolling('24H', on=sensor_readings['reading_time']).mean())\n\nsensor_readings['value_24h_max'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].transform(lambda x: x.rolling('24H', on=sensor_readings['reading_time']).max())\n\nsensor_readings['value_24h_min'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].transform(lambda x: x.rolling('24H', on=sensor_readings['reading_time']).min())\n\n# Calculate rate of change\nsensor_readings['value_diff'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].diff()\n\nsensor_readings['value_pct_change'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].pct_change()\n\n# Detect anomalies using z-score\nsensor_readings['z_score'] = (\n    (sensor_readings['sensor_value'] - sensor_readings['value_24h_mean']) /\n    sensor_readings['value_1h_std'].replace(0, 1)\n)\nsensor_readings['is_anomaly'] = (abs(sensor_readings['z_score']) > 3).astype(int)\n\n# Pivot sensor types to create feature vectors\nsensor_features = sensor_readings.pivot_table(\n    index=['equipment_id', 'reading_time'],\n    columns='sensor_type',\n    values=['sensor_value', 'value_1h_mean', 'value_24h_max', 'z_score', 'is_anomaly'],\n    aggfunc='first'\n).reset_index()\n\n# Flatten column names\nsensor_features.columns = ['_'.join(str(c) for c in col).strip('_')\n                           for col in sensor_features.columns]\n\n# Add equipment metadata\nsensor_features = pd.merge(\n    sensor_features,\n    equipment_registry[['equipment_id', 'equipment_type', 'manufacturer',\n                        'installation_date', 'rated_capacity', 'location']],\n    on='equipment_id',\n    how='left'\n)\n\n# Calculate equipment age\nsensor_features['installation_date'] = pd.to_datetime(sensor_features['installation_date'])\nsensor_features['equipment_age_days'] = (\n    sensor_features['reading_time'] - sensor_features['installation_date']\n).dt.days\n\n# Add operating conditions\noperating_conditions['condition_time'] = pd.to_datetime(operating_conditions['condition_time'])\nsensor_features = pd.merge_asof(\n    sensor_features.sort_values('reading_time'),\n    operating_conditions.sort_values('condition_time'),\n    left_on='reading_time',\n    right_on='condition_time',\n    by='equipment_id',\n    direction='backward'\n)\n\n# Calculate maintenance history features\nmaintenance_agg = maintenance_history.groupby('equipment_id').agg({\n    'maintenance_id': 'count',\n    'maintenance_date': 'max',\n    'maintenance_cost': 'sum',\n    'downtime_hours': 'sum'\n}).reset_index()\nmaintenance_agg.columns = ['equipment_id', 'total_maintenance_count',\n                           'last_maintenance_date', 'total_maintenance_cost',\n                           'total_downtime_hours']\n\nsensor_features = pd.merge(\n    sensor_features,\n    maintenance_agg,\n    on='equipment_id',\n    how='left'\n)\n\n# Calculate days since last maintenance\nsensor_features['last_maintenance_date'] = pd.to_datetime(sensor_features['last_maintenance_date'])\nsensor_features['days_since_maintenance'] = (\n    sensor_features['reading_time'] - sensor_features['last_maintenance_date']\n).dt.days\n\n# Calculate failure history features\nfailure_agg = failure_logs.groupby('equipment_id').agg({\n    'failure_id': 'count',\n    'failure_time': 'max',\n    'failure_severity': 'mean',\n    'repair_time_hours': 'sum'\n}).reset_index()\nfailure_agg.columns = ['equipment_id', 'total_failures', 'last_failure_date',\n                       'avg_failure_severity', 'total_repair_hours']\n\nsensor_features = pd.merge(\n    sensor_features,\n    failure_agg,\n    on='equipment_id',\n    how='left'\n)\n\n# Create target variable: failure within next N days\nfailure_logs_sorted = failure_logs.sort_values(['equipment_id', 'failure_time'])\nsensor_features = pd.merge_asof(\n    sensor_features.sort_values('reading_time'),\n    failure_logs_sorted[['equipment_id', 'failure_time']].rename(\n        columns={'failure_time': 'next_failure_time'}\n    ),\n    left_on='reading_time',\n    right_on='next_failure_time',\n    by='equipment_id',\n    direction='forward'\n)\n\nsensor_features['days_to_failure'] = (\n    sensor_features['next_failure_time'] - sensor_features['reading_time']\n).dt.total_seconds() / 86400\n\nsensor_features['failure_within_7d'] = (sensor_features['days_to_failure'] <= 7).astype(int)\nsensor_features['failure_within_30d'] = (sensor_features['days_to_failure'] <= 30).astype(int)\n\n# Calculate health score\nsensor_features['anomaly_count_24h'] = sensor_features.groupby('equipment_id')['is_anomaly_temperature'].transform(\n    lambda x: x.rolling('24H', on=sensor_features['reading_time']).sum()\n)\n\nsensor_features['health_score'] = 100 - (\n    sensor_features['anomaly_count_24h'] * 5 +\n    sensor_features['total_failures'].fillna(0) * 2 +\n    np.minimum(sensor_features['days_since_maintenance'].fillna(0) / 365, 1) * 20\n).clip(0, 100)\n\n# Prioritize maintenance\nsensor_features['maintenance_priority'] = np.where(\n    sensor_features['health_score'] < 50, 'Critical',\n    np.where(sensor_features['health_score'] < 70, 'High',\n    np.where(sensor_features['health_score'] < 85, 'Medium', 'Low'))\n)\n\n# Join parts availability\nparts_available = parts_inventory.groupby('equipment_type').agg({\n    'part_id': 'count',\n    'quantity_available': 'sum',\n    'lead_time_days': 'mean'\n}).reset_index()\nparts_available.columns = ['equipment_type', 'part_types_available',\n                           'total_parts_stock', 'avg_part_lead_time']\n\nsensor_features = pd.merge(\n    sensor_features,\n    parts_available,\n    on='equipment_type',\n    how='left'\n)\n\n# Save outputs\nsensor_features.to_csv('predictive_maintenance_features.csv', index=False)\n\n# Create maintenance schedule recommendations\nmaintenance_schedule = sensor_features[sensor_features['maintenance_priority'].isin(['Critical', 'High'])].groupby(\n    'equipment_id'\n).agg({\n    'health_score': 'min',\n    'maintenance_priority': 'first',\n    'days_since_maintenance': 'max',\n    'reading_time': 'max'\n}).reset_index()\n\nmaintenance_schedule.columns = ['equipment_id', 'current_health', 'priority',\n                                'days_since_last_maintenance', 'last_reading']\nmaintenance_schedule.to_csv('maintenance_schedule_recommendations.csv', index=False)\n] PASSED [ 29%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineMetrics::test_pipeline_has_datasets[genomic_analysis-\nimport pandas as pd\nimport numpy as np\n\n# Load genomic data\nvariants = pd.read_csv('variants.csv')\nsamples = pd.read_csv('samples.csv')\ngene_annotations = pd.read_csv('gene_annotations.csv')\nclinical_data = pd.read_csv('clinical_data.csv')\npathway_mappings = pd.read_csv('pathway_mappings.csv')\npopulation_frequencies = pd.read_csv('population_frequencies.csv')\nfunctional_predictions = pd.read_csv('functional_predictions.csv')\ndisease_associations = pd.read_csv('disease_associations.csv')\n\n# Standardize variant identifiers\nvariants['variant_id'] = (\n    variants['chromosome'].astype(str) + ':' +\n    variants['position'].astype(str) + ':' +\n    variants['reference'] + '>' + variants['alternate']\n)\n\n# Calculate variant quality metrics\nvariants['quality_pass'] = (\n    (variants['quality_score'] >= 30) &\n    (variants['read_depth'] >= 10) &\n    (variants['allele_frequency'] >= 0.2)\n).astype(int)\n\n# Filter high-quality variants\nhigh_quality_variants = variants[variants['quality_pass'] == 1].copy()\n\n# Annotate with gene information\nhigh_quality_variants = pd.merge(\n    high_quality_variants,\n    gene_annotations[['chromosome', 'start_position', 'end_position',\n                      'gene_symbol', 'gene_type', 'strand']],\n    on='chromosome',\n    how='left'\n)\n\n# Filter to variants within gene boundaries\nhigh_quality_variants = high_quality_variants[\n    (high_quality_variants['position'] >= high_quality_variants['start_position']) &\n    (high_quality_variants['position'] <= high_quality_variants['end_position'])\n]\n\n# Add functional impact predictions\nhigh_quality_variants = pd.merge(\n    high_quality_variants,\n    functional_predictions[['variant_id', 'sift_score', 'polyphen_score',\n                            'cadd_score', 'predicted_impact']],\n    on='variant_id',\n    how='left'\n)\n\n# Classify variant impact\nhigh_quality_variants['impact_category'] = np.where(\n    (high_quality_variants['sift_score'] < 0.05) | (high_quality_variants['polyphen_score'] > 0.85),\n    'Damaging',\n    np.where(\n        (high_quality_variants['sift_score'] < 0.1) | (high_quality_variants['polyphen_score'] > 0.5),\n        'Possibly_Damaging',\n        'Benign'\n    )\n)\n\n# Add population allele frequencies\nhigh_quality_variants = pd.merge(\n    high_quality_variants,\n    population_frequencies[['variant_id', 'gnomad_af', 'gnomad_af_eas',\n                            'gnomad_af_eur', 'gnomad_af_afr']],\n    on='variant_id',\n    how='left'\n)\n\n# Flag rare variants (MAF < 1%)\nhigh_quality_variants['is_rare'] = (high_quality_variants['gnomad_af'] < 0.01).astype(int)\nhigh_quality_variants['is_ultra_rare'] = (high_quality_variants['gnomad_af'] < 0.001).astype(int)\n\n# Add disease associations\nhigh_quality_variants = pd.merge(\n    high_quality_variants,\n    disease_associations[['gene_symbol', 'disease_name', 'inheritance_pattern',\n                          'clinical_significance']],\n    on='gene_symbol',\n    how='left'\n)\n\n# Calculate per-sample variant burden\nsample_burden = high_quality_variants.groupby('sample_id').agg({\n    'variant_id': 'count',\n    'is_rare': 'sum',\n    'impact_category': lambda x: (x == 'Damaging').sum()\n}).reset_index()\nsample_burden.columns = ['sample_id', 'total_variants', 'rare_variants', 'damaging_variants']\n\n# Add clinical phenotype data\nsample_burden = pd.merge(\n    sample_burden,\n    samples[['sample_id', 'patient_id', 'tissue_type', 'collection_date']],\n    on='sample_id',\n    how='left'\n)\n\nsample_burden = pd.merge(\n    sample_burden,\n    clinical_data[['patient_id', 'diagnosis', 'age_at_diagnosis',\n                   'sex', 'ethnicity', 'family_history']],\n    on='patient_id',\n    how='left'\n)\n\n# Calculate gene-level burden\ngene_burden = high_quality_variants.groupby(['sample_id', 'gene_symbol']).agg({\n    'variant_id': 'count',\n    'is_rare': 'sum',\n    'impact_category': lambda x: (x == 'Damaging').sum(),\n    'cadd_score': 'max'\n}).reset_index()\ngene_burden.columns = ['sample_id', 'gene_symbol', 'variants_in_gene',\n                       'rare_in_gene', 'damaging_in_gene', 'max_cadd']\n\n# Add pathway information\ngene_burden = pd.merge(\n    gene_burden,\n    pathway_mappings[['gene_symbol', 'pathway_id', 'pathway_name']],\n    on='gene_symbol',\n    how='left'\n)\n\n# Calculate pathway-level burden\npathway_burden = gene_burden.groupby(['sample_id', 'pathway_id', 'pathway_name']).agg({\n    'gene_symbol': 'nunique',\n    'variants_in_gene': 'sum',\n    'damaging_in_gene': 'sum',\n    'max_cadd': 'max'\n}).reset_index()\npathway_burden.columns = ['sample_id', 'pathway_id', 'pathway_name',\n                          'genes_affected', 'pathway_variants',\n                          'pathway_damaging', 'pathway_max_cadd']\n\n# Identify potentially pathogenic variants\npathogenic_candidates = high_quality_variants[\n    (high_quality_variants['impact_category'] == 'Damaging') &\n    (high_quality_variants['is_rare'] == 1) &\n    (high_quality_variants['clinical_significance'].isin(['Pathogenic', 'Likely_Pathogenic']).fillna(False))\n].copy()\n\n# Create variant report\nvariant_report = pathogenic_candidates.groupby(['sample_id', 'gene_symbol']).agg({\n    'variant_id': lambda x: '; '.join(x),\n    'disease_name': 'first',\n    'inheritance_pattern': 'first',\n    'cadd_score': 'max'\n}).reset_index()\n\n# Save outputs\nhigh_quality_variants.to_csv('annotated_variants.csv', index=False)\nsample_burden.to_csv('sample_variant_burden.csv', index=False)\ngene_burden.to_csv('gene_level_burden.csv', index=False)\npathway_burden.to_csv('pathway_analysis.csv', index=False)\nvariant_report.to_csv('pathogenic_variant_report.csv', index=False)\n] PASSED [ 29%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineMetrics::test_pipeline_has_datasets[clickstream_analysis-\nimport pandas as pd\nimport numpy as np\n\n# Load clickstream data\npage_views = pd.read_csv('page_views.csv')\nclick_events = pd.read_csv('click_events.csv')\nform_submissions = pd.read_csv('form_submissions.csv')\nsearch_queries = pd.read_csv('search_queries.csv')\nproduct_impressions = pd.read_csv('product_impressions.csv')\ncart_events = pd.read_csv('cart_events.csv')\nuser_agents = pd.read_csv('user_agents.csv')\ngeo_locations = pd.read_csv('geo_locations.csv')\n\n# Parse timestamps\npage_views['timestamp'] = pd.to_datetime(page_views['timestamp'])\nclick_events['timestamp'] = pd.to_datetime(click_events['timestamp'])\n\n# Combine all events\npage_views['event_type'] = 'page_view'\nclick_events['event_type'] = 'click'\nform_submissions['event_type'] = 'form_submit'\nsearch_queries['event_type'] = 'search'\ncart_events['event_type'] = cart_events['cart_action']\n\nall_events = pd.concat([\n    page_views[['session_id', 'user_id', 'timestamp', 'event_type', 'page_url', 'referrer']],\n    click_events[['session_id', 'user_id', 'timestamp', 'event_type', 'element_id', 'page_url']],\n    form_submissions[['session_id', 'user_id', 'timestamp', 'event_type', 'form_id', 'page_url']],\n    search_queries[['session_id', 'user_id', 'timestamp', 'event_type', 'query', 'results_count']],\n    cart_events[['session_id', 'user_id', 'timestamp', 'event_type', 'product_id', 'quantity']]\n], ignore_index=True, sort=False)\n\nall_events = all_events.sort_values(['session_id', 'timestamp'])\n\n# Session-level aggregations\nsession_metrics = all_events.groupby('session_id').agg({\n    'user_id': 'first',\n    'timestamp': ['min', 'max', 'count'],\n    'page_url': 'nunique',\n    'event_type': lambda x: x.value_counts().to_dict()\n}).reset_index()\n\nsession_metrics.columns = ['session_id', 'user_id', 'session_start',\n                            'session_end', 'total_events', 'unique_pages', 'event_breakdown']\n\n# Calculate session duration\nsession_metrics['session_duration_seconds'] = (\n    session_metrics['session_end'] - session_metrics['session_start']\n).dt.total_seconds()\n\n# Extract event counts\nsession_metrics['page_views'] = session_metrics['event_breakdown'].apply(\n    lambda x: x.get('page_view', 0) if isinstance(x, dict) else 0\n)\nsession_metrics['clicks'] = session_metrics['event_breakdown'].apply(\n    lambda x: x.get('click', 0) if isinstance(x, dict) else 0\n)\nsession_metrics['searches'] = session_metrics['event_breakdown'].apply(\n    lambda x: x.get('search', 0) if isinstance(x, dict) else 0\n)\nsession_metrics['cart_adds'] = session_metrics['event_breakdown'].apply(\n    lambda x: x.get('add_to_cart', 0) if isinstance(x, dict) else 0\n)\n\n# Calculate engagement metrics\nsession_metrics['pages_per_minute'] = (\n    session_metrics['unique_pages'] /\n    (session_metrics['session_duration_seconds'] / 60).replace(0, 1)\n)\nsession_metrics['events_per_page'] = (\n    session_metrics['total_events'] / session_metrics['unique_pages'].replace(0, 1)\n)\n\n# Identify bounce sessions\nsession_metrics['is_bounce'] = (\n    (session_metrics['unique_pages'] == 1) &\n    (session_metrics['session_duration_seconds'] < 10)\n).astype(int)\n\n# Create page flow sequences\npage_sequences = all_events[all_events['event_type'] == 'page_view'].copy()\npage_sequences['page_order'] = page_sequences.groupby('session_id').cumcount() + 1\npage_sequences['next_page'] = page_sequences.groupby('session_id')['page_url'].shift(-1)\n\n# Calculate page transitions\npage_transitions = page_sequences.groupby(['page_url', 'next_page']).size().reset_index(name='transition_count')\npage_transitions = page_transitions[page_transitions['next_page'].notna()]\n\n# Calculate exit rates per page\npage_stats = page_sequences.groupby('page_url').agg({\n    'session_id': 'count',\n    'next_page': lambda x: x.isna().sum()\n}).reset_index()\npage_stats.columns = ['page_url', 'total_views', 'exits']\npage_stats['exit_rate'] = page_stats['exits'] / page_stats['total_views']\n\n# Analyze search behavior\nsearch_analysis = search_queries.groupby('session_id').agg({\n    'query': ['count', lambda x: ' | '.join(x)],\n    'results_count': ['mean', 'min'],\n    'clicked_result': 'sum'\n}).reset_index()\nsearch_analysis.columns = ['session_id', 'search_count', 'search_queries',\n                            'avg_results', 'min_results', 'result_clicks']\nsearch_analysis['search_success_rate'] = (\n    search_analysis['result_clicks'] / search_analysis['search_count'].replace(0, 1)\n)\n\n# Analyze cart behavior\ncart_analysis = cart_events.groupby('session_id').agg({\n    'product_id': 'nunique',\n    'quantity': 'sum',\n    'cart_action': lambda x: (x == 'add_to_cart').sum()\n}).reset_index()\ncart_analysis.columns = ['session_id', 'unique_cart_products',\n                          'total_cart_quantity', 'add_to_cart_events']\n\n# Add purchase flag\npurchases = cart_events[cart_events['cart_action'] == 'purchase'].groupby('session_id').agg({\n    'order_value': 'sum'\n}).reset_index()\n\ncart_analysis = pd.merge(cart_analysis, purchases, on='session_id', how='left')\ncart_analysis['converted'] = cart_analysis['order_value'].notna().astype(int)\ncart_analysis['order_value'] = cart_analysis['order_value'].fillna(0)\n\n# Merge all session data\nsession_complete = session_metrics.copy()\nsession_complete = pd.merge(session_complete, search_analysis, on='session_id', how='left')\nsession_complete = pd.merge(session_complete, cart_analysis, on='session_id', how='left')\n\n# Add user agent info\nsession_complete = pd.merge(\n    session_complete,\n    user_agents[['session_id', 'device_type', 'browser', 'os', 'is_mobile']],\n    on='session_id',\n    how='left'\n)\n\n# Add geo location\nsession_complete = pd.merge(\n    session_complete,\n    geo_locations[['session_id', 'country', 'region', 'city', 'timezone']],\n    on='session_id',\n    how='left'\n)\n\n# Calculate user-level aggregations\nuser_metrics = session_complete.groupby('user_id').agg({\n    'session_id': 'count',\n    'session_duration_seconds': ['mean', 'sum'],\n    'page_views': ['mean', 'sum'],\n    'converted': 'sum',\n    'order_value': 'sum',\n    'session_start': ['min', 'max']\n}).reset_index()\n\nuser_metrics.columns = ['user_id', 'total_sessions', 'avg_session_duration',\n                        'total_time_spent', 'avg_pages_per_session', 'total_pages_viewed',\n                        'total_conversions', 'total_revenue', 'first_visit', 'last_visit']\n\nuser_metrics['conversion_rate'] = (\n    user_metrics['total_conversions'] / user_metrics['total_sessions']\n)\nuser_metrics['avg_order_value'] = (\n    user_metrics['total_revenue'] / user_metrics['total_conversions'].replace(0, 1)\n)\n\n# Segment users by engagement\nuser_metrics['engagement_segment'] = pd.cut(\n    user_metrics['total_time_spent'] / 3600,\n    bins=[-np.inf, 0.1, 1, 5, 20, np.inf],\n    labels=['Minimal', 'Light', 'Medium', 'Heavy', 'Power']\n)\n\n# Save outputs\nsession_complete.to_csv('session_analytics.csv', index=False)\nuser_metrics.to_csv('user_engagement_metrics.csv', index=False)\npage_stats.to_csv('page_performance.csv', index=False)\npage_transitions.to_csv('page_flow_analysis.csv', index=False)\n] PASSED [ 30%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineMetrics::test_pipeline_has_datasets[portfolio_risk-\nimport pandas as pd\nimport numpy as np\n\n# Load financial data\npositions = pd.read_csv('positions.csv')\nprice_history = pd.read_csv('price_history.csv')\ninstruments = pd.read_csv('instruments.csv')\nmarket_data = pd.read_csv('market_data.csv')\nfx_rates = pd.read_csv('fx_rates.csv')\nbenchmark_returns = pd.read_csv('benchmark_returns.csv')\nfactor_exposures = pd.read_csv('factor_exposures.csv')\ncredit_ratings = pd.read_csv('credit_ratings.csv')\n\n# Parse dates\nprice_history['date'] = pd.to_datetime(price_history['date'])\nmarket_data['date'] = pd.to_datetime(market_data['date'])\nfx_rates['date'] = pd.to_datetime(fx_rates['date'])\n\n# Calculate daily returns\nprice_history = price_history.sort_values(['instrument_id', 'date'])\nprice_history['daily_return'] = price_history.groupby('instrument_id')['close_price'].pct_change()\nprice_history['log_return'] = np.log(\n    price_history['close_price'] / price_history.groupby('instrument_id')['close_price'].shift(1)\n)\n\n# Calculate rolling volatility\nprice_history['volatility_20d'] = price_history.groupby('instrument_id')['daily_return'].transform(\n    lambda x: x.rolling(20).std() * np.sqrt(252)\n)\nprice_history['volatility_60d'] = price_history.groupby('instrument_id')['daily_return'].transform(\n    lambda x: x.rolling(60).std() * np.sqrt(252)\n)\n\n# Calculate rolling beta to market\nmarket_returns = market_data[market_data['index_id'] == 'SPX'][['date', 'daily_return']].copy()\nmarket_returns.columns = ['date', 'market_return']\n\nprice_with_market = pd.merge(price_history, market_returns, on='date', how='left')\n\ndef calculate_beta(group):\n    if len(group) < 60:\n        return np.nan\n    cov = group['daily_return'].cov(group['market_return'])\n    var = group['market_return'].var()\n    return cov / var if var != 0 else np.nan\n\nprice_with_market['beta'] = price_with_market.groupby('instrument_id').apply(\n    lambda x: x['daily_return'].rolling(60).cov(x['market_return']) /\n              x['market_return'].rolling(60).var()\n).reset_index(level=0, drop=True)\n\n# Enrich positions with instrument details\npositions_enriched = pd.merge(\n    positions,\n    instruments[['instrument_id', 'instrument_type', 'currency', 'sector',\n                 'country', 'maturity_date', 'coupon_rate', 'issuer']],\n    on='instrument_id',\n    how='left'\n)\n\n# Get latest prices and metrics\nlatest_prices = price_with_market.groupby('instrument_id').last().reset_index()\npositions_enriched = pd.merge(\n    positions_enriched,\n    latest_prices[['instrument_id', 'close_price', 'daily_return',\n                   'volatility_20d', 'volatility_60d', 'beta']],\n    on='instrument_id',\n    how='left'\n)\n\n# Convert to base currency (USD)\nfx_latest = fx_rates.groupby('currency_pair').last().reset_index()\nfx_latest['to_currency'] = fx_latest['currency_pair'].str[-3:]\nfx_latest['from_currency'] = fx_latest['currency_pair'].str[:3]\n\npositions_enriched = pd.merge(\n    positions_enriched,\n    fx_latest[['from_currency', 'rate']].rename(columns={'from_currency': 'currency', 'rate': 'fx_rate'}),\n    on='currency',\n    how='left'\n)\npositions_enriched['fx_rate'] = positions_enriched['fx_rate'].fillna(1.0)\n\n# Calculate market values\npositions_enriched['market_value_local'] = positions_enriched['quantity'] * positions_enriched['close_price']\npositions_enriched['market_value_usd'] = positions_enriched['market_value_local'] / positions_enriched['fx_rate']\n\n# Calculate position-level risk metrics\npositions_enriched['position_volatility'] = (\n    positions_enriched['market_value_usd'] * positions_enriched['volatility_20d']\n)\n\n# Portfolio-level calculations\nportfolio_value = positions_enriched.groupby('portfolio_id')['market_value_usd'].sum().reset_index()\nportfolio_value.columns = ['portfolio_id', 'total_nav']\n\npositions_enriched = pd.merge(positions_enriched, portfolio_value, on='portfolio_id', how='left')\npositions_enriched['weight'] = positions_enriched['market_value_usd'] / positions_enriched['total_nav']\n\n# Calculate weighted portfolio beta\nportfolio_beta = positions_enriched.groupby('portfolio_id').apply(\n    lambda x: (x['weight'] * x['beta']).sum()\n).reset_index(name='portfolio_beta')\n\n# Calculate portfolio volatility (simplified - assuming no correlation)\nportfolio_vol_contrib = positions_enriched.groupby('portfolio_id').apply(\n    lambda x: np.sqrt((x['weight']**2 * x['volatility_20d']**2).sum())\n).reset_index(name='portfolio_volatility')\n\n# Sector concentration\nsector_concentration = positions_enriched.groupby(['portfolio_id', 'sector']).agg({\n    'market_value_usd': 'sum',\n    'weight': 'sum'\n}).reset_index()\nsector_concentration.columns = ['portfolio_id', 'sector', 'sector_value', 'sector_weight']\n\n# Country concentration\ncountry_concentration = positions_enriched.groupby(['portfolio_id', 'country']).agg({\n    'market_value_usd': 'sum',\n    'weight': 'sum'\n}).reset_index()\n\n# Add credit risk for fixed income\npositions_enriched = pd.merge(\n    positions_enriched,\n    credit_ratings[['issuer', 'rating', 'rating_numeric', 'default_probability']],\n    on='issuer',\n    how='left'\n)\n\n# Calculate credit VaR contribution\npositions_enriched['credit_var_contrib'] = (\n    positions_enriched['market_value_usd'] *\n    positions_enriched['default_probability'].fillna(0) *\n    (1 - 0.4)  # Assuming 40% recovery rate\n)\n\n# Calculate VaR (parametric approach)\nconfidence_level = 0.99\nz_score = 2.326  # 99% confidence\n\npositions_enriched['var_1d'] = (\n    positions_enriched['market_value_usd'] *\n    positions_enriched['volatility_20d'] / np.sqrt(252) *\n    z_score\n)\n\nportfolio_var = positions_enriched.groupby('portfolio_id')['var_1d'].sum().reset_index()\nportfolio_var.columns = ['portfolio_id', 'total_var_1d']\n\n# Historical VaR calculation\nreturns_pivot = price_history.pivot_table(\n    index='date',\n    columns='instrument_id',\n    values='daily_return'\n).fillna(0)\n\n# Calculate portfolio returns for each portfolio\nportfolio_returns = []\nfor portfolio_id in positions_enriched['portfolio_id'].unique():\n    port_positions = positions_enriched[positions_enriched['portfolio_id'] == portfolio_id]\n    weights = port_positions.set_index('instrument_id')['weight'].to_dict()\n\n    port_return = returns_pivot[list(weights.keys())].mul(\n        pd.Series(weights)\n    ).sum(axis=1)\n\n    portfolio_returns.append(pd.DataFrame({\n        'portfolio_id': portfolio_id,\n        'date': returns_pivot.index,\n        'portfolio_return': port_return\n    }))\n\nportfolio_returns_df = pd.concat(portfolio_returns, ignore_index=True)\n\n# Calculate historical VaR\nhistorical_var = portfolio_returns_df.groupby('portfolio_id')['portfolio_return'].apply(\n    lambda x: x.quantile(1 - confidence_level)\n).reset_index(name='historical_var_1d')\n\n# Calculate Sharpe ratio\nrisk_free_rate = 0.05 / 252  # Daily risk-free rate\n\nportfolio_sharpe = portfolio_returns_df.groupby('portfolio_id').agg({\n    'portfolio_return': ['mean', 'std']\n}).reset_index()\nportfolio_sharpe.columns = ['portfolio_id', 'avg_return', 'return_std']\nportfolio_sharpe['sharpe_ratio'] = (\n    (portfolio_sharpe['avg_return'] - risk_free_rate) / portfolio_sharpe['return_std']\n) * np.sqrt(252)\n\n# Compile portfolio risk summary\nportfolio_risk = portfolio_value.copy()\nportfolio_risk = pd.merge(portfolio_risk, portfolio_beta, on='portfolio_id', how='left')\nportfolio_risk = pd.merge(portfolio_risk, portfolio_vol_contrib, on='portfolio_id', how='left')\nportfolio_risk = pd.merge(portfolio_risk, portfolio_var, on='portfolio_id', how='left')\nportfolio_risk = pd.merge(portfolio_risk, historical_var, on='portfolio_id', how='left')\nportfolio_risk = pd.merge(portfolio_risk, portfolio_sharpe, on='portfolio_id', how='left')\n\n# Save outputs\npositions_enriched.to_csv('positions_with_risk.csv', index=False)\nportfolio_risk.to_csv('portfolio_risk_summary.csv', index=False)\nsector_concentration.to_csv('sector_concentration.csv', index=False)\ncountry_concentration.to_csv('country_concentration.csv', index=False)\nportfolio_returns_df.to_csv('portfolio_returns_history.csv', index=False)\n] PASSED [ 30%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineMetrics::test_pipeline_conversion_no_errors[fraud_detection-\nimport pandas as pd\nimport numpy as np\n\n# Load transaction streams\ntransactions = pd.read_csv('transactions.csv')\nuser_profiles = pd.read_csv('user_profiles.csv')\ndevice_fingerprints = pd.read_csv('device_fingerprints.csv')\nmerchant_risk_scores = pd.read_csv('merchant_risk_scores.csv')\nhistorical_fraud = pd.read_csv('historical_fraud.csv')\nip_geolocation = pd.read_csv('ip_geolocation.csv')\n\n# Parse timestamps\ntransactions['timestamp'] = pd.to_datetime(transactions['timestamp'])\ntransactions['date'] = transactions['timestamp'].dt.date\ntransactions['hour'] = transactions['timestamp'].dt.hour\ntransactions['minute'] = transactions['timestamp'].dt.minute\ntransactions['day_of_week'] = transactions['timestamp'].dt.dayofweek\ntransactions['is_weekend'] = transactions['day_of_week'].isin([5, 6]).astype(int)\ntransactions['is_night'] = transactions['hour'].between(0, 6).astype(int)\n\n# Enrich with user profile data\ntransactions_enriched = pd.merge(\n    transactions,\n    user_profiles[['user_id', 'account_age_days', 'verification_level',\n                   'avg_transaction_amount', 'typical_login_hour',\n                   'home_country', 'risk_tier']],\n    on='user_id',\n    how='left'\n)\n\n# Add device fingerprint risk\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    device_fingerprints[['device_id', 'device_trust_score', 'is_known_device',\n                         'browser_anomaly_score', 'last_seen_country']],\n    on='device_id',\n    how='left'\n)\n\n# Add merchant risk scores\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    merchant_risk_scores[['merchant_id', 'merchant_risk_score',\n                          'chargeback_rate', 'merchant_category']],\n    on='merchant_id',\n    how='left'\n)\n\n# Add IP geolocation\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    ip_geolocation[['ip_address', 'ip_country', 'ip_city',\n                    'is_vpn', 'is_tor', 'ip_risk_score']],\n    on='ip_address',\n    how='left'\n)\n\n# Calculate velocity features - transactions per user in last N minutes\ntransactions_enriched = transactions_enriched.sort_values(['user_id', 'timestamp'])\n\n# Rolling window aggregations per user\nuser_velocity = transactions_enriched.groupby('user_id').agg({\n    'transaction_id': 'count',\n    'amount': ['sum', 'mean', 'std', 'max'],\n    'merchant_id': 'nunique',\n    'ip_address': 'nunique',\n    'device_id': 'nunique'\n}).reset_index()\nuser_velocity.columns = ['user_id', 'tx_count_session', 'total_amount_session',\n                         'avg_amount_session', 'std_amount_session', 'max_amount_session',\n                         'unique_merchants', 'unique_ips', 'unique_devices']\n\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    user_velocity,\n    on='user_id',\n    how='left'\n)\n\n# Calculate deviation from user's typical behavior\ntransactions_enriched['amount_deviation'] = (\n    (transactions_enriched['amount'] - transactions_enriched['avg_transaction_amount']) /\n    transactions_enriched['avg_transaction_amount'].replace(0, 1)\n)\n\ntransactions_enriched['hour_deviation'] = abs(\n    transactions_enriched['hour'] - transactions_enriched['typical_login_hour']\n)\n\n# Geographic anomaly detection\ntransactions_enriched['country_mismatch'] = (\n    transactions_enriched['ip_country'] != transactions_enriched['home_country']\n).astype(int)\n\ntransactions_enriched['device_country_mismatch'] = (\n    transactions_enriched['ip_country'] != transactions_enriched['last_seen_country']\n).astype(int)\n\n# Composite risk score calculation\ntransactions_enriched['composite_risk_score'] = (\n    transactions_enriched['ip_risk_score'] * 0.2 +\n    transactions_enriched['merchant_risk_score'] * 0.15 +\n    (1 - transactions_enriched['device_trust_score']) * 0.2 +\n    transactions_enriched['amount_deviation'].clip(0, 5) * 0.15 +\n    transactions_enriched['country_mismatch'] * 0.15 +\n    transactions_enriched['is_vpn'] * 0.1 +\n    transactions_enriched['is_tor'] * 0.05\n)\n\n# Flag high-risk transactions\ntransactions_enriched['is_high_risk'] = (\n    transactions_enriched['composite_risk_score'] > 0.7\n).astype(int)\n\n# Join historical fraud patterns\nfraud_patterns = historical_fraud.groupby('user_id').agg({\n    'fraud_flag': 'sum',\n    'transaction_id': 'count'\n}).reset_index()\nfraud_patterns.columns = ['user_id', 'historical_fraud_count', 'historical_tx_count']\nfraud_patterns['historical_fraud_rate'] = (\n    fraud_patterns['historical_fraud_count'] / fraud_patterns['historical_tx_count']\n)\n\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    fraud_patterns[['user_id', 'historical_fraud_rate']],\n    on='user_id',\n    how='left'\n)\ntransactions_enriched['historical_fraud_rate'] = transactions_enriched['historical_fraud_rate'].fillna(0)\n\n# Final feature selection for ML model\nml_features = transactions_enriched[[\n    'transaction_id', 'user_id', 'amount', 'amount_deviation',\n    'hour_deviation', 'is_weekend', 'is_night',\n    'device_trust_score', 'is_known_device', 'browser_anomaly_score',\n    'merchant_risk_score', 'chargeback_rate',\n    'ip_risk_score', 'is_vpn', 'is_tor',\n    'country_mismatch', 'device_country_mismatch',\n    'tx_count_session', 'unique_merchants', 'unique_ips', 'unique_devices',\n    'composite_risk_score', 'historical_fraud_rate', 'is_high_risk'\n]].copy()\n\nml_features.to_csv('fraud_detection_features.csv', index=False)\ntransactions_enriched.to_csv('transactions_enriched_full.csv', index=False)\n] PASSED [ 30%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineMetrics::test_pipeline_conversion_no_errors[customer_360-\nimport pandas as pd\nimport numpy as np\n\n# Load data from multiple source systems\ncrm_customers = pd.read_csv('crm_customers.csv')\necommerce_users = pd.read_csv('ecommerce_users.csv')\nmobile_app_users = pd.read_csv('mobile_app_users.csv')\ncall_center_interactions = pd.read_csv('call_center_interactions.csv')\nemail_campaigns = pd.read_csv('email_campaigns.csv')\nweb_analytics = pd.read_csv('web_analytics.csv')\nsocial_media = pd.read_csv('social_media.csv')\nloyalty_program = pd.read_csv('loyalty_program.csv')\nsupport_tickets = pd.read_csv('support_tickets.csv')\nnps_surveys = pd.read_csv('nps_surveys.csv')\n\n# Standardize customer identifiers across systems\ncrm_customers['source'] = 'crm'\ncrm_customers['master_id'] = crm_customers['crm_id']\n\necommerce_users['source'] = 'ecommerce'\necommerce_users = pd.merge(\n    ecommerce_users,\n    crm_customers[['email', 'crm_id']],\n    on='email',\n    how='left'\n)\necommerce_users['master_id'] = ecommerce_users['crm_id'].fillna(\n    'ECO_' + ecommerce_users['ecommerce_user_id'].astype(str)\n)\n\nmobile_app_users['source'] = 'mobile'\nmobile_app_users = pd.merge(\n    mobile_app_users,\n    crm_customers[['phone', 'crm_id']],\n    on='phone',\n    how='left'\n)\nmobile_app_users['master_id'] = mobile_app_users['crm_id'].fillna(\n    'MOB_' + mobile_app_users['app_user_id'].astype(str)\n)\n\n# Create unified customer profile\ncustomer_base = crm_customers[['master_id', 'email', 'phone', 'first_name',\n                               'last_name', 'date_of_birth', 'gender',\n                               'address', 'city', 'state', 'country',\n                               'customer_since']].copy()\n\n# Aggregate e-commerce behavior\necommerce_metrics = ecommerce_users.groupby('master_id').agg({\n    'order_id': 'count',\n    'order_total': ['sum', 'mean', 'max'],\n    'items_purchased': 'sum',\n    'last_order_date': 'max',\n    'cart_abandonment_count': 'sum'\n}).reset_index()\necommerce_metrics.columns = ['master_id', 'ecom_order_count', 'ecom_total_spend',\n                              'ecom_avg_order', 'ecom_max_order', 'ecom_items_total',\n                              'ecom_last_order', 'ecom_cart_abandonments']\n\n# Aggregate mobile app engagement\nmobile_metrics = mobile_app_users.groupby('master_id').agg({\n    'session_count': 'sum',\n    'total_time_minutes': 'sum',\n    'push_notifications_clicked': 'sum',\n    'app_crashes': 'sum',\n    'features_used': lambda x: len(set(','.join(x.dropna()).split(','))),\n    'last_active_date': 'max'\n}).reset_index()\nmobile_metrics.columns = ['master_id', 'app_sessions', 'app_time_minutes',\n                          'push_clicks', 'app_crashes', 'features_used_count',\n                          'app_last_active']\n\n# Aggregate call center interactions\ncall_metrics = call_center_interactions.groupby('master_id').agg({\n    'call_id': 'count',\n    'call_duration_seconds': ['sum', 'mean'],\n    'issue_resolved': 'mean',\n    'escalated': 'sum',\n    'sentiment_score': 'mean'\n}).reset_index()\ncall_metrics.columns = ['master_id', 'call_count', 'total_call_duration',\n                        'avg_call_duration', 'resolution_rate',\n                        'escalation_count', 'call_sentiment']\n\n# Aggregate email engagement\nemail_metrics = email_campaigns.groupby('master_id').agg({\n    'email_sent': 'sum',\n    'email_opened': 'sum',\n    'email_clicked': 'sum',\n    'unsubscribed': 'max'\n}).reset_index()\nemail_metrics['email_open_rate'] = email_metrics['email_opened'] / email_metrics['email_sent']\nemail_metrics['email_click_rate'] = email_metrics['email_clicked'] / email_metrics['email_opened'].replace(0, 1)\n\n# Aggregate web analytics\nweb_metrics = web_analytics.groupby('master_id').agg({\n    'page_views': 'sum',\n    'unique_sessions': 'sum',\n    'bounce_rate': 'mean',\n    'avg_session_duration': 'mean',\n    'conversion_events': 'sum'\n}).reset_index()\n\n# Aggregate loyalty program data\nloyalty_metrics = loyalty_program.groupby('master_id').agg({\n    'points_earned': 'sum',\n    'points_redeemed': 'sum',\n    'tier_level': 'max',\n    'referrals_made': 'sum'\n}).reset_index()\nloyalty_metrics['points_balance'] = loyalty_metrics['points_earned'] - loyalty_metrics['points_redeemed']\n\n# Aggregate support tickets\nsupport_metrics = support_tickets.groupby('master_id').agg({\n    'ticket_id': 'count',\n    'resolution_time_hours': 'mean',\n    'satisfaction_rating': 'mean',\n    'ticket_reopened': 'sum'\n}).reset_index()\nsupport_metrics.columns = ['master_id', 'ticket_count', 'avg_resolution_time',\n                           'support_satisfaction', 'tickets_reopened']\n\n# Aggregate NPS surveys\nnps_metrics = nps_surveys.groupby('master_id').agg({\n    'nps_score': 'mean',\n    'survey_id': 'count',\n    'would_recommend': 'mean'\n}).reset_index()\nnps_metrics.columns = ['master_id', 'avg_nps_score', 'surveys_completed', 'recommend_rate']\n\n# Merge all metrics into unified customer 360 view\ncustomer_360 = customer_base.copy()\n\nfor metrics_df in [ecommerce_metrics, mobile_metrics, call_metrics,\n                   email_metrics, web_metrics, loyalty_metrics,\n                   support_metrics, nps_metrics]:\n    customer_360 = pd.merge(customer_360, metrics_df, on='master_id', how='left')\n\n# Fill missing values appropriately\nnumeric_cols = customer_360.select_dtypes(include=[np.number]).columns\ncustomer_360[numeric_cols] = customer_360[numeric_cols].fillna(0)\n\n# Calculate derived metrics\ncustomer_360['total_interactions'] = (\n    customer_360['ecom_order_count'] +\n    customer_360['app_sessions'] +\n    customer_360['call_count'] +\n    customer_360['ticket_count']\n)\n\ncustomer_360['digital_engagement_score'] = (\n    customer_360['app_sessions'] * 0.3 +\n    customer_360['email_open_rate'] * 20 +\n    customer_360['page_views'] * 0.1 +\n    customer_360['push_clicks'] * 0.5\n).clip(0, 100)\n\ncustomer_360['customer_health_score'] = (\n    customer_360['avg_nps_score'] / 10 * 25 +\n    customer_360['resolution_rate'] * 25 +\n    customer_360['support_satisfaction'] / 5 * 25 +\n    (1 - customer_360['escalation_count'].clip(0, 5) / 5) * 25\n)\n\n# Customer segmentation\ncustomer_360['value_segment'] = pd.cut(\n    customer_360['ecom_total_spend'],\n    bins=[-np.inf, 100, 500, 2000, 10000, np.inf],\n    labels=['Dormant', 'Bronze', 'Silver', 'Gold', 'Platinum']\n)\n\ncustomer_360['engagement_segment'] = pd.cut(\n    customer_360['digital_engagement_score'],\n    bins=[-np.inf, 20, 40, 60, 80, np.inf],\n    labels=['Inactive', 'Low', 'Medium', 'High', 'Power User']\n)\n\n# Calculate churn risk\ncustomer_360['days_since_activity'] = (\n    pd.to_datetime('today') - pd.to_datetime(customer_360['ecom_last_order'])\n).dt.days.fillna(999)\n\ncustomer_360['churn_risk_score'] = (\n    customer_360['days_since_activity'] / 365 * 30 +\n    (1 - customer_360['email_open_rate'].fillna(0)) * 20 +\n    customer_360['tickets_reopened'] * 10 +\n    (10 - customer_360['avg_nps_score'].fillna(5)) * 4\n).clip(0, 100)\n\n# Save outputs\ncustomer_360.to_csv('customer_360_unified.csv', index=False)\n] PASSED [ 30%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineMetrics::test_pipeline_conversion_no_errors[supply_chain-\nimport pandas as pd\nimport numpy as np\n\n# Load supply chain data\nsales_history = pd.read_csv('sales_history.csv')\ninventory_levels = pd.read_csv('inventory_levels.csv')\nsupplier_data = pd.read_csv('supplier_data.csv')\nwarehouse_locations = pd.read_csv('warehouse_locations.csv')\nshipping_costs = pd.read_csv('shipping_costs.csv')\nproduct_catalog = pd.read_csv('product_catalog.csv')\npromotions_calendar = pd.read_csv('promotions_calendar.csv')\nweather_data = pd.read_csv('weather_data.csv')\neconomic_indicators = pd.read_csv('economic_indicators.csv')\n\n# Parse dates\nsales_history['sale_date'] = pd.to_datetime(sales_history['sale_date'])\nsales_history['year'] = sales_history['sale_date'].dt.year\nsales_history['month'] = sales_history['sale_date'].dt.month\nsales_history['week'] = sales_history['sale_date'].dt.isocalendar().week\nsales_history['day_of_week'] = sales_history['sale_date'].dt.dayofweek\nsales_history['is_weekend'] = sales_history['day_of_week'].isin([5, 6]).astype(int)\n\n# Enrich sales with product info\nsales_enriched = pd.merge(\n    sales_history,\n    product_catalog[['product_id', 'category', 'subcategory', 'brand',\n                     'unit_cost', 'weight_kg', 'is_perishable', 'shelf_life_days']],\n    on='product_id',\n    how='left'\n)\n\n# Add promotion flags\npromotions_calendar['promo_date'] = pd.to_datetime(promotions_calendar['promo_date'])\nsales_enriched = pd.merge(\n    sales_enriched,\n    promotions_calendar[['product_id', 'promo_date', 'discount_pct', 'promo_type']],\n    left_on=['product_id', 'sale_date'],\n    right_on=['product_id', 'promo_date'],\n    how='left'\n)\nsales_enriched['is_promotion'] = sales_enriched['discount_pct'].notna().astype(int)\nsales_enriched['discount_pct'] = sales_enriched['discount_pct'].fillna(0)\n\n# Add weather impact\nweather_data['date'] = pd.to_datetime(weather_data['date'])\nsales_enriched = pd.merge(\n    sales_enriched,\n    weather_data[['date', 'region', 'temperature', 'precipitation', 'weather_condition']],\n    left_on=['sale_date', 'region'],\n    right_on=['date', 'region'],\n    how='left'\n)\n\n# Calculate time-based aggregations for demand patterns\ndaily_demand = sales_enriched.groupby(['product_id', 'warehouse_id', 'sale_date']).agg({\n    'quantity': 'sum',\n    'revenue': 'sum',\n    'is_promotion': 'max',\n    'temperature': 'mean'\n}).reset_index()\n\n# Calculate rolling averages for demand forecasting\ndaily_demand = daily_demand.sort_values(['product_id', 'warehouse_id', 'sale_date'])\ndaily_demand['demand_7d_avg'] = daily_demand.groupby(['product_id', 'warehouse_id'])['quantity'].transform(\n    lambda x: x.rolling(7, min_periods=1).mean()\n)\ndaily_demand['demand_30d_avg'] = daily_demand.groupby(['product_id', 'warehouse_id'])['quantity'].transform(\n    lambda x: x.rolling(30, min_periods=1).mean()\n)\ndaily_demand['demand_90d_avg'] = daily_demand.groupby(['product_id', 'warehouse_id'])['quantity'].transform(\n    lambda x: x.rolling(90, min_periods=1).mean()\n)\n\n# Calculate demand volatility\ndaily_demand['demand_7d_std'] = daily_demand.groupby(['product_id', 'warehouse_id'])['quantity'].transform(\n    lambda x: x.rolling(7, min_periods=1).std()\n)\ndaily_demand['demand_coefficient_variation'] = (\n    daily_demand['demand_7d_std'] / daily_demand['demand_7d_avg'].replace(0, 1)\n)\n\n# Seasonality detection\nmonthly_patterns = sales_enriched.groupby(['product_id', 'month']).agg({\n    'quantity': 'mean'\n}).reset_index()\nmonthly_patterns.columns = ['product_id', 'month', 'monthly_avg_demand']\n\nyearly_avg = monthly_patterns.groupby('product_id')['monthly_avg_demand'].transform('mean')\nmonthly_patterns['seasonality_index'] = monthly_patterns['monthly_avg_demand'] / yearly_avg\n\n# Join current inventory levels\ninventory_current = inventory_levels.groupby(['product_id', 'warehouse_id']).agg({\n    'quantity_on_hand': 'sum',\n    'quantity_reserved': 'sum',\n    'quantity_in_transit': 'sum',\n    'last_restock_date': 'max'\n}).reset_index()\ninventory_current['available_inventory'] = (\n    inventory_current['quantity_on_hand'] - inventory_current['quantity_reserved']\n)\n\n# Calculate days of supply\ninventory_analysis = pd.merge(\n    inventory_current,\n    daily_demand.groupby(['product_id', 'warehouse_id']).agg({\n        'demand_30d_avg': 'last'\n    }).reset_index(),\n    on=['product_id', 'warehouse_id'],\n    how='left'\n)\ninventory_analysis['days_of_supply'] = (\n    inventory_analysis['available_inventory'] /\n    inventory_analysis['demand_30d_avg'].replace(0, 0.1)\n)\n\n# Add supplier lead times\ninventory_analysis = pd.merge(\n    inventory_analysis,\n    supplier_data[['product_id', 'supplier_id', 'lead_time_days',\n                   'min_order_qty', 'unit_cost', 'reliability_score']],\n    on='product_id',\n    how='left'\n)\n\n# Calculate reorder points with safety stock\ninventory_analysis['safety_stock'] = (\n    inventory_analysis['demand_30d_avg'] *\n    inventory_analysis['lead_time_days'] * 0.5  # Safety factor\n)\ninventory_analysis['reorder_point'] = (\n    inventory_analysis['demand_30d_avg'] * inventory_analysis['lead_time_days'] +\n    inventory_analysis['safety_stock']\n)\n\n# Flag items needing reorder\ninventory_analysis['needs_reorder'] = (\n    inventory_analysis['available_inventory'] < inventory_analysis['reorder_point']\n).astype(int)\n\n# Calculate optimal order quantity (EOQ approximation)\ninventory_analysis['holding_cost_annual'] = inventory_analysis['unit_cost'] * 0.25\ninventory_analysis['order_cost'] = 50  # Fixed ordering cost\ninventory_analysis['annual_demand'] = inventory_analysis['demand_30d_avg'] * 12 * 30\n\ninventory_analysis['economic_order_qty'] = np.sqrt(\n    2 * inventory_analysis['annual_demand'] * inventory_analysis['order_cost'] /\n    inventory_analysis['holding_cost_annual'].replace(0, 1)\n)\n\n# Calculate shipping optimization\nshipping_analysis = pd.merge(\n    inventory_analysis,\n    warehouse_locations[['warehouse_id', 'latitude', 'longitude', 'capacity_units']],\n    on='warehouse_id',\n    how='left'\n)\n\n# Add shipping costs\nshipping_analysis = pd.merge(\n    shipping_analysis,\n    shipping_costs,\n    on='warehouse_id',\n    how='left'\n)\n\n# Calculate total landed cost\nshipping_analysis['landed_cost'] = (\n    shipping_analysis['unit_cost'] +\n    shipping_analysis['shipping_cost_per_unit'] +\n    shipping_analysis['handling_cost']\n)\n\n# Identify stock-out risk\nshipping_analysis['stockout_risk'] = np.where(\n    shipping_analysis['days_of_supply'] < shipping_analysis['lead_time_days'],\n    'High',\n    np.where(\n        shipping_analysis['days_of_supply'] < shipping_analysis['lead_time_days'] * 1.5,\n        'Medium',\n        'Low'\n    )\n)\n\n# Save outputs\ndaily_demand.to_csv('demand_forecast_features.csv', index=False)\ninventory_analysis.to_csv('inventory_optimization.csv', index=False)\nshipping_analysis.to_csv('supply_chain_analysis.csv', index=False)\nmonthly_patterns.to_csv('seasonality_patterns.csv', index=False)\n] PASSED [ 30%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineMetrics::test_pipeline_conversion_no_errors[marketing_attribution-\nimport pandas as pd\nimport numpy as np\n\n# Load marketing data\ntouchpoints = pd.read_csv('touchpoints.csv')\nconversions = pd.read_csv('conversions.csv')\nad_spend = pd.read_csv('ad_spend.csv')\nchannel_costs = pd.read_csv('channel_costs.csv')\ncampaign_metadata = pd.read_csv('campaign_metadata.csv')\ncustomer_journeys = pd.read_csv('customer_journeys.csv')\norganic_traffic = pd.read_csv('organic_traffic.csv')\noffline_media = pd.read_csv('offline_media.csv')\n\n# Parse timestamps\ntouchpoints['touchpoint_time'] = pd.to_datetime(touchpoints['touchpoint_time'])\nconversions['conversion_time'] = pd.to_datetime(conversions['conversion_time'])\n\n# Create customer journey sequences\ntouchpoints_sorted = touchpoints.sort_values(['customer_id', 'touchpoint_time'])\n\n# Assign journey IDs (new journey if gap > 30 days)\ntouchpoints_sorted['time_since_last'] = touchpoints_sorted.groupby('customer_id')['touchpoint_time'].diff()\ntouchpoints_sorted['new_journey'] = (\n    touchpoints_sorted['time_since_last'] > pd.Timedelta(days=30)\n).fillna(True).astype(int)\ntouchpoints_sorted['journey_id'] = touchpoints_sorted.groupby('customer_id')['new_journey'].cumsum()\n\n# Create journey-level aggregations\njourney_touchpoints = touchpoints_sorted.groupby(['customer_id', 'journey_id']).agg({\n    'touchpoint_id': 'count',\n    'channel': lambda x: ' > '.join(x),\n    'campaign_id': lambda x: list(x.unique()),\n    'touchpoint_time': ['min', 'max'],\n    'cost': 'sum'\n}).reset_index()\njourney_touchpoints.columns = ['customer_id', 'journey_id', 'touchpoint_count',\n                                'channel_path', 'campaigns_touched',\n                                'journey_start', 'journey_end', 'journey_cost']\n\n# Calculate journey duration\njourney_touchpoints['journey_duration_days'] = (\n    journey_touchpoints['journey_end'] - journey_touchpoints['journey_start']\n).dt.total_seconds() / 86400\n\n# Join with conversions\njourney_conversions = pd.merge(\n    journey_touchpoints,\n    conversions[['customer_id', 'conversion_time', 'conversion_value', 'product_category']],\n    on='customer_id',\n    how='left'\n)\n\n# Filter to conversions within journey window\njourney_conversions['converted'] = (\n    (journey_conversions['conversion_time'] >= journey_conversions['journey_start']) &\n    (journey_conversions['conversion_time'] <= journey_conversions['journey_end'] + pd.Timedelta(days=7))\n).astype(int)\n\njourney_conversions['conversion_value'] = np.where(\n    journey_conversions['converted'] == 1,\n    journey_conversions['conversion_value'],\n    0\n)\n\n# Calculate attribution models\n\n# 1. First-touch attribution\ntouchpoints_sorted['is_first_touch'] = touchpoints_sorted.groupby(\n    ['customer_id', 'journey_id']\n).cumcount() == 0\n\nfirst_touch = touchpoints_sorted[touchpoints_sorted['is_first_touch']].copy()\nfirst_touch_attribution = first_touch.groupby('channel').agg({\n    'touchpoint_id': 'count',\n    'cost': 'sum'\n}).reset_index()\nfirst_touch_attribution.columns = ['channel', 'first_touch_count', 'first_touch_cost']\n\n# 2. Last-touch attribution\nlast_touch = touchpoints_sorted.groupby(['customer_id', 'journey_id']).last().reset_index()\nlast_touch_attribution = last_touch.groupby('channel').agg({\n    'touchpoint_id': 'count',\n    'cost': 'sum'\n}).reset_index()\nlast_touch_attribution.columns = ['channel', 'last_touch_count', 'last_touch_cost']\n\n# 3. Linear attribution (equal credit to all touchpoints)\njourney_conversions_exploded = journey_conversions.explode('campaigns_touched')\njourney_conversions_exploded['linear_credit'] = (\n    journey_conversions_exploded['conversion_value'] /\n    journey_conversions_exploded['touchpoint_count']\n)\n\n# 4. Time-decay attribution\ntouchpoints_with_conversion = pd.merge(\n    touchpoints_sorted,\n    journey_conversions[['customer_id', 'journey_id', 'conversion_time', 'conversion_value', 'converted']],\n    on=['customer_id', 'journey_id'],\n    how='left'\n)\n\ntouchpoints_with_conversion['days_to_conversion'] = (\n    touchpoints_with_conversion['conversion_time'] - touchpoints_with_conversion['touchpoint_time']\n).dt.total_seconds() / 86400\n\n# Exponential decay weight (half-life = 7 days)\ntouchpoints_with_conversion['decay_weight'] = np.exp(\n    -touchpoints_with_conversion['days_to_conversion'] / 7\n)\n\n# Normalize weights within journey\ntouchpoints_with_conversion['weight_sum'] = touchpoints_with_conversion.groupby(\n    ['customer_id', 'journey_id']\n)['decay_weight'].transform('sum')\n\ntouchpoints_with_conversion['time_decay_credit'] = (\n    touchpoints_with_conversion['decay_weight'] /\n    touchpoints_with_conversion['weight_sum'].replace(0, 1) *\n    touchpoints_with_conversion['conversion_value']\n)\n\n# 5. Position-based attribution (40% first, 40% last, 20% middle)\ntouchpoints_with_conversion['position'] = touchpoints_with_conversion.groupby(\n    ['customer_id', 'journey_id']\n).cumcount() + 1\n\ntouchpoints_with_conversion['total_positions'] = touchpoints_with_conversion.groupby(\n    ['customer_id', 'journey_id']\n)['position'].transform('max')\n\ntouchpoints_with_conversion['position_weight'] = np.where(\n    touchpoints_with_conversion['position'] == 1,\n    0.4,\n    np.where(\n        touchpoints_with_conversion['position'] == touchpoints_with_conversion['total_positions'],\n        0.4,\n        0.2 / (touchpoints_with_conversion['total_positions'] - 2).replace(0, 1)\n    )\n)\n\ntouchpoints_with_conversion['position_based_credit'] = (\n    touchpoints_with_conversion['position_weight'] *\n    touchpoints_with_conversion['conversion_value']\n)\n\n# Aggregate attribution by channel\nchannel_attribution = touchpoints_with_conversion.groupby('channel').agg({\n    'touchpoint_id': 'count',\n    'cost': 'sum',\n    'linear_credit': 'sum',\n    'time_decay_credit': 'sum',\n    'position_based_credit': 'sum'\n}).reset_index()\n\nchannel_attribution = pd.merge(channel_attribution, first_touch_attribution, on='channel', how='left')\nchannel_attribution = pd.merge(channel_attribution, last_touch_attribution, on='channel', how='left')\n\n# Calculate ROI by attribution model\nchannel_attribution['roi_linear'] = (\n    channel_attribution['linear_credit'] - channel_attribution['cost']\n) / channel_attribution['cost'].replace(0, 1)\n\nchannel_attribution['roi_time_decay'] = (\n    channel_attribution['time_decay_credit'] - channel_attribution['cost']\n) / channel_attribution['cost'].replace(0, 1)\n\nchannel_attribution['roi_position'] = (\n    channel_attribution['position_based_credit'] - channel_attribution['cost']\n) / channel_attribution['cost'].replace(0, 1)\n\n# Marketing mix modeling aggregations\ndaily_spend = ad_spend.groupby(['date', 'channel']).agg({\n    'spend': 'sum',\n    'impressions': 'sum',\n    'clicks': 'sum'\n}).reset_index()\n\ndaily_spend['cpm'] = daily_spend['spend'] / daily_spend['impressions'] * 1000\ndaily_spend['cpc'] = daily_spend['spend'] / daily_spend['clicks'].replace(0, 1)\ndaily_spend['ctr'] = daily_spend['clicks'] / daily_spend['impressions'].replace(0, 1)\n\n# Add adstock transformation (carryover effect)\ndaily_spend = daily_spend.sort_values(['channel', 'date'])\ndaily_spend['spend_adstock'] = daily_spend.groupby('channel')['spend'].transform(\n    lambda x: x.ewm(halflife=7).mean()\n)\n\n# Save outputs\njourney_touchpoints.to_csv('customer_journeys_analyzed.csv', index=False)\nchannel_attribution.to_csv('channel_attribution_models.csv', index=False)\ntouchpoints_with_conversion.to_csv('touchpoint_attribution_detail.csv', index=False)\ndaily_spend.to_csv('marketing_mix_features.csv', index=False)\n] PASSED [ 30%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineMetrics::test_pipeline_conversion_no_errors[iot_predictive_maintenance-\nimport pandas as pd\nimport numpy as np\n\n# Load IoT sensor data\nsensor_readings = pd.read_csv('sensor_readings.csv')\nequipment_registry = pd.read_csv('equipment_registry.csv')\nmaintenance_history = pd.read_csv('maintenance_history.csv')\nfailure_logs = pd.read_csv('failure_logs.csv')\noperating_conditions = pd.read_csv('operating_conditions.csv')\nparts_inventory = pd.read_csv('parts_inventory.csv')\ntechnician_schedules = pd.read_csv('technician_schedules.csv')\n\n# Parse timestamps\nsensor_readings['reading_time'] = pd.to_datetime(sensor_readings['reading_time'])\nmaintenance_history['maintenance_date'] = pd.to_datetime(maintenance_history['maintenance_date'])\nfailure_logs['failure_time'] = pd.to_datetime(failure_logs['failure_time'])\n\n# Clean sensor data - remove outliers\nsensor_readings = sensor_readings.sort_values(['equipment_id', 'sensor_type', 'reading_time'])\n\n# Calculate rolling statistics per sensor\nsensor_readings['value_1h_mean'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].transform(lambda x: x.rolling('1H', on=sensor_readings['reading_time']).mean())\n\nsensor_readings['value_1h_std'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].transform(lambda x: x.rolling('1H', on=sensor_readings['reading_time']).std())\n\nsensor_readings['value_24h_mean'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].transform(lambda x: x.rolling('24H', on=sensor_readings['reading_time']).mean())\n\nsensor_readings['value_24h_max'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].transform(lambda x: x.rolling('24H', on=sensor_readings['reading_time']).max())\n\nsensor_readings['value_24h_min'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].transform(lambda x: x.rolling('24H', on=sensor_readings['reading_time']).min())\n\n# Calculate rate of change\nsensor_readings['value_diff'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].diff()\n\nsensor_readings['value_pct_change'] = sensor_readings.groupby(\n    ['equipment_id', 'sensor_type']\n)['sensor_value'].pct_change()\n\n# Detect anomalies using z-score\nsensor_readings['z_score'] = (\n    (sensor_readings['sensor_value'] - sensor_readings['value_24h_mean']) /\n    sensor_readings['value_1h_std'].replace(0, 1)\n)\nsensor_readings['is_anomaly'] = (abs(sensor_readings['z_score']) > 3).astype(int)\n\n# Pivot sensor types to create feature vectors\nsensor_features = sensor_readings.pivot_table(\n    index=['equipment_id', 'reading_time'],\n    columns='sensor_type',\n    values=['sensor_value', 'value_1h_mean', 'value_24h_max', 'z_score', 'is_anomaly'],\n    aggfunc='first'\n).reset_index()\n\n# Flatten column names\nsensor_features.columns = ['_'.join(str(c) for c in col).strip('_')\n                           for col in sensor_features.columns]\n\n# Add equipment metadata\nsensor_features = pd.merge(\n    sensor_features,\n    equipment_registry[['equipment_id', 'equipment_type', 'manufacturer',\n                        'installation_date', 'rated_capacity', 'location']],\n    on='equipment_id',\n    how='left'\n)\n\n# Calculate equipment age\nsensor_features['installation_date'] = pd.to_datetime(sensor_features['installation_date'])\nsensor_features['equipment_age_days'] = (\n    sensor_features['reading_time'] - sensor_features['installation_date']\n).dt.days\n\n# Add operating conditions\noperating_conditions['condition_time'] = pd.to_datetime(operating_conditions['condition_time'])\nsensor_features = pd.merge_asof(\n    sensor_features.sort_values('reading_time'),\n    operating_conditions.sort_values('condition_time'),\n    left_on='reading_time',\n    right_on='condition_time',\n    by='equipment_id',\n    direction='backward'\n)\n\n# Calculate maintenance history features\nmaintenance_agg = maintenance_history.groupby('equipment_id').agg({\n    'maintenance_id': 'count',\n    'maintenance_date': 'max',\n    'maintenance_cost': 'sum',\n    'downtime_hours': 'sum'\n}).reset_index()\nmaintenance_agg.columns = ['equipment_id', 'total_maintenance_count',\n                           'last_maintenance_date', 'total_maintenance_cost',\n                           'total_downtime_hours']\n\nsensor_features = pd.merge(\n    sensor_features,\n    maintenance_agg,\n    on='equipment_id',\n    how='left'\n)\n\n# Calculate days since last maintenance\nsensor_features['last_maintenance_date'] = pd.to_datetime(sensor_features['last_maintenance_date'])\nsensor_features['days_since_maintenance'] = (\n    sensor_features['reading_time'] - sensor_features['last_maintenance_date']\n).dt.days\n\n# Calculate failure history features\nfailure_agg = failure_logs.groupby('equipment_id').agg({\n    'failure_id': 'count',\n    'failure_time': 'max',\n    'failure_severity': 'mean',\n    'repair_time_hours': 'sum'\n}).reset_index()\nfailure_agg.columns = ['equipment_id', 'total_failures', 'last_failure_date',\n                       'avg_failure_severity', 'total_repair_hours']\n\nsensor_features = pd.merge(\n    sensor_features,\n    failure_agg,\n    on='equipment_id',\n    how='left'\n)\n\n# Create target variable: failure within next N days\nfailure_logs_sorted = failure_logs.sort_values(['equipment_id', 'failure_time'])\nsensor_features = pd.merge_asof(\n    sensor_features.sort_values('reading_time'),\n    failure_logs_sorted[['equipment_id', 'failure_time']].rename(\n        columns={'failure_time': 'next_failure_time'}\n    ),\n    left_on='reading_time',\n    right_on='next_failure_time',\n    by='equipment_id',\n    direction='forward'\n)\n\nsensor_features['days_to_failure'] = (\n    sensor_features['next_failure_time'] - sensor_features['reading_time']\n).dt.total_seconds() / 86400\n\nsensor_features['failure_within_7d'] = (sensor_features['days_to_failure'] <= 7).astype(int)\nsensor_features['failure_within_30d'] = (sensor_features['days_to_failure'] <= 30).astype(int)\n\n# Calculate health score\nsensor_features['anomaly_count_24h'] = sensor_features.groupby('equipment_id')['is_anomaly_temperature'].transform(\n    lambda x: x.rolling('24H', on=sensor_features['reading_time']).sum()\n)\n\nsensor_features['health_score'] = 100 - (\n    sensor_features['anomaly_count_24h'] * 5 +\n    sensor_features['total_failures'].fillna(0) * 2 +\n    np.minimum(sensor_features['days_since_maintenance'].fillna(0) / 365, 1) * 20\n).clip(0, 100)\n\n# Prioritize maintenance\nsensor_features['maintenance_priority'] = np.where(\n    sensor_features['health_score'] < 50, 'Critical',\n    np.where(sensor_features['health_score'] < 70, 'High',\n    np.where(sensor_features['health_score'] < 85, 'Medium', 'Low'))\n)\n\n# Join parts availability\nparts_available = parts_inventory.groupby('equipment_type').agg({\n    'part_id': 'count',\n    'quantity_available': 'sum',\n    'lead_time_days': 'mean'\n}).reset_index()\nparts_available.columns = ['equipment_type', 'part_types_available',\n                           'total_parts_stock', 'avg_part_lead_time']\n\nsensor_features = pd.merge(\n    sensor_features,\n    parts_available,\n    on='equipment_type',\n    how='left'\n)\n\n# Save outputs\nsensor_features.to_csv('predictive_maintenance_features.csv', index=False)\n\n# Create maintenance schedule recommendations\nmaintenance_schedule = sensor_features[sensor_features['maintenance_priority'].isin(['Critical', 'High'])].groupby(\n    'equipment_id'\n).agg({\n    'health_score': 'min',\n    'maintenance_priority': 'first',\n    'days_since_maintenance': 'max',\n    'reading_time': 'max'\n}).reset_index()\n\nmaintenance_schedule.columns = ['equipment_id', 'current_health', 'priority',\n                                'days_since_last_maintenance', 'last_reading']\nmaintenance_schedule.to_csv('maintenance_schedule_recommendations.csv', index=False)\n] PASSED [ 30%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineMetrics::test_pipeline_conversion_no_errors[genomic_analysis-\nimport pandas as pd\nimport numpy as np\n\n# Load genomic data\nvariants = pd.read_csv('variants.csv')\nsamples = pd.read_csv('samples.csv')\ngene_annotations = pd.read_csv('gene_annotations.csv')\nclinical_data = pd.read_csv('clinical_data.csv')\npathway_mappings = pd.read_csv('pathway_mappings.csv')\npopulation_frequencies = pd.read_csv('population_frequencies.csv')\nfunctional_predictions = pd.read_csv('functional_predictions.csv')\ndisease_associations = pd.read_csv('disease_associations.csv')\n\n# Standardize variant identifiers\nvariants['variant_id'] = (\n    variants['chromosome'].astype(str) + ':' +\n    variants['position'].astype(str) + ':' +\n    variants['reference'] + '>' + variants['alternate']\n)\n\n# Calculate variant quality metrics\nvariants['quality_pass'] = (\n    (variants['quality_score'] >= 30) &\n    (variants['read_depth'] >= 10) &\n    (variants['allele_frequency'] >= 0.2)\n).astype(int)\n\n# Filter high-quality variants\nhigh_quality_variants = variants[variants['quality_pass'] == 1].copy()\n\n# Annotate with gene information\nhigh_quality_variants = pd.merge(\n    high_quality_variants,\n    gene_annotations[['chromosome', 'start_position', 'end_position',\n                      'gene_symbol', 'gene_type', 'strand']],\n    on='chromosome',\n    how='left'\n)\n\n# Filter to variants within gene boundaries\nhigh_quality_variants = high_quality_variants[\n    (high_quality_variants['position'] >= high_quality_variants['start_position']) &\n    (high_quality_variants['position'] <= high_quality_variants['end_position'])\n]\n\n# Add functional impact predictions\nhigh_quality_variants = pd.merge(\n    high_quality_variants,\n    functional_predictions[['variant_id', 'sift_score', 'polyphen_score',\n                            'cadd_score', 'predicted_impact']],\n    on='variant_id',\n    how='left'\n)\n\n# Classify variant impact\nhigh_quality_variants['impact_category'] = np.where(\n    (high_quality_variants['sift_score'] < 0.05) | (high_quality_variants['polyphen_score'] > 0.85),\n    'Damaging',\n    np.where(\n        (high_quality_variants['sift_score'] < 0.1) | (high_quality_variants['polyphen_score'] > 0.5),\n        'Possibly_Damaging',\n        'Benign'\n    )\n)\n\n# Add population allele frequencies\nhigh_quality_variants = pd.merge(\n    high_quality_variants,\n    population_frequencies[['variant_id', 'gnomad_af', 'gnomad_af_eas',\n                            'gnomad_af_eur', 'gnomad_af_afr']],\n    on='variant_id',\n    how='left'\n)\n\n# Flag rare variants (MAF < 1%)\nhigh_quality_variants['is_rare'] = (high_quality_variants['gnomad_af'] < 0.01).astype(int)\nhigh_quality_variants['is_ultra_rare'] = (high_quality_variants['gnomad_af'] < 0.001).astype(int)\n\n# Add disease associations\nhigh_quality_variants = pd.merge(\n    high_quality_variants,\n    disease_associations[['gene_symbol', 'disease_name', 'inheritance_pattern',\n                          'clinical_significance']],\n    on='gene_symbol',\n    how='left'\n)\n\n# Calculate per-sample variant burden\nsample_burden = high_quality_variants.groupby('sample_id').agg({\n    'variant_id': 'count',\n    'is_rare': 'sum',\n    'impact_category': lambda x: (x == 'Damaging').sum()\n}).reset_index()\nsample_burden.columns = ['sample_id', 'total_variants', 'rare_variants', 'damaging_variants']\n\n# Add clinical phenotype data\nsample_burden = pd.merge(\n    sample_burden,\n    samples[['sample_id', 'patient_id', 'tissue_type', 'collection_date']],\n    on='sample_id',\n    how='left'\n)\n\nsample_burden = pd.merge(\n    sample_burden,\n    clinical_data[['patient_id', 'diagnosis', 'age_at_diagnosis',\n                   'sex', 'ethnicity', 'family_history']],\n    on='patient_id',\n    how='left'\n)\n\n# Calculate gene-level burden\ngene_burden = high_quality_variants.groupby(['sample_id', 'gene_symbol']).agg({\n    'variant_id': 'count',\n    'is_rare': 'sum',\n    'impact_category': lambda x: (x == 'Damaging').sum(),\n    'cadd_score': 'max'\n}).reset_index()\ngene_burden.columns = ['sample_id', 'gene_symbol', 'variants_in_gene',\n                       'rare_in_gene', 'damaging_in_gene', 'max_cadd']\n\n# Add pathway information\ngene_burden = pd.merge(\n    gene_burden,\n    pathway_mappings[['gene_symbol', 'pathway_id', 'pathway_name']],\n    on='gene_symbol',\n    how='left'\n)\n\n# Calculate pathway-level burden\npathway_burden = gene_burden.groupby(['sample_id', 'pathway_id', 'pathway_name']).agg({\n    'gene_symbol': 'nunique',\n    'variants_in_gene': 'sum',\n    'damaging_in_gene': 'sum',\n    'max_cadd': 'max'\n}).reset_index()\npathway_burden.columns = ['sample_id', 'pathway_id', 'pathway_name',\n                          'genes_affected', 'pathway_variants',\n                          'pathway_damaging', 'pathway_max_cadd']\n\n# Identify potentially pathogenic variants\npathogenic_candidates = high_quality_variants[\n    (high_quality_variants['impact_category'] == 'Damaging') &\n    (high_quality_variants['is_rare'] == 1) &\n    (high_quality_variants['clinical_significance'].isin(['Pathogenic', 'Likely_Pathogenic']).fillna(False))\n].copy()\n\n# Create variant report\nvariant_report = pathogenic_candidates.groupby(['sample_id', 'gene_symbol']).agg({\n    'variant_id': lambda x: '; '.join(x),\n    'disease_name': 'first',\n    'inheritance_pattern': 'first',\n    'cadd_score': 'max'\n}).reset_index()\n\n# Save outputs\nhigh_quality_variants.to_csv('annotated_variants.csv', index=False)\nsample_burden.to_csv('sample_variant_burden.csv', index=False)\ngene_burden.to_csv('gene_level_burden.csv', index=False)\npathway_burden.to_csv('pathway_analysis.csv', index=False)\nvariant_report.to_csv('pathogenic_variant_report.csv', index=False)\n] PASSED [ 30%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineMetrics::test_pipeline_conversion_no_errors[clickstream_analysis-\nimport pandas as pd\nimport numpy as np\n\n# Load clickstream data\npage_views = pd.read_csv('page_views.csv')\nclick_events = pd.read_csv('click_events.csv')\nform_submissions = pd.read_csv('form_submissions.csv')\nsearch_queries = pd.read_csv('search_queries.csv')\nproduct_impressions = pd.read_csv('product_impressions.csv')\ncart_events = pd.read_csv('cart_events.csv')\nuser_agents = pd.read_csv('user_agents.csv')\ngeo_locations = pd.read_csv('geo_locations.csv')\n\n# Parse timestamps\npage_views['timestamp'] = pd.to_datetime(page_views['timestamp'])\nclick_events['timestamp'] = pd.to_datetime(click_events['timestamp'])\n\n# Combine all events\npage_views['event_type'] = 'page_view'\nclick_events['event_type'] = 'click'\nform_submissions['event_type'] = 'form_submit'\nsearch_queries['event_type'] = 'search'\ncart_events['event_type'] = cart_events['cart_action']\n\nall_events = pd.concat([\n    page_views[['session_id', 'user_id', 'timestamp', 'event_type', 'page_url', 'referrer']],\n    click_events[['session_id', 'user_id', 'timestamp', 'event_type', 'element_id', 'page_url']],\n    form_submissions[['session_id', 'user_id', 'timestamp', 'event_type', 'form_id', 'page_url']],\n    search_queries[['session_id', 'user_id', 'timestamp', 'event_type', 'query', 'results_count']],\n    cart_events[['session_id', 'user_id', 'timestamp', 'event_type', 'product_id', 'quantity']]\n], ignore_index=True, sort=False)\n\nall_events = all_events.sort_values(['session_id', 'timestamp'])\n\n# Session-level aggregations\nsession_metrics = all_events.groupby('session_id').agg({\n    'user_id': 'first',\n    'timestamp': ['min', 'max', 'count'],\n    'page_url': 'nunique',\n    'event_type': lambda x: x.value_counts().to_dict()\n}).reset_index()\n\nsession_metrics.columns = ['session_id', 'user_id', 'session_start',\n                            'session_end', 'total_events', 'unique_pages', 'event_breakdown']\n\n# Calculate session duration\nsession_metrics['session_duration_seconds'] = (\n    session_metrics['session_end'] - session_metrics['session_start']\n).dt.total_seconds()\n\n# Extract event counts\nsession_metrics['page_views'] = session_metrics['event_breakdown'].apply(\n    lambda x: x.get('page_view', 0) if isinstance(x, dict) else 0\n)\nsession_metrics['clicks'] = session_metrics['event_breakdown'].apply(\n    lambda x: x.get('click', 0) if isinstance(x, dict) else 0\n)\nsession_metrics['searches'] = session_metrics['event_breakdown'].apply(\n    lambda x: x.get('search', 0) if isinstance(x, dict) else 0\n)\nsession_metrics['cart_adds'] = session_metrics['event_breakdown'].apply(\n    lambda x: x.get('add_to_cart', 0) if isinstance(x, dict) else 0\n)\n\n# Calculate engagement metrics\nsession_metrics['pages_per_minute'] = (\n    session_metrics['unique_pages'] /\n    (session_metrics['session_duration_seconds'] / 60).replace(0, 1)\n)\nsession_metrics['events_per_page'] = (\n    session_metrics['total_events'] / session_metrics['unique_pages'].replace(0, 1)\n)\n\n# Identify bounce sessions\nsession_metrics['is_bounce'] = (\n    (session_metrics['unique_pages'] == 1) &\n    (session_metrics['session_duration_seconds'] < 10)\n).astype(int)\n\n# Create page flow sequences\npage_sequences = all_events[all_events['event_type'] == 'page_view'].copy()\npage_sequences['page_order'] = page_sequences.groupby('session_id').cumcount() + 1\npage_sequences['next_page'] = page_sequences.groupby('session_id')['page_url'].shift(-1)\n\n# Calculate page transitions\npage_transitions = page_sequences.groupby(['page_url', 'next_page']).size().reset_index(name='transition_count')\npage_transitions = page_transitions[page_transitions['next_page'].notna()]\n\n# Calculate exit rates per page\npage_stats = page_sequences.groupby('page_url').agg({\n    'session_id': 'count',\n    'next_page': lambda x: x.isna().sum()\n}).reset_index()\npage_stats.columns = ['page_url', 'total_views', 'exits']\npage_stats['exit_rate'] = page_stats['exits'] / page_stats['total_views']\n\n# Analyze search behavior\nsearch_analysis = search_queries.groupby('session_id').agg({\n    'query': ['count', lambda x: ' | '.join(x)],\n    'results_count': ['mean', 'min'],\n    'clicked_result': 'sum'\n}).reset_index()\nsearch_analysis.columns = ['session_id', 'search_count', 'search_queries',\n                            'avg_results', 'min_results', 'result_clicks']\nsearch_analysis['search_success_rate'] = (\n    search_analysis['result_clicks'] / search_analysis['search_count'].replace(0, 1)\n)\n\n# Analyze cart behavior\ncart_analysis = cart_events.groupby('session_id').agg({\n    'product_id': 'nunique',\n    'quantity': 'sum',\n    'cart_action': lambda x: (x == 'add_to_cart').sum()\n}).reset_index()\ncart_analysis.columns = ['session_id', 'unique_cart_products',\n                          'total_cart_quantity', 'add_to_cart_events']\n\n# Add purchase flag\npurchases = cart_events[cart_events['cart_action'] == 'purchase'].groupby('session_id').agg({\n    'order_value': 'sum'\n}).reset_index()\n\ncart_analysis = pd.merge(cart_analysis, purchases, on='session_id', how='left')\ncart_analysis['converted'] = cart_analysis['order_value'].notna().astype(int)\ncart_analysis['order_value'] = cart_analysis['order_value'].fillna(0)\n\n# Merge all session data\nsession_complete = session_metrics.copy()\nsession_complete = pd.merge(session_complete, search_analysis, on='session_id', how='left')\nsession_complete = pd.merge(session_complete, cart_analysis, on='session_id', how='left')\n\n# Add user agent info\nsession_complete = pd.merge(\n    session_complete,\n    user_agents[['session_id', 'device_type', 'browser', 'os', 'is_mobile']],\n    on='session_id',\n    how='left'\n)\n\n# Add geo location\nsession_complete = pd.merge(\n    session_complete,\n    geo_locations[['session_id', 'country', 'region', 'city', 'timezone']],\n    on='session_id',\n    how='left'\n)\n\n# Calculate user-level aggregations\nuser_metrics = session_complete.groupby('user_id').agg({\n    'session_id': 'count',\n    'session_duration_seconds': ['mean', 'sum'],\n    'page_views': ['mean', 'sum'],\n    'converted': 'sum',\n    'order_value': 'sum',\n    'session_start': ['min', 'max']\n}).reset_index()\n\nuser_metrics.columns = ['user_id', 'total_sessions', 'avg_session_duration',\n                        'total_time_spent', 'avg_pages_per_session', 'total_pages_viewed',\n                        'total_conversions', 'total_revenue', 'first_visit', 'last_visit']\n\nuser_metrics['conversion_rate'] = (\n    user_metrics['total_conversions'] / user_metrics['total_sessions']\n)\nuser_metrics['avg_order_value'] = (\n    user_metrics['total_revenue'] / user_metrics['total_conversions'].replace(0, 1)\n)\n\n# Segment users by engagement\nuser_metrics['engagement_segment'] = pd.cut(\n    user_metrics['total_time_spent'] / 3600,\n    bins=[-np.inf, 0.1, 1, 5, 20, np.inf],\n    labels=['Minimal', 'Light', 'Medium', 'Heavy', 'Power']\n)\n\n# Save outputs\nsession_complete.to_csv('session_analytics.csv', index=False)\nuser_metrics.to_csv('user_engagement_metrics.csv', index=False)\npage_stats.to_csv('page_performance.csv', index=False)\npage_transitions.to_csv('page_flow_analysis.csv', index=False)\n] PASSED [ 30%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineMetrics::test_pipeline_conversion_no_errors[portfolio_risk-\nimport pandas as pd\nimport numpy as np\n\n# Load financial data\npositions = pd.read_csv('positions.csv')\nprice_history = pd.read_csv('price_history.csv')\ninstruments = pd.read_csv('instruments.csv')\nmarket_data = pd.read_csv('market_data.csv')\nfx_rates = pd.read_csv('fx_rates.csv')\nbenchmark_returns = pd.read_csv('benchmark_returns.csv')\nfactor_exposures = pd.read_csv('factor_exposures.csv')\ncredit_ratings = pd.read_csv('credit_ratings.csv')\n\n# Parse dates\nprice_history['date'] = pd.to_datetime(price_history['date'])\nmarket_data['date'] = pd.to_datetime(market_data['date'])\nfx_rates['date'] = pd.to_datetime(fx_rates['date'])\n\n# Calculate daily returns\nprice_history = price_history.sort_values(['instrument_id', 'date'])\nprice_history['daily_return'] = price_history.groupby('instrument_id')['close_price'].pct_change()\nprice_history['log_return'] = np.log(\n    price_history['close_price'] / price_history.groupby('instrument_id')['close_price'].shift(1)\n)\n\n# Calculate rolling volatility\nprice_history['volatility_20d'] = price_history.groupby('instrument_id')['daily_return'].transform(\n    lambda x: x.rolling(20).std() * np.sqrt(252)\n)\nprice_history['volatility_60d'] = price_history.groupby('instrument_id')['daily_return'].transform(\n    lambda x: x.rolling(60).std() * np.sqrt(252)\n)\n\n# Calculate rolling beta to market\nmarket_returns = market_data[market_data['index_id'] == 'SPX'][['date', 'daily_return']].copy()\nmarket_returns.columns = ['date', 'market_return']\n\nprice_with_market = pd.merge(price_history, market_returns, on='date', how='left')\n\ndef calculate_beta(group):\n    if len(group) < 60:\n        return np.nan\n    cov = group['daily_return'].cov(group['market_return'])\n    var = group['market_return'].var()\n    return cov / var if var != 0 else np.nan\n\nprice_with_market['beta'] = price_with_market.groupby('instrument_id').apply(\n    lambda x: x['daily_return'].rolling(60).cov(x['market_return']) /\n              x['market_return'].rolling(60).var()\n).reset_index(level=0, drop=True)\n\n# Enrich positions with instrument details\npositions_enriched = pd.merge(\n    positions,\n    instruments[['instrument_id', 'instrument_type', 'currency', 'sector',\n                 'country', 'maturity_date', 'coupon_rate', 'issuer']],\n    on='instrument_id',\n    how='left'\n)\n\n# Get latest prices and metrics\nlatest_prices = price_with_market.groupby('instrument_id').last().reset_index()\npositions_enriched = pd.merge(\n    positions_enriched,\n    latest_prices[['instrument_id', 'close_price', 'daily_return',\n                   'volatility_20d', 'volatility_60d', 'beta']],\n    on='instrument_id',\n    how='left'\n)\n\n# Convert to base currency (USD)\nfx_latest = fx_rates.groupby('currency_pair').last().reset_index()\nfx_latest['to_currency'] = fx_latest['currency_pair'].str[-3:]\nfx_latest['from_currency'] = fx_latest['currency_pair'].str[:3]\n\npositions_enriched = pd.merge(\n    positions_enriched,\n    fx_latest[['from_currency', 'rate']].rename(columns={'from_currency': 'currency', 'rate': 'fx_rate'}),\n    on='currency',\n    how='left'\n)\npositions_enriched['fx_rate'] = positions_enriched['fx_rate'].fillna(1.0)\n\n# Calculate market values\npositions_enriched['market_value_local'] = positions_enriched['quantity'] * positions_enriched['close_price']\npositions_enriched['market_value_usd'] = positions_enriched['market_value_local'] / positions_enriched['fx_rate']\n\n# Calculate position-level risk metrics\npositions_enriched['position_volatility'] = (\n    positions_enriched['market_value_usd'] * positions_enriched['volatility_20d']\n)\n\n# Portfolio-level calculations\nportfolio_value = positions_enriched.groupby('portfolio_id')['market_value_usd'].sum().reset_index()\nportfolio_value.columns = ['portfolio_id', 'total_nav']\n\npositions_enriched = pd.merge(positions_enriched, portfolio_value, on='portfolio_id', how='left')\npositions_enriched['weight'] = positions_enriched['market_value_usd'] / positions_enriched['total_nav']\n\n# Calculate weighted portfolio beta\nportfolio_beta = positions_enriched.groupby('portfolio_id').apply(\n    lambda x: (x['weight'] * x['beta']).sum()\n).reset_index(name='portfolio_beta')\n\n# Calculate portfolio volatility (simplified - assuming no correlation)\nportfolio_vol_contrib = positions_enriched.groupby('portfolio_id').apply(\n    lambda x: np.sqrt((x['weight']**2 * x['volatility_20d']**2).sum())\n).reset_index(name='portfolio_volatility')\n\n# Sector concentration\nsector_concentration = positions_enriched.groupby(['portfolio_id', 'sector']).agg({\n    'market_value_usd': 'sum',\n    'weight': 'sum'\n}).reset_index()\nsector_concentration.columns = ['portfolio_id', 'sector', 'sector_value', 'sector_weight']\n\n# Country concentration\ncountry_concentration = positions_enriched.groupby(['portfolio_id', 'country']).agg({\n    'market_value_usd': 'sum',\n    'weight': 'sum'\n}).reset_index()\n\n# Add credit risk for fixed income\npositions_enriched = pd.merge(\n    positions_enriched,\n    credit_ratings[['issuer', 'rating', 'rating_numeric', 'default_probability']],\n    on='issuer',\n    how='left'\n)\n\n# Calculate credit VaR contribution\npositions_enriched['credit_var_contrib'] = (\n    positions_enriched['market_value_usd'] *\n    positions_enriched['default_probability'].fillna(0) *\n    (1 - 0.4)  # Assuming 40% recovery rate\n)\n\n# Calculate VaR (parametric approach)\nconfidence_level = 0.99\nz_score = 2.326  # 99% confidence\n\npositions_enriched['var_1d'] = (\n    positions_enriched['market_value_usd'] *\n    positions_enriched['volatility_20d'] / np.sqrt(252) *\n    z_score\n)\n\nportfolio_var = positions_enriched.groupby('portfolio_id')['var_1d'].sum().reset_index()\nportfolio_var.columns = ['portfolio_id', 'total_var_1d']\n\n# Historical VaR calculation\nreturns_pivot = price_history.pivot_table(\n    index='date',\n    columns='instrument_id',\n    values='daily_return'\n).fillna(0)\n\n# Calculate portfolio returns for each portfolio\nportfolio_returns = []\nfor portfolio_id in positions_enriched['portfolio_id'].unique():\n    port_positions = positions_enriched[positions_enriched['portfolio_id'] == portfolio_id]\n    weights = port_positions.set_index('instrument_id')['weight'].to_dict()\n\n    port_return = returns_pivot[list(weights.keys())].mul(\n        pd.Series(weights)\n    ).sum(axis=1)\n\n    portfolio_returns.append(pd.DataFrame({\n        'portfolio_id': portfolio_id,\n        'date': returns_pivot.index,\n        'portfolio_return': port_return\n    }))\n\nportfolio_returns_df = pd.concat(portfolio_returns, ignore_index=True)\n\n# Calculate historical VaR\nhistorical_var = portfolio_returns_df.groupby('portfolio_id')['portfolio_return'].apply(\n    lambda x: x.quantile(1 - confidence_level)\n).reset_index(name='historical_var_1d')\n\n# Calculate Sharpe ratio\nrisk_free_rate = 0.05 / 252  # Daily risk-free rate\n\nportfolio_sharpe = portfolio_returns_df.groupby('portfolio_id').agg({\n    'portfolio_return': ['mean', 'std']\n}).reset_index()\nportfolio_sharpe.columns = ['portfolio_id', 'avg_return', 'return_std']\nportfolio_sharpe['sharpe_ratio'] = (\n    (portfolio_sharpe['avg_return'] - risk_free_rate) / portfolio_sharpe['return_std']\n) * np.sqrt(252)\n\n# Compile portfolio risk summary\nportfolio_risk = portfolio_value.copy()\nportfolio_risk = pd.merge(portfolio_risk, portfolio_beta, on='portfolio_id', how='left')\nportfolio_risk = pd.merge(portfolio_risk, portfolio_vol_contrib, on='portfolio_id', how='left')\nportfolio_risk = pd.merge(portfolio_risk, portfolio_var, on='portfolio_id', how='left')\nportfolio_risk = pd.merge(portfolio_risk, historical_var, on='portfolio_id', how='left')\nportfolio_risk = pd.merge(portfolio_risk, portfolio_sharpe, on='portfolio_id', how='left')\n\n# Save outputs\npositions_enriched.to_csv('positions_with_risk.csv', index=False)\nportfolio_risk.to_csv('portfolio_risk_summary.csv', index=False)\nsector_concentration.to_csv('sector_concentration.csv', index=False)\ncountry_concentration.to_csv('country_concentration.csv', index=False)\nportfolio_returns_df.to_csv('portfolio_returns_history.csv', index=False)\n] PASSED [ 31%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineMetrics::test_fraud_detection_complexity PASSED [ 31%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineMetrics::test_customer_360_complexity PASSED [ 31%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineMetrics::test_supply_chain_complexity PASSED [ 31%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineExports::test_fraud_detection_to_json PASSED [ 31%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineExports::test_customer_360_to_yaml PASSED [ 31%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineExports::test_supply_chain_to_dict PASSED [ 31%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineCodePatterns::test_all_pipelines_have_imports PASSED [ 31%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineCodePatterns::test_all_pipelines_have_data_loading PASSED [ 32%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineCodePatterns::test_all_pipelines_have_data_output PASSED [ 32%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineCodePatterns::test_all_pipelines_have_transformations PASSED [ 32%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineDataSources::test_fraud_detection_data_sources PASSED [ 32%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineDataSources::test_customer_360_data_sources PASSED [ 32%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineDataSources::test_supply_chain_data_sources PASSED [ 32%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineDataSources::test_iot_predictive_maintenance_data_sources PASSED [ 32%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineKeyOperations::test_fraud_detection_operations PASSED [ 32%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineKeyOperations::test_customer_360_operations PASSED [ 32%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineKeyOperations::test_marketing_attribution_operations PASSED [ 33%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineIntegration::test_py2dataiku_convert_fraud_detection PASSED [ 33%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineIntegration::test_py2dataiku_convert_all_complex PASSED [ 33%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineIntegration::test_py2dataiku_visualize_complex PASSED [ 33%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineEdgeCases::test_very_long_pipeline_conversion PASSED [ 33%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineEdgeCases::test_pipeline_with_many_outputs PASSED [ 33%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineEdgeCases::test_pipeline_with_nested_aggregations PASSED [ 33%]
tests/test_py2dataiku/test_complex_pipelines.py::TestComplexPipelineEdgeCases::test_pipeline_with_window_functions PASSED [ 33%]
tests/test_py2dataiku/test_integration.py::TestEndToEndConversion::test_simple_prepare_pipeline PASSED [ 34%]
tests/test_py2dataiku/test_integration.py::TestEndToEndConversion::test_join_pipeline PASSED [ 34%]
tests/test_py2dataiku/test_integration.py::TestEndToEndConversion::test_groupby_pipeline PASSED [ 34%]
tests/test_py2dataiku/test_integration.py::TestEndToEndConversion::test_filter_pipeline PASSED [ 34%]
tests/test_py2dataiku/test_integration.py::TestEndToEndConversion::test_concat_pipeline PASSED [ 34%]
tests/test_py2dataiku/test_integration.py::TestDiagramGeneration::test_mermaid_generation PASSED [ 34%]
tests/test_py2dataiku/test_integration.py::TestDiagramGeneration::test_graphviz_generation PASSED [ 34%]
tests/test_py2dataiku/test_integration.py::TestDiagramGeneration::test_ascii_generation PASSED [ 34%]
tests/test_py2dataiku/test_integration.py::TestDiagramGeneration::test_plantuml_generation PASSED [ 34%]
tests/test_py2dataiku/test_integration.py::TestCodeAnalyzer::test_parse_read_csv PASSED [ 35%]
tests/test_py2dataiku/test_integration.py::TestCodeAnalyzer::test_parse_fillna PASSED [ 35%]
tests/test_py2dataiku/test_integration.py::TestCodeAnalyzer::test_parse_dropna PASSED [ 35%]
tests/test_py2dataiku/test_integration.py::TestCodeAnalyzer::test_parse_rename PASSED [ 35%]
tests/test_py2dataiku/test_integration.py::TestCodeAnalyzer::test_parse_drop_columns PASSED [ 35%]
tests/test_py2dataiku/test_integration.py::TestCodeAnalyzer::test_parse_merge PASSED [ 35%]
tests/test_py2dataiku/test_integration.py::TestCodeAnalyzer::test_parse_sort_values PASSED [ 35%]
tests/test_py2dataiku/test_integration.py::TestCodeAnalyzer::test_syntax_error_handling PASSED [ 35%]
tests/test_py2dataiku/test_integration.py::TestRecipeConfigurations::test_prepare_recipe_json_structure PASSED [ 36%]
tests/test_py2dataiku/test_integration.py::TestRecipeConfigurations::test_join_recipe_json_structure PASSED [ 36%]
tests/test_py2dataiku/test_llm.py::TestDataStep::test_create_data_step PASSED [ 36%]
tests/test_py2dataiku/test_llm.py::TestDataStep::test_data_step_from_dict PASSED [ 36%]
tests/test_py2dataiku/test_llm.py::TestDataStep::test_data_step_to_dict PASSED [ 36%]
tests/test_py2dataiku/test_llm.py::TestDataStep::test_operation_types PASSED [ 36%]
tests/test_py2dataiku/test_llm.py::TestAnalysisResult::test_create_analysis_result PASSED [ 36%]
tests/test_py2dataiku/test_llm.py::TestAnalysisResult::test_analysis_result_to_json PASSED [ 36%]
tests/test_py2dataiku/test_llm.py::TestAnalysisResult::test_analysis_result_from_dict PASSED [ 37%]
tests/test_py2dataiku/test_llm.py::TestMockProvider::test_mock_provider_default_response PASSED [ 37%]
tests/test_py2dataiku/test_llm.py::TestMockProvider::test_mock_provider_custom_response PASSED [ 37%]
tests/test_py2dataiku/test_llm.py::TestMockProvider::test_mock_provider_json PASSED [ 37%]
tests/test_py2dataiku/test_llm.py::TestMockProvider::test_mock_provider_tracks_calls PASSED [ 37%]
tests/test_py2dataiku/test_llm.py::TestLLMCodeAnalyzer::test_analyze_with_mock PASSED [ 37%]
tests/test_py2dataiku/test_llm.py::TestLLMCodeAnalyzer::test_analyze_error_handling PASSED [ 37%]
tests/test_py2dataiku/test_llm.py::TestLLMCodeAnalyzer::test_post_processing PASSED [ 37%]
tests/test_py2dataiku/test_llm.py::TestLLMFlowGenerator::test_generate_simple_flow PASSED [ 37%]
tests/test_py2dataiku/test_llm.py::TestLLMFlowGenerator::test_generate_grouping_recipe PASSED [ 38%]
tests/test_py2dataiku/test_llm.py::TestLLMFlowGenerator::test_generate_join_recipe PASSED [ 38%]
tests/test_py2dataiku/test_llm.py::TestLLMFlowGenerator::test_generate_prepare_with_transforms PASSED [ 38%]
tests/test_py2dataiku/test_llm.py::TestLLMFlowGenerator::test_generate_python_fallback PASSED [ 38%]
tests/test_py2dataiku/test_llm.py::TestGetProvider::test_get_mock_provider PASSED [ 38%]
tests/test_py2dataiku/test_llm.py::TestGetProvider::test_get_unknown_provider PASSED [ 38%]
tests/test_py2dataiku/test_llm.py::TestGetProvider::test_anthropic_requires_key PASSED [ 38%]
tests/test_py2dataiku/test_llm.py::TestGetProvider::test_openai_requires_key PASSED [ 38%]
tests/test_py2dataiku/test_llm.py::TestEndToEndLLM::test_full_pipeline PASSED [ 39%]
tests/test_py2dataiku/test_models.py::TestDataikuDataset::test_create_dataset PASSED [ 39%]
tests/test_py2dataiku/test_models.py::TestDataikuDataset::test_input_dataset PASSED [ 39%]
tests/test_py2dataiku/test_models.py::TestDataikuDataset::test_output_dataset PASSED [ 39%]
tests/test_py2dataiku/test_models.py::TestDataikuDataset::test_add_column PASSED [ 39%]
tests/test_py2dataiku/test_models.py::TestDataikuDataset::test_to_dict PASSED [ 39%]
tests/test_py2dataiku/test_models.py::TestPrepareStep::test_fill_empty PASSED [ 39%]
tests/test_py2dataiku/test_models.py::TestPrepareStep::test_rename_columns PASSED [ 39%]
tests/test_py2dataiku/test_models.py::TestPrepareStep::test_delete_columns PASSED [ 39%]
tests/test_py2dataiku/test_models.py::TestPrepareStep::test_string_transform PASSED [ 40%]
tests/test_py2dataiku/test_models.py::TestPrepareStep::test_to_json PASSED [ 40%]
tests/test_py2dataiku/test_models.py::TestPrepareStep::test_get_description PASSED [ 40%]
tests/test_py2dataiku/test_models.py::TestDataikuRecipe::test_create_prepare PASSED [ 40%]
tests/test_py2dataiku/test_models.py::TestDataikuRecipe::test_create_join PASSED [ 40%]
tests/test_py2dataiku/test_models.py::TestDataikuRecipe::test_create_grouping PASSED [ 40%]
tests/test_py2dataiku/test_models.py::TestDataikuRecipe::test_add_step PASSED [ 40%]
tests/test_py2dataiku/test_models.py::TestDataikuRecipe::test_add_step_wrong_type PASSED [ 40%]
tests/test_py2dataiku/test_models.py::TestDataikuRecipe::test_to_json PASSED [ 41%]
tests/test_py2dataiku/test_models.py::TestDataikuFlow::test_create_flow PASSED [ 41%]
tests/test_py2dataiku/test_models.py::TestDataikuFlow::test_add_dataset PASSED [ 41%]
tests/test_py2dataiku/test_models.py::TestDataikuFlow::test_add_recipe PASSED [ 41%]
tests/test_py2dataiku/test_models.py::TestDataikuFlow::test_get_recipes_by_type PASSED [ 41%]
tests/test_py2dataiku/test_models.py::TestDataikuFlow::test_to_yaml PASSED [ 41%]
tests/test_py2dataiku/test_models.py::TestDataikuFlow::test_validate PASSED [ 41%]
tests/test_py2dataiku/test_models.py::TestDataikuFlow::test_get_summary PASSED [ 41%]
tests/test_py2dataiku/test_models.py::TestTransformation::test_read_csv PASSED [ 41%]
tests/test_py2dataiku/test_models.py::TestTransformation::test_fillna PASSED [ 42%]
tests/test_py2dataiku/test_models.py::TestTransformation::test_merge PASSED [ 42%]
tests/test_py2dataiku/test_models.py::TestTransformation::test_groupby_agg PASSED [ 42%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_pipeline_converts[cleaning-\nimport pandas as pd\n\n# Load raw data\ndf = pd.read_csv('raw_data.csv')\n\n# Remove rows with missing values\ndf = df.dropna()\n\n# Save cleaned data\ndf.to_csv('cleaned_data.csv', index=False)\n] PASSED [ 42%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_pipeline_converts[column_transform-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('customers.csv')\n\n# Clean text columns\ndf['name'] = df['name'].str.strip()\ndf['name'] = df['name'].str.title()\ndf['email'] = df['email'].str.lower()\n\n# Save result\ndf.to_csv('customers_cleaned.csv', index=False)\n] PASSED [ 42%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_pipeline_converts[filtering-\nimport pandas as pd\n\n# Load sales data\nsales = pd.read_csv('sales.csv')\n\n# Filter to high-value transactions\nhigh_value = sales[sales['amount'] > 1000]\n\n# Save filtered data\nhigh_value.to_csv('high_value_sales.csv', index=False)\n] PASSED [ 42%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_pipeline_converts[aggregation-\nimport pandas as pd\n\n# Load transaction data\ntransactions = pd.read_csv('transactions.csv')\n\n# Group by category and sum amounts\nsummary = transactions.groupby('category').agg({\n    'amount': 'sum'\n}).reset_index()\n\n# Save summary\nsummary.to_csv('category_summary.csv', index=False)\n] PASSED [ 42%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_pipeline_converts[sorting-\nimport pandas as pd\n\n# Load products\nproducts = pd.read_csv('products.csv')\n\n# Sort by price descending\nproducts_sorted = products.sort_values('price', ascending=False)\n\n# Save sorted data\nproducts_sorted.to_csv('products_by_price.csv', index=False)\n] PASSED [ 42%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_pipeline_converts[deduplication-\nimport pandas as pd\n\n# Load customer list\ncustomers = pd.read_csv('customer_list.csv')\n\n# Remove duplicate emails\ncustomers_unique = customers.drop_duplicates(subset=['email'])\n\n# Save deduplicated list\ncustomers_unique.to_csv('unique_customers.csv', index=False)\n] PASSED [ 43%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_pipeline_converts[type_conversion-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('data.csv')\n\n# Convert date column\ndf['order_date'] = pd.to_datetime(df['order_date'])\n\n# Convert amount to float\ndf['amount'] = df['amount'].astype(float)\n\n# Save result\ndf.to_csv('data_typed.csv', index=False)\n] PASSED [ 43%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_pipeline_converts[fill_missing-\nimport pandas as pd\n\n# Load survey data\nsurvey = pd.read_csv('survey_responses.csv')\n\n# Fill missing numeric values with 0\nsurvey['score'] = survey['score'].fillna(0)\n\n# Fill missing text with 'Unknown'\nsurvey['category'] = survey['category'].fillna('Unknown')\n\n# Save result\nsurvey.to_csv('survey_complete.csv', index=False)\n] PASSED [ 43%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_pipeline_converts[column_selection-\nimport pandas as pd\n\n# Load full dataset\nfull_data = pd.read_csv('full_dataset.csv')\n\n# Select only needed columns\nselected = full_data[['id', 'name', 'email', 'created_at']]\n\n# Save selected columns\nselected.to_csv('selected_columns.csv', index=False)\n] PASSED [ 43%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_pipeline_converts[top_n-\nimport pandas as pd\n\n# Load sales data\nsales = pd.read_csv('sales.csv')\n\n# Get top 10 highest sales\ntop_10 = sales.nlargest(10, 'amount')\n\n# Save top records\ntop_10.to_csv('top_10_sales.csv', index=False)\n] PASSED [ 43%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_pipeline_has_datasets[cleaning-\nimport pandas as pd\n\n# Load raw data\ndf = pd.read_csv('raw_data.csv')\n\n# Remove rows with missing values\ndf = df.dropna()\n\n# Save cleaned data\ndf.to_csv('cleaned_data.csv', index=False)\n] PASSED [ 43%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_pipeline_has_datasets[column_transform-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('customers.csv')\n\n# Clean text columns\ndf['name'] = df['name'].str.strip()\ndf['name'] = df['name'].str.title()\ndf['email'] = df['email'].str.lower()\n\n# Save result\ndf.to_csv('customers_cleaned.csv', index=False)\n] PASSED [ 43%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_pipeline_has_datasets[filtering-\nimport pandas as pd\n\n# Load sales data\nsales = pd.read_csv('sales.csv')\n\n# Filter to high-value transactions\nhigh_value = sales[sales['amount'] > 1000]\n\n# Save filtered data\nhigh_value.to_csv('high_value_sales.csv', index=False)\n] PASSED [ 43%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_pipeline_has_datasets[aggregation-\nimport pandas as pd\n\n# Load transaction data\ntransactions = pd.read_csv('transactions.csv')\n\n# Group by category and sum amounts\nsummary = transactions.groupby('category').agg({\n    'amount': 'sum'\n}).reset_index()\n\n# Save summary\nsummary.to_csv('category_summary.csv', index=False)\n] PASSED [ 44%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_pipeline_has_datasets[sorting-\nimport pandas as pd\n\n# Load products\nproducts = pd.read_csv('products.csv')\n\n# Sort by price descending\nproducts_sorted = products.sort_values('price', ascending=False)\n\n# Save sorted data\nproducts_sorted.to_csv('products_by_price.csv', index=False)\n] PASSED [ 44%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_pipeline_has_datasets[deduplication-\nimport pandas as pd\n\n# Load customer list\ncustomers = pd.read_csv('customer_list.csv')\n\n# Remove duplicate emails\ncustomers_unique = customers.drop_duplicates(subset=['email'])\n\n# Save deduplicated list\ncustomers_unique.to_csv('unique_customers.csv', index=False)\n] PASSED [ 44%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_pipeline_has_datasets[type_conversion-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('data.csv')\n\n# Convert date column\ndf['order_date'] = pd.to_datetime(df['order_date'])\n\n# Convert amount to float\ndf['amount'] = df['amount'].astype(float)\n\n# Save result\ndf.to_csv('data_typed.csv', index=False)\n] PASSED [ 44%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_pipeline_has_datasets[fill_missing-\nimport pandas as pd\n\n# Load survey data\nsurvey = pd.read_csv('survey_responses.csv')\n\n# Fill missing numeric values with 0\nsurvey['score'] = survey['score'].fillna(0)\n\n# Fill missing text with 'Unknown'\nsurvey['category'] = survey['category'].fillna('Unknown')\n\n# Save result\nsurvey.to_csv('survey_complete.csv', index=False)\n] PASSED [ 44%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_pipeline_has_datasets[column_selection-\nimport pandas as pd\n\n# Load full dataset\nfull_data = pd.read_csv('full_dataset.csv')\n\n# Select only needed columns\nselected = full_data[['id', 'name', 'email', 'created_at']]\n\n# Save selected columns\nselected.to_csv('selected_columns.csv', index=False)\n] PASSED [ 44%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_pipeline_has_datasets[top_n-\nimport pandas as pd\n\n# Load sales data\nsales = pd.read_csv('sales.csv')\n\n# Get top 10 highest sales\ntop_10 = sales.nlargest(10, 'amount')\n\n# Save top records\ntop_10.to_csv('top_10_sales.csv', index=False)\n] PASSED [ 44%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_pipeline_has_recipes[cleaning-\nimport pandas as pd\n\n# Load raw data\ndf = pd.read_csv('raw_data.csv')\n\n# Remove rows with missing values\ndf = df.dropna()\n\n# Save cleaned data\ndf.to_csv('cleaned_data.csv', index=False)\n] PASSED [ 44%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_pipeline_has_recipes[column_transform-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('customers.csv')\n\n# Clean text columns\ndf['name'] = df['name'].str.strip()\ndf['name'] = df['name'].str.title()\ndf['email'] = df['email'].str.lower()\n\n# Save result\ndf.to_csv('customers_cleaned.csv', index=False)\n] PASSED [ 44%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_pipeline_has_recipes[filtering-\nimport pandas as pd\n\n# Load sales data\nsales = pd.read_csv('sales.csv')\n\n# Filter to high-value transactions\nhigh_value = sales[sales['amount'] > 1000]\n\n# Save filtered data\nhigh_value.to_csv('high_value_sales.csv', index=False)\n] PASSED [ 45%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_pipeline_has_recipes[aggregation-\nimport pandas as pd\n\n# Load transaction data\ntransactions = pd.read_csv('transactions.csv')\n\n# Group by category and sum amounts\nsummary = transactions.groupby('category').agg({\n    'amount': 'sum'\n}).reset_index()\n\n# Save summary\nsummary.to_csv('category_summary.csv', index=False)\n] PASSED [ 45%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_pipeline_has_recipes[sorting-\nimport pandas as pd\n\n# Load products\nproducts = pd.read_csv('products.csv')\n\n# Sort by price descending\nproducts_sorted = products.sort_values('price', ascending=False)\n\n# Save sorted data\nproducts_sorted.to_csv('products_by_price.csv', index=False)\n] PASSED [ 45%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_pipeline_has_recipes[deduplication-\nimport pandas as pd\n\n# Load customer list\ncustomers = pd.read_csv('customer_list.csv')\n\n# Remove duplicate emails\ncustomers_unique = customers.drop_duplicates(subset=['email'])\n\n# Save deduplicated list\ncustomers_unique.to_csv('unique_customers.csv', index=False)\n] PASSED [ 45%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_pipeline_has_recipes[type_conversion-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('data.csv')\n\n# Convert date column\ndf['order_date'] = pd.to_datetime(df['order_date'])\n\n# Convert amount to float\ndf['amount'] = df['amount'].astype(float)\n\n# Save result\ndf.to_csv('data_typed.csv', index=False)\n] PASSED [ 45%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_pipeline_has_recipes[fill_missing-\nimport pandas as pd\n\n# Load survey data\nsurvey = pd.read_csv('survey_responses.csv')\n\n# Fill missing numeric values with 0\nsurvey['score'] = survey['score'].fillna(0)\n\n# Fill missing text with 'Unknown'\nsurvey['category'] = survey['category'].fillna('Unknown')\n\n# Save result\nsurvey.to_csv('survey_complete.csv', index=False)\n] PASSED [ 45%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_pipeline_has_recipes[column_selection-\nimport pandas as pd\n\n# Load full dataset\nfull_data = pd.read_csv('full_dataset.csv')\n\n# Select only needed columns\nselected = full_data[['id', 'name', 'email', 'created_at']]\n\n# Save selected columns\nselected.to_csv('selected_columns.csv', index=False)\n] PASSED [ 45%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_pipeline_has_recipes[top_n-\nimport pandas as pd\n\n# Load sales data\nsales = pd.read_csv('sales.csv')\n\n# Get top 10 highest sales\ntop_10 = sales.nlargest(10, 'amount')\n\n# Save top records\ntop_10.to_csv('top_10_sales.csv', index=False)\n] PASSED [ 45%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_cleaning_produces_prepare_recipe PASSED [ 46%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_aggregation_produces_grouping_recipe PASSED [ 46%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_filtering_produces_split_recipe PASSED [ 46%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_sorting_produces_sort_recipe PASSED [ 46%]
tests/test_py2dataiku/test_pipelines.py::TestBasicPipelines::test_basic_deduplication_produces_distinct_recipe PASSED [ 46%]
tests/test_py2dataiku/test_pipelines.py::TestIntermediatePipelines::test_intermediate_pipeline_converts[customer_order_analysis-\nimport pandas as pd\n\n# Load data\ncustomers = pd.read_csv('customers.csv')\norders = pd.read_csv('orders.csv')\n\n# Clean customer data\ncustomers['name'] = customers['name'].str.strip().str.title()\ncustomers = customers.dropna(subset=['customer_id'])\n\n# Join customers with orders\ncustomer_orders = pd.merge(\n    customers,\n    orders,\n    on='customer_id',\n    how='left'\n)\n\n# Calculate order statistics per customer\ncustomer_summary = customer_orders.groupby('customer_id').agg({\n    'order_id': 'count',\n    'amount': 'sum'\n}).reset_index()\n\ncustomer_summary.columns = ['customer_id', 'order_count', 'total_amount']\n\n# Save result\ncustomer_summary.to_csv('customer_order_summary.csv', index=False)\n] PASSED [ 46%]
tests/test_py2dataiku/test_pipelines.py::TestIntermediatePipelines::test_intermediate_pipeline_converts[sales_product_enrichment-\nimport pandas as pd\n\n# Load data\nsales = pd.read_csv('sales.csv')\nproducts = pd.read_csv('products.csv')\ncategories = pd.read_csv('categories.csv')\n\n# Join sales with products\nsales_with_products = pd.merge(\n    sales,\n    products[['product_id', 'product_name', 'category_id', 'unit_price']],\n    on='product_id',\n    how='inner'\n)\n\n# Join with categories\nsales_enriched = pd.merge(\n    sales_with_products,\n    categories[['category_id', 'category_name']],\n    on='category_id',\n    how='left'\n)\n\n# Calculate line total\nsales_enriched['line_total'] = sales_enriched['quantity'] * sales_enriched['unit_price']\n\n# Save enriched data\nsales_enriched.to_csv('sales_enriched.csv', index=False)\n] PASSED [ 46%]
tests/test_py2dataiku/test_pipelines.py::TestIntermediatePipelines::test_intermediate_pipeline_converts[time_based_aggregation-\nimport pandas as pd\n\n# Load transaction data\ntransactions = pd.read_csv('transactions.csv')\n\n# Convert date column\ntransactions['transaction_date'] = pd.to_datetime(transactions['transaction_date'])\n\n# Extract date parts\ntransactions['year'] = transactions['transaction_date'].dt.year\ntransactions['month'] = transactions['transaction_date'].dt.month\ntransactions['day_of_week'] = transactions['transaction_date'].dt.dayofweek\n\n# Aggregate by month\nmonthly_summary = transactions.groupby(['year', 'month']).agg({\n    'amount': ['sum', 'mean', 'count'],\n    'customer_id': 'nunique'\n}).reset_index()\n\nmonthly_summary.columns = ['year', 'month', 'total_amount', 'avg_amount',\n                           'transaction_count', 'unique_customers']\n\n# Save result\nmonthly_summary.to_csv('monthly_summary.csv', index=False)\n] PASSED [ 46%]
tests/test_py2dataiku/test_pipelines.py::TestIntermediatePipelines::test_intermediate_pipeline_converts[customer_segmentation-\nimport pandas as pd\n\n# Load customer transaction data\ncustomers = pd.read_csv('customers.csv')\ntransactions = pd.read_csv('transactions.csv')\n\n# Calculate customer metrics\ncustomer_metrics = transactions.groupby('customer_id').agg({\n    'amount': 'sum',\n    'transaction_id': 'count',\n    'transaction_date': 'max'\n}).reset_index()\n\ncustomer_metrics.columns = ['customer_id', 'total_spend', 'transaction_count', 'last_purchase']\n\n# Join with customer info\ncustomer_data = pd.merge(customers, customer_metrics, on='customer_id', how='left')\n\n# Fill missing values for customers with no transactions\ncustomer_data['total_spend'] = customer_data['total_spend'].fillna(0)\ncustomer_data['transaction_count'] = customer_data['transaction_count'].fillna(0)\n\n# Segment customers by spend\ncustomer_data['segment'] = pd.cut(\n    customer_data['total_spend'],\n    bins=[0, 100, 500, 1000, float('inf')],\n    labels=['Low', 'Medium', 'High', 'VIP']\n)\n\n# Save segmented data\ncustomer_data.to_csv('customer_segments.csv', index=False)\n] PASSED [ 46%]
tests/test_py2dataiku/test_pipelines.py::TestIntermediatePipelines::test_intermediate_pipeline_converts[data_stacking-\nimport pandas as pd\n\n# Load data from multiple sources\nsales_2022 = pd.read_csv('sales_2022.csv')\nsales_2023 = pd.read_csv('sales_2023.csv')\nsales_2024 = pd.read_csv('sales_2024.csv')\n\n# Add year column to each\nsales_2022['data_year'] = 2022\nsales_2023['data_year'] = 2023\nsales_2024['data_year'] = 2024\n\n# Stack all years together\nall_sales = pd.concat([sales_2022, sales_2023, sales_2024], ignore_index=True)\n\n# Remove duplicates\nall_sales = all_sales.drop_duplicates(subset=['transaction_id'])\n\n# Sort by date\nall_sales = all_sales.sort_values('transaction_date')\n\n# Save combined data\nall_sales.to_csv('all_sales_combined.csv', index=False)\n] PASSED [ 47%]
tests/test_py2dataiku/test_pipelines.py::TestIntermediatePipelines::test_intermediate_pipeline_converts[pivot_analysis-\nimport pandas as pd\n\n# Load sales data\nsales = pd.read_csv('sales.csv')\n\n# Create pivot table: products vs months\npivot_table = sales.pivot_table(\n    values='amount',\n    index='product_category',\n    columns='month',\n    aggfunc='sum',\n    fill_value=0\n)\n\n# Reset index for export\npivot_table = pivot_table.reset_index()\n\n# Save pivot table\npivot_table.to_csv('sales_pivot.csv', index=False)\n] PASSED [ 47%]
tests/test_py2dataiku/test_pipelines.py::TestIntermediatePipelines::test_intermediate_pipeline_converts[window_running_total-\nimport pandas as pd\n\n# Load daily sales\ndaily_sales = pd.read_csv('daily_sales.csv')\n\n# Sort by date\ndaily_sales = daily_sales.sort_values('sale_date')\n\n# Calculate running total\ndaily_sales['running_total'] = daily_sales['amount'].cumsum()\n\n# Calculate 7-day moving average\ndaily_sales['moving_avg_7d'] = daily_sales['amount'].rolling(window=7).mean()\n\n# Save with running calculations\ndaily_sales.to_csv('daily_sales_with_running.csv', index=False)\n] PASSED [ 47%]
tests/test_py2dataiku/test_pipelines.py::TestIntermediatePipelines::test_intermediate_pipeline_converts[multi_level_grouping-\nimport pandas as pd\n\n# Load order data\norders = pd.read_csv('orders.csv')\n\n# Group by region and product category\nregional_summary = orders.groupby(['region', 'product_category']).agg({\n    'order_id': 'count',\n    'amount': ['sum', 'mean'],\n    'quantity': 'sum'\n}).reset_index()\n\n# Flatten column names\nregional_summary.columns = [\n    'region', 'product_category', 'order_count',\n    'total_amount', 'avg_amount', 'total_quantity'\n]\n\n# Sort by total amount\nregional_summary = regional_summary.sort_values('total_amount', ascending=False)\n\n# Save summary\nregional_summary.to_csv('regional_product_summary.csv', index=False)\n] PASSED [ 47%]
tests/test_py2dataiku/test_pipelines.py::TestIntermediatePipelines::test_intermediate_pipeline_converts[data_sampling-\nimport pandas as pd\n\n# Load full dataset\nfull_data = pd.read_csv('full_dataset.csv')\n\n# Remove any rows with critical missing values\nfull_data = full_data.dropna(subset=['target_column'])\n\n# Create random sample for validation (10%)\nvalidation_sample = full_data.sample(frac=0.1, random_state=42)\n\n# Get training set (remaining 90%)\ntraining_set = full_data.drop(validation_sample.index)\n\n# Save splits\ntraining_set.to_csv('training_set.csv', index=False)\nvalidation_sample.to_csv('validation_set.csv', index=False)\n] PASSED [ 47%]
tests/test_py2dataiku/test_pipelines.py::TestIntermediatePipelines::test_intermediate_pipeline_converts[feature_engineering-\nimport pandas as pd\nimport numpy as np\n\n# Load user activity data\nactivity = pd.read_csv('user_activity.csv')\n\n# Parse timestamps\nactivity['timestamp'] = pd.to_datetime(activity['timestamp'])\n\n# Create time-based features\nactivity['hour'] = activity['timestamp'].dt.hour\nactivity['day_of_week'] = activity['timestamp'].dt.dayofweek\nactivity['is_weekend'] = activity['day_of_week'].isin([5, 6]).astype(int)\n\n# Create interaction features\nactivity['session_duration_minutes'] = activity['session_duration'] / 60\nactivity['pages_per_minute'] = activity['pages_viewed'] / activity['session_duration_minutes']\n\n# Handle infinite values\nactivity['pages_per_minute'] = activity['pages_per_minute'].replace([np.inf, -np.inf], 0)\n\n# Clip outliers\nactivity['session_duration_minutes'] = activity['session_duration_minutes'].clip(upper=120)\n\n# Save engineered features\nactivity.to_csv('user_activity_features.csv', index=False)\n] PASSED [ 47%]
tests/test_py2dataiku/test_pipelines.py::TestIntermediatePipelines::test_intermediate_pipeline_produces_multiple_recipes[customer_order_analysis-\nimport pandas as pd\n\n# Load data\ncustomers = pd.read_csv('customers.csv')\norders = pd.read_csv('orders.csv')\n\n# Clean customer data\ncustomers['name'] = customers['name'].str.strip().str.title()\ncustomers = customers.dropna(subset=['customer_id'])\n\n# Join customers with orders\ncustomer_orders = pd.merge(\n    customers,\n    orders,\n    on='customer_id',\n    how='left'\n)\n\n# Calculate order statistics per customer\ncustomer_summary = customer_orders.groupby('customer_id').agg({\n    'order_id': 'count',\n    'amount': 'sum'\n}).reset_index()\n\ncustomer_summary.columns = ['customer_id', 'order_count', 'total_amount']\n\n# Save result\ncustomer_summary.to_csv('customer_order_summary.csv', index=False)\n] PASSED [ 47%]
tests/test_py2dataiku/test_pipelines.py::TestIntermediatePipelines::test_intermediate_pipeline_produces_multiple_recipes[sales_product_enrichment-\nimport pandas as pd\n\n# Load data\nsales = pd.read_csv('sales.csv')\nproducts = pd.read_csv('products.csv')\ncategories = pd.read_csv('categories.csv')\n\n# Join sales with products\nsales_with_products = pd.merge(\n    sales,\n    products[['product_id', 'product_name', 'category_id', 'unit_price']],\n    on='product_id',\n    how='inner'\n)\n\n# Join with categories\nsales_enriched = pd.merge(\n    sales_with_products,\n    categories[['category_id', 'category_name']],\n    on='category_id',\n    how='left'\n)\n\n# Calculate line total\nsales_enriched['line_total'] = sales_enriched['quantity'] * sales_enriched['unit_price']\n\n# Save enriched data\nsales_enriched.to_csv('sales_enriched.csv', index=False)\n] PASSED [ 47%]
tests/test_py2dataiku/test_pipelines.py::TestIntermediatePipelines::test_intermediate_pipeline_produces_multiple_recipes[time_based_aggregation-\nimport pandas as pd\n\n# Load transaction data\ntransactions = pd.read_csv('transactions.csv')\n\n# Convert date column\ntransactions['transaction_date'] = pd.to_datetime(transactions['transaction_date'])\n\n# Extract date parts\ntransactions['year'] = transactions['transaction_date'].dt.year\ntransactions['month'] = transactions['transaction_date'].dt.month\ntransactions['day_of_week'] = transactions['transaction_date'].dt.dayofweek\n\n# Aggregate by month\nmonthly_summary = transactions.groupby(['year', 'month']).agg({\n    'amount': ['sum', 'mean', 'count'],\n    'customer_id': 'nunique'\n}).reset_index()\n\nmonthly_summary.columns = ['year', 'month', 'total_amount', 'avg_amount',\n                           'transaction_count', 'unique_customers']\n\n# Save result\nmonthly_summary.to_csv('monthly_summary.csv', index=False)\n] PASSED [ 48%]
tests/test_py2dataiku/test_pipelines.py::TestIntermediatePipelines::test_intermediate_pipeline_produces_multiple_recipes[customer_segmentation-\nimport pandas as pd\n\n# Load customer transaction data\ncustomers = pd.read_csv('customers.csv')\ntransactions = pd.read_csv('transactions.csv')\n\n# Calculate customer metrics\ncustomer_metrics = transactions.groupby('customer_id').agg({\n    'amount': 'sum',\n    'transaction_id': 'count',\n    'transaction_date': 'max'\n}).reset_index()\n\ncustomer_metrics.columns = ['customer_id', 'total_spend', 'transaction_count', 'last_purchase']\n\n# Join with customer info\ncustomer_data = pd.merge(customers, customer_metrics, on='customer_id', how='left')\n\n# Fill missing values for customers with no transactions\ncustomer_data['total_spend'] = customer_data['total_spend'].fillna(0)\ncustomer_data['transaction_count'] = customer_data['transaction_count'].fillna(0)\n\n# Segment customers by spend\ncustomer_data['segment'] = pd.cut(\n    customer_data['total_spend'],\n    bins=[0, 100, 500, 1000, float('inf')],\n    labels=['Low', 'Medium', 'High', 'VIP']\n)\n\n# Save segmented data\ncustomer_data.to_csv('customer_segments.csv', index=False)\n] PASSED [ 48%]
tests/test_py2dataiku/test_pipelines.py::TestIntermediatePipelines::test_intermediate_pipeline_produces_multiple_recipes[data_stacking-\nimport pandas as pd\n\n# Load data from multiple sources\nsales_2022 = pd.read_csv('sales_2022.csv')\nsales_2023 = pd.read_csv('sales_2023.csv')\nsales_2024 = pd.read_csv('sales_2024.csv')\n\n# Add year column to each\nsales_2022['data_year'] = 2022\nsales_2023['data_year'] = 2023\nsales_2024['data_year'] = 2024\n\n# Stack all years together\nall_sales = pd.concat([sales_2022, sales_2023, sales_2024], ignore_index=True)\n\n# Remove duplicates\nall_sales = all_sales.drop_duplicates(subset=['transaction_id'])\n\n# Sort by date\nall_sales = all_sales.sort_values('transaction_date')\n\n# Save combined data\nall_sales.to_csv('all_sales_combined.csv', index=False)\n] PASSED [ 48%]
tests/test_py2dataiku/test_pipelines.py::TestIntermediatePipelines::test_intermediate_pipeline_produces_multiple_recipes[pivot_analysis-\nimport pandas as pd\n\n# Load sales data\nsales = pd.read_csv('sales.csv')\n\n# Create pivot table: products vs months\npivot_table = sales.pivot_table(\n    values='amount',\n    index='product_category',\n    columns='month',\n    aggfunc='sum',\n    fill_value=0\n)\n\n# Reset index for export\npivot_table = pivot_table.reset_index()\n\n# Save pivot table\npivot_table.to_csv('sales_pivot.csv', index=False)\n] PASSED [ 48%]
tests/test_py2dataiku/test_pipelines.py::TestIntermediatePipelines::test_intermediate_pipeline_produces_multiple_recipes[window_running_total-\nimport pandas as pd\n\n# Load daily sales\ndaily_sales = pd.read_csv('daily_sales.csv')\n\n# Sort by date\ndaily_sales = daily_sales.sort_values('sale_date')\n\n# Calculate running total\ndaily_sales['running_total'] = daily_sales['amount'].cumsum()\n\n# Calculate 7-day moving average\ndaily_sales['moving_avg_7d'] = daily_sales['amount'].rolling(window=7).mean()\n\n# Save with running calculations\ndaily_sales.to_csv('daily_sales_with_running.csv', index=False)\n] PASSED [ 48%]
tests/test_py2dataiku/test_pipelines.py::TestIntermediatePipelines::test_intermediate_pipeline_produces_multiple_recipes[multi_level_grouping-\nimport pandas as pd\n\n# Load order data\norders = pd.read_csv('orders.csv')\n\n# Group by region and product category\nregional_summary = orders.groupby(['region', 'product_category']).agg({\n    'order_id': 'count',\n    'amount': ['sum', 'mean'],\n    'quantity': 'sum'\n}).reset_index()\n\n# Flatten column names\nregional_summary.columns = [\n    'region', 'product_category', 'order_count',\n    'total_amount', 'avg_amount', 'total_quantity'\n]\n\n# Sort by total amount\nregional_summary = regional_summary.sort_values('total_amount', ascending=False)\n\n# Save summary\nregional_summary.to_csv('regional_product_summary.csv', index=False)\n] PASSED [ 48%]
tests/test_py2dataiku/test_pipelines.py::TestIntermediatePipelines::test_intermediate_pipeline_produces_multiple_recipes[data_sampling-\nimport pandas as pd\n\n# Load full dataset\nfull_data = pd.read_csv('full_dataset.csv')\n\n# Remove any rows with critical missing values\nfull_data = full_data.dropna(subset=['target_column'])\n\n# Create random sample for validation (10%)\nvalidation_sample = full_data.sample(frac=0.1, random_state=42)\n\n# Get training set (remaining 90%)\ntraining_set = full_data.drop(validation_sample.index)\n\n# Save splits\ntraining_set.to_csv('training_set.csv', index=False)\nvalidation_sample.to_csv('validation_set.csv', index=False)\n] PASSED [ 48%]
tests/test_py2dataiku/test_pipelines.py::TestIntermediatePipelines::test_intermediate_pipeline_produces_multiple_recipes[feature_engineering-\nimport pandas as pd\nimport numpy as np\n\n# Load user activity data\nactivity = pd.read_csv('user_activity.csv')\n\n# Parse timestamps\nactivity['timestamp'] = pd.to_datetime(activity['timestamp'])\n\n# Create time-based features\nactivity['hour'] = activity['timestamp'].dt.hour\nactivity['day_of_week'] = activity['timestamp'].dt.dayofweek\nactivity['is_weekend'] = activity['day_of_week'].isin([5, 6]).astype(int)\n\n# Create interaction features\nactivity['session_duration_minutes'] = activity['session_duration'] / 60\nactivity['pages_per_minute'] = activity['pages_viewed'] / activity['session_duration_minutes']\n\n# Handle infinite values\nactivity['pages_per_minute'] = activity['pages_per_minute'].replace([np.inf, -np.inf], 0)\n\n# Clip outliers\nactivity['session_duration_minutes'] = activity['session_duration_minutes'].clip(upper=120)\n\n# Save engineered features\nactivity.to_csv('user_activity_features.csv', index=False)\n] PASSED [ 48%]
tests/test_py2dataiku/test_pipelines.py::TestIntermediatePipelines::test_customer_order_analysis_has_join PASSED [ 48%]
tests/test_py2dataiku/test_pipelines.py::TestIntermediatePipelines::test_sales_enrichment_has_multiple_joins PASSED [ 49%]
tests/test_py2dataiku/test_pipelines.py::TestIntermediatePipelines::test_data_stacking_has_stack_recipe PASSED [ 49%]
tests/test_py2dataiku/test_pipelines.py::TestIntermediatePipelines::test_pivot_analysis_has_grouping PASSED [ 49%]
tests/test_py2dataiku/test_pipelines.py::TestIntermediatePipelines::test_window_running_total_has_window_recipe PASSED [ 49%]
tests/test_py2dataiku/test_pipelines.py::TestAdvancedPipelines::test_advanced_pipeline_converts[ecommerce_analytics-\nimport pandas as pd\nimport numpy as np\n\n# Load all data sources\ncustomers = pd.read_csv('customers.csv')\norders = pd.read_csv('orders.csv')\norder_items = pd.read_csv('order_items.csv')\nproducts = pd.read_csv('products.csv')\ncategories = pd.read_csv('categories.csv')\n\n# Clean customer data\ncustomers['email'] = customers['email'].str.lower().str.strip()\ncustomers['name'] = customers['name'].str.strip().str.title()\ncustomers = customers.drop_duplicates(subset=['email'])\n\n# Join order items with products\nitems_with_products = pd.merge(\n    order_items,\n    products[['product_id', 'product_name', 'category_id', 'unit_cost']],\n    on='product_id',\n    how='left'\n)\n\n# Add category info\nitems_enriched = pd.merge(\n    items_with_products,\n    categories[['category_id', 'category_name']],\n    on='category_id',\n    how='left'\n)\n\n# Calculate item metrics\nitems_enriched['line_total'] = items_enriched['quantity'] * items_enriched['unit_price']\nitems_enriched['profit'] = items_enriched['line_total'] - (items_enriched['quantity'] * items_enriched['unit_cost'])\n\n# Aggregate to order level\norder_summary = items_enriched.groupby('order_id').agg({\n    'line_total': 'sum',\n    'profit': 'sum',\n    'quantity': 'sum',\n    'product_id': 'nunique'\n}).reset_index()\n\norder_summary.columns = ['order_id', 'order_total', 'order_profit', 'total_items', 'unique_products']\n\n# Join with orders\norders_complete = pd.merge(orders, order_summary, on='order_id', how='left')\n\n# Parse dates\norders_complete['order_date'] = pd.to_datetime(orders_complete['order_date'])\n\n# Join with customers\ncustomer_orders = pd.merge(\n    orders_complete,\n    customers[['customer_id', 'name', 'email', 'signup_date', 'region']],\n    on='customer_id',\n    how='left'\n)\n\n# Calculate customer lifetime metrics\ncustomer_lifetime = customer_orders.groupby('customer_id').agg({\n    'order_id': 'count',\n    'order_total': 'sum',\n    'order_profit': 'sum',\n    'order_date': ['min', 'max']\n}).reset_index()\n\ncustomer_lifetime.columns = [\n    'customer_id', 'total_orders', 'lifetime_value',\n    'lifetime_profit', 'first_order', 'last_order'\n]\n\n# Calculate days since last order\ncustomer_lifetime['days_since_last_order'] = (\n    pd.Timestamp.now() - customer_lifetime['last_order']\n).dt.days\n\n# Segment customers\ncustomer_lifetime['value_segment'] = pd.cut(\n    customer_lifetime['lifetime_value'],\n    bins=[0, 100, 500, 2000, float('inf')],\n    labels=['Bronze', 'Silver', 'Gold', 'Platinum']\n)\n\n# Save all outputs\ncustomer_orders.to_csv('customer_orders_enriched.csv', index=False)\ncustomer_lifetime.to_csv('customer_lifetime_metrics.csv', index=False)\n] PASSED [ 49%]
tests/test_py2dataiku/test_pipelines.py::TestAdvancedPipelines::test_advanced_pipeline_converts[financial_transactions-\nimport pandas as pd\nimport numpy as np\n\n# Load data\ntransactions = pd.read_csv('transactions.csv')\naccounts = pd.read_csv('accounts.csv')\nmerchants = pd.read_csv('merchants.csv')\nfraud_labels = pd.read_csv('fraud_labels.csv')\n\n# Parse transaction timestamps\ntransactions['transaction_time'] = pd.to_datetime(transactions['transaction_time'])\n\n# Clean and validate\ntransactions = transactions.dropna(subset=['account_id', 'amount'])\ntransactions['amount'] = transactions['amount'].abs()  # Ensure positive amounts\n\n# Add time features\ntransactions['hour'] = transactions['transaction_time'].dt.hour\ntransactions['day_of_week'] = transactions['transaction_time'].dt.dayofweek\ntransactions['is_weekend'] = transactions['day_of_week'].isin([5, 6]).astype(int)\ntransactions['is_night'] = transactions['hour'].between(22, 6).astype(int)\n\n# Join with account info\ntransactions_enriched = pd.merge(\n    transactions,\n    accounts[['account_id', 'account_type', 'credit_limit', 'customer_id', 'open_date']],\n    on='account_id',\n    how='left'\n)\n\n# Join with merchant info\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    merchants[['merchant_id', 'merchant_category', 'merchant_country', 'risk_score']],\n    on='merchant_id',\n    how='left'\n)\n\n# Calculate account-level aggregates\naccount_stats = transactions_enriched.groupby('account_id').agg({\n    'amount': ['mean', 'std', 'max', 'count'],\n    'merchant_id': 'nunique'\n}).reset_index()\naccount_stats.columns = ['account_id', 'avg_amount', 'std_amount', 'max_amount',\n                         'transaction_count', 'unique_merchants']\n\n# Join stats back\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    account_stats,\n    on='account_id',\n    how='left'\n)\n\n# Calculate deviation from normal\ntransactions_enriched['amount_zscore'] = (\n    (transactions_enriched['amount'] - transactions_enriched['avg_amount']) /\n    transactions_enriched['std_amount'].replace(0, 1)\n)\n\n# Flag suspicious transactions\ntransactions_enriched['high_amount_flag'] = (transactions_enriched['amount_zscore'] > 3).astype(int)\ntransactions_enriched['new_merchant_flag'] = 0  # Would need historical data\n\n# Join with fraud labels for training\nlabeled_transactions = pd.merge(\n    transactions_enriched,\n    fraud_labels[['transaction_id', 'is_fraud']],\n    on='transaction_id',\n    how='left'\n)\n\n# Fill unlabeled as not fraud\nlabeled_transactions['is_fraud'] = labeled_transactions['is_fraud'].fillna(0)\n\n# Save outputs\nlabeled_transactions.to_csv('transactions_processed.csv', index=False)\naccount_stats.to_csv('account_statistics.csv', index=False)\n] PASSED [ 49%]
tests/test_py2dataiku/test_pipelines.py::TestAdvancedPipelines::test_advanced_pipeline_converts[supply_chain-\nimport pandas as pd\nimport numpy as np\n\n# Load data sources\ninventory = pd.read_csv('inventory.csv')\nsales_history = pd.read_csv('sales_history.csv')\nsuppliers = pd.read_csv('suppliers.csv')\nwarehouses = pd.read_csv('warehouses.csv')\npurchase_orders = pd.read_csv('purchase_orders.csv')\n\n# Clean data\ninventory = inventory.dropna(subset=['sku', 'warehouse_id'])\nsales_history['sale_date'] = pd.to_datetime(sales_history['sale_date'])\n\n# Calculate sales velocity (daily average over last 30 days)\nrecent_sales = sales_history[\n    sales_history['sale_date'] >= (pd.Timestamp.now() - pd.Timedelta(days=30))\n]\n\nsales_velocity = recent_sales.groupby(['sku', 'warehouse_id']).agg({\n    'quantity_sold': 'sum'\n}).reset_index()\nsales_velocity['daily_velocity'] = sales_velocity['quantity_sold'] / 30\n\n# Join inventory with velocity\ninventory_enriched = pd.merge(\n    inventory,\n    sales_velocity[['sku', 'warehouse_id', 'daily_velocity']],\n    on=['sku', 'warehouse_id'],\n    how='left'\n)\ninventory_enriched['daily_velocity'] = inventory_enriched['daily_velocity'].fillna(0)\n\n# Calculate days of supply\ninventory_enriched['days_of_supply'] = np.where(\n    inventory_enriched['daily_velocity'] > 0,\n    inventory_enriched['quantity_on_hand'] / inventory_enriched['daily_velocity'],\n    999  # Infinite supply if no velocity\n)\n\n# Join with warehouse info\ninventory_enriched = pd.merge(\n    inventory_enriched,\n    warehouses[['warehouse_id', 'warehouse_name', 'region', 'capacity']],\n    on='warehouse_id',\n    how='left'\n)\n\n# Join with supplier info\ninventory_enriched = pd.merge(\n    inventory_enriched,\n    suppliers[['supplier_id', 'supplier_name', 'lead_time_days', 'reliability_score']],\n    left_on='primary_supplier_id',\n    right_on='supplier_id',\n    how='left'\n)\n\n# Calculate reorder flags\ninventory_enriched['needs_reorder'] = (\n    inventory_enriched['days_of_supply'] < inventory_enriched['lead_time_days'] * 1.5\n).astype(int)\n\ninventory_enriched['critical_low'] = (\n    inventory_enriched['days_of_supply'] < inventory_enriched['lead_time_days']\n).astype(int)\n\n# Calculate suggested order quantity\ninventory_enriched['suggested_order_qty'] = np.where(\n    inventory_enriched['needs_reorder'] == 1,\n    inventory_enriched['daily_velocity'] * 30 - inventory_enriched['quantity_on_hand'],\n    0\n).clip(lower=0)\n\n# Aggregate by region for reporting\nregional_summary = inventory_enriched.groupby('region').agg({\n    'quantity_on_hand': 'sum',\n    'needs_reorder': 'sum',\n    'critical_low': 'sum',\n    'sku': 'count'\n}).reset_index()\nregional_summary.columns = ['region', 'total_inventory', 'items_need_reorder',\n                           'critical_items', 'total_skus']\n\n# Save outputs\ninventory_enriched.to_csv('inventory_analysis.csv', index=False)\nregional_summary.to_csv('regional_inventory_summary.csv', index=False)\n] PASSED [ 49%]
tests/test_py2dataiku/test_pipelines.py::TestAdvancedPipelines::test_advanced_pipeline_converts[marketing_campaign-\nimport pandas as pd\nimport numpy as np\n\n# Load campaign data\ncampaigns = pd.read_csv('campaigns.csv')\nimpressions = pd.read_csv('ad_impressions.csv')\nclicks = pd.read_csv('ad_clicks.csv')\nconversions = pd.read_csv('conversions.csv')\ncustomers = pd.read_csv('customers.csv')\norders = pd.read_csv('orders.csv')\n\n# Parse dates\nimpressions['impression_time'] = pd.to_datetime(impressions['impression_time'])\nclicks['click_time'] = pd.to_datetime(clicks['click_time'])\nconversions['conversion_time'] = pd.to_datetime(conversions['conversion_time'])\n\n# Aggregate impressions by campaign\ncampaign_impressions = impressions.groupby('campaign_id').agg({\n    'impression_id': 'count',\n    'user_id': 'nunique'\n}).reset_index()\ncampaign_impressions.columns = ['campaign_id', 'total_impressions', 'unique_reach']\n\n# Aggregate clicks by campaign\ncampaign_clicks = clicks.groupby('campaign_id').agg({\n    'click_id': 'count',\n    'user_id': 'nunique'\n}).reset_index()\ncampaign_clicks.columns = ['campaign_id', 'total_clicks', 'unique_clickers']\n\n# Aggregate conversions\ncampaign_conversions = conversions.groupby('campaign_id').agg({\n    'conversion_id': 'count',\n    'conversion_value': 'sum',\n    'user_id': 'nunique'\n}).reset_index()\ncampaign_conversions.columns = ['campaign_id', 'total_conversions',\n                                'total_revenue', 'unique_converters']\n\n# Combine metrics\ncampaign_metrics = pd.merge(campaigns, campaign_impressions, on='campaign_id', how='left')\ncampaign_metrics = pd.merge(campaign_metrics, campaign_clicks, on='campaign_id', how='left')\ncampaign_metrics = pd.merge(campaign_metrics, campaign_conversions, on='campaign_id', how='left')\n\n# Fill missing values\nfor col in ['total_impressions', 'unique_reach', 'total_clicks', 'unique_clickers',\n            'total_conversions', 'total_revenue', 'unique_converters']:\n    campaign_metrics[col] = campaign_metrics[col].fillna(0)\n\n# Calculate rates\ncampaign_metrics['ctr'] = (\n    campaign_metrics['total_clicks'] / campaign_metrics['total_impressions'].replace(0, 1)\n) * 100\n\ncampaign_metrics['conversion_rate'] = (\n    campaign_metrics['total_conversions'] / campaign_metrics['total_clicks'].replace(0, 1)\n) * 100\n\ncampaign_metrics['cost_per_click'] = (\n    campaign_metrics['budget_spent'] / campaign_metrics['total_clicks'].replace(0, 1)\n)\n\ncampaign_metrics['cost_per_conversion'] = (\n    campaign_metrics['budget_spent'] / campaign_metrics['total_conversions'].replace(0, 1)\n)\n\ncampaign_metrics['roas'] = (\n    campaign_metrics['total_revenue'] / campaign_metrics['budget_spent'].replace(0, 1)\n)\n\n# Segment by performance\ncampaign_metrics['performance_tier'] = pd.cut(\n    campaign_metrics['roas'],\n    bins=[-float('inf'), 1, 2, 4, float('inf')],\n    labels=['Poor', 'Break-even', 'Good', 'Excellent']\n)\n\n# Save results\ncampaign_metrics.to_csv('campaign_performance.csv', index=False)\n] PASSED [ 49%]
tests/test_py2dataiku/test_pipelines.py::TestAdvancedPipelines::test_advanced_pipeline_converts[healthcare_patient-\nimport pandas as pd\nimport numpy as np\n\n# Load healthcare data\npatients = pd.read_csv('patients.csv')\nencounters = pd.read_csv('encounters.csv')\ndiagnoses = pd.read_csv('diagnoses.csv')\nprocedures = pd.read_csv('procedures.csv')\nprescriptions = pd.read_csv('prescriptions.csv')\nlab_results = pd.read_csv('lab_results.csv')\n\n# Parse dates\nencounters['encounter_date'] = pd.to_datetime(encounters['encounter_date'])\npatients['birth_date'] = pd.to_datetime(patients['birth_date'])\n\n# Calculate patient age\npatients['age'] = (pd.Timestamp.now() - patients['birth_date']).dt.days // 365\n\n# Age groups\npatients['age_group'] = pd.cut(\n    patients['age'],\n    bins=[0, 18, 35, 50, 65, 100],\n    labels=['Pediatric', 'Young Adult', 'Adult', 'Middle Age', 'Senior']\n)\n\n# Join diagnoses to encounters\nencounters_with_dx = pd.merge(\n    encounters,\n    diagnoses[['encounter_id', 'diagnosis_code', 'diagnosis_description', 'is_primary']],\n    on='encounter_id',\n    how='left'\n)\n\n# Get primary diagnosis per encounter\nprimary_dx = encounters_with_dx[encounters_with_dx['is_primary'] == 1].copy()\n\n# Aggregate encounters per patient\npatient_encounter_summary = encounters.groupby('patient_id').agg({\n    'encounter_id': 'count',\n    'encounter_date': ['min', 'max'],\n    'total_charges': 'sum'\n}).reset_index()\npatient_encounter_summary.columns = [\n    'patient_id', 'total_encounters', 'first_encounter',\n    'last_encounter', 'total_charges'\n]\n\n# Count diagnoses per patient\npatient_dx_count = diagnoses.groupby(\n    diagnoses.merge(encounters[['encounter_id', 'patient_id']], on='encounter_id')['patient_id']\n).agg({\n    'diagnosis_code': 'nunique'\n}).reset_index()\npatient_dx_count.columns = ['patient_id', 'unique_diagnoses']\n\n# Count procedures per patient\npatient_proc_count = procedures.merge(\n    encounters[['encounter_id', 'patient_id']], on='encounter_id'\n).groupby('patient_id').agg({\n    'procedure_code': 'count'\n}).reset_index()\npatient_proc_count.columns = ['patient_id', 'total_procedures']\n\n# Combine patient profile\npatient_profile = pd.merge(patients, patient_encounter_summary, on='patient_id', how='left')\npatient_profile = pd.merge(patient_profile, patient_dx_count, on='patient_id', how='left')\npatient_profile = pd.merge(patient_profile, patient_proc_count, on='patient_id', how='left')\n\n# Fill missing values\npatient_profile['total_encounters'] = patient_profile['total_encounters'].fillna(0)\npatient_profile['unique_diagnoses'] = patient_profile['unique_diagnoses'].fillna(0)\npatient_profile['total_procedures'] = patient_profile['total_procedures'].fillna(0)\npatient_profile['total_charges'] = patient_profile['total_charges'].fillna(0)\n\n# Calculate risk score (simplified)\npatient_profile['complexity_score'] = (\n    patient_profile['unique_diagnoses'] * 0.3 +\n    patient_profile['total_procedures'] * 0.2 +\n    patient_profile['total_encounters'] * 0.1 +\n    (patient_profile['age'] / 10) * 0.4\n).round(2)\n\n# Identify high-risk patients\npatient_profile['high_risk'] = (patient_profile['complexity_score'] > 5).astype(int)\n\n# Save outputs\npatient_profile.to_csv('patient_profiles.csv', index=False)\n] PASSED [ 50%]
tests/test_py2dataiku/test_pipelines.py::TestAdvancedPipelines::test_advanced_pipeline_has_datasets[ecommerce_analytics-\nimport pandas as pd\nimport numpy as np\n\n# Load all data sources\ncustomers = pd.read_csv('customers.csv')\norders = pd.read_csv('orders.csv')\norder_items = pd.read_csv('order_items.csv')\nproducts = pd.read_csv('products.csv')\ncategories = pd.read_csv('categories.csv')\n\n# Clean customer data\ncustomers['email'] = customers['email'].str.lower().str.strip()\ncustomers['name'] = customers['name'].str.strip().str.title()\ncustomers = customers.drop_duplicates(subset=['email'])\n\n# Join order items with products\nitems_with_products = pd.merge(\n    order_items,\n    products[['product_id', 'product_name', 'category_id', 'unit_cost']],\n    on='product_id',\n    how='left'\n)\n\n# Add category info\nitems_enriched = pd.merge(\n    items_with_products,\n    categories[['category_id', 'category_name']],\n    on='category_id',\n    how='left'\n)\n\n# Calculate item metrics\nitems_enriched['line_total'] = items_enriched['quantity'] * items_enriched['unit_price']\nitems_enriched['profit'] = items_enriched['line_total'] - (items_enriched['quantity'] * items_enriched['unit_cost'])\n\n# Aggregate to order level\norder_summary = items_enriched.groupby('order_id').agg({\n    'line_total': 'sum',\n    'profit': 'sum',\n    'quantity': 'sum',\n    'product_id': 'nunique'\n}).reset_index()\n\norder_summary.columns = ['order_id', 'order_total', 'order_profit', 'total_items', 'unique_products']\n\n# Join with orders\norders_complete = pd.merge(orders, order_summary, on='order_id', how='left')\n\n# Parse dates\norders_complete['order_date'] = pd.to_datetime(orders_complete['order_date'])\n\n# Join with customers\ncustomer_orders = pd.merge(\n    orders_complete,\n    customers[['customer_id', 'name', 'email', 'signup_date', 'region']],\n    on='customer_id',\n    how='left'\n)\n\n# Calculate customer lifetime metrics\ncustomer_lifetime = customer_orders.groupby('customer_id').agg({\n    'order_id': 'count',\n    'order_total': 'sum',\n    'order_profit': 'sum',\n    'order_date': ['min', 'max']\n}).reset_index()\n\ncustomer_lifetime.columns = [\n    'customer_id', 'total_orders', 'lifetime_value',\n    'lifetime_profit', 'first_order', 'last_order'\n]\n\n# Calculate days since last order\ncustomer_lifetime['days_since_last_order'] = (\n    pd.Timestamp.now() - customer_lifetime['last_order']\n).dt.days\n\n# Segment customers\ncustomer_lifetime['value_segment'] = pd.cut(\n    customer_lifetime['lifetime_value'],\n    bins=[0, 100, 500, 2000, float('inf')],\n    labels=['Bronze', 'Silver', 'Gold', 'Platinum']\n)\n\n# Save all outputs\ncustomer_orders.to_csv('customer_orders_enriched.csv', index=False)\ncustomer_lifetime.to_csv('customer_lifetime_metrics.csv', index=False)\n] PASSED [ 50%]
tests/test_py2dataiku/test_pipelines.py::TestAdvancedPipelines::test_advanced_pipeline_has_datasets[financial_transactions-\nimport pandas as pd\nimport numpy as np\n\n# Load data\ntransactions = pd.read_csv('transactions.csv')\naccounts = pd.read_csv('accounts.csv')\nmerchants = pd.read_csv('merchants.csv')\nfraud_labels = pd.read_csv('fraud_labels.csv')\n\n# Parse transaction timestamps\ntransactions['transaction_time'] = pd.to_datetime(transactions['transaction_time'])\n\n# Clean and validate\ntransactions = transactions.dropna(subset=['account_id', 'amount'])\ntransactions['amount'] = transactions['amount'].abs()  # Ensure positive amounts\n\n# Add time features\ntransactions['hour'] = transactions['transaction_time'].dt.hour\ntransactions['day_of_week'] = transactions['transaction_time'].dt.dayofweek\ntransactions['is_weekend'] = transactions['day_of_week'].isin([5, 6]).astype(int)\ntransactions['is_night'] = transactions['hour'].between(22, 6).astype(int)\n\n# Join with account info\ntransactions_enriched = pd.merge(\n    transactions,\n    accounts[['account_id', 'account_type', 'credit_limit', 'customer_id', 'open_date']],\n    on='account_id',\n    how='left'\n)\n\n# Join with merchant info\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    merchants[['merchant_id', 'merchant_category', 'merchant_country', 'risk_score']],\n    on='merchant_id',\n    how='left'\n)\n\n# Calculate account-level aggregates\naccount_stats = transactions_enriched.groupby('account_id').agg({\n    'amount': ['mean', 'std', 'max', 'count'],\n    'merchant_id': 'nunique'\n}).reset_index()\naccount_stats.columns = ['account_id', 'avg_amount', 'std_amount', 'max_amount',\n                         'transaction_count', 'unique_merchants']\n\n# Join stats back\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    account_stats,\n    on='account_id',\n    how='left'\n)\n\n# Calculate deviation from normal\ntransactions_enriched['amount_zscore'] = (\n    (transactions_enriched['amount'] - transactions_enriched['avg_amount']) /\n    transactions_enriched['std_amount'].replace(0, 1)\n)\n\n# Flag suspicious transactions\ntransactions_enriched['high_amount_flag'] = (transactions_enriched['amount_zscore'] > 3).astype(int)\ntransactions_enriched['new_merchant_flag'] = 0  # Would need historical data\n\n# Join with fraud labels for training\nlabeled_transactions = pd.merge(\n    transactions_enriched,\n    fraud_labels[['transaction_id', 'is_fraud']],\n    on='transaction_id',\n    how='left'\n)\n\n# Fill unlabeled as not fraud\nlabeled_transactions['is_fraud'] = labeled_transactions['is_fraud'].fillna(0)\n\n# Save outputs\nlabeled_transactions.to_csv('transactions_processed.csv', index=False)\naccount_stats.to_csv('account_statistics.csv', index=False)\n] PASSED [ 50%]
tests/test_py2dataiku/test_pipelines.py::TestAdvancedPipelines::test_advanced_pipeline_has_datasets[supply_chain-\nimport pandas as pd\nimport numpy as np\n\n# Load data sources\ninventory = pd.read_csv('inventory.csv')\nsales_history = pd.read_csv('sales_history.csv')\nsuppliers = pd.read_csv('suppliers.csv')\nwarehouses = pd.read_csv('warehouses.csv')\npurchase_orders = pd.read_csv('purchase_orders.csv')\n\n# Clean data\ninventory = inventory.dropna(subset=['sku', 'warehouse_id'])\nsales_history['sale_date'] = pd.to_datetime(sales_history['sale_date'])\n\n# Calculate sales velocity (daily average over last 30 days)\nrecent_sales = sales_history[\n    sales_history['sale_date'] >= (pd.Timestamp.now() - pd.Timedelta(days=30))\n]\n\nsales_velocity = recent_sales.groupby(['sku', 'warehouse_id']).agg({\n    'quantity_sold': 'sum'\n}).reset_index()\nsales_velocity['daily_velocity'] = sales_velocity['quantity_sold'] / 30\n\n# Join inventory with velocity\ninventory_enriched = pd.merge(\n    inventory,\n    sales_velocity[['sku', 'warehouse_id', 'daily_velocity']],\n    on=['sku', 'warehouse_id'],\n    how='left'\n)\ninventory_enriched['daily_velocity'] = inventory_enriched['daily_velocity'].fillna(0)\n\n# Calculate days of supply\ninventory_enriched['days_of_supply'] = np.where(\n    inventory_enriched['daily_velocity'] > 0,\n    inventory_enriched['quantity_on_hand'] / inventory_enriched['daily_velocity'],\n    999  # Infinite supply if no velocity\n)\n\n# Join with warehouse info\ninventory_enriched = pd.merge(\n    inventory_enriched,\n    warehouses[['warehouse_id', 'warehouse_name', 'region', 'capacity']],\n    on='warehouse_id',\n    how='left'\n)\n\n# Join with supplier info\ninventory_enriched = pd.merge(\n    inventory_enriched,\n    suppliers[['supplier_id', 'supplier_name', 'lead_time_days', 'reliability_score']],\n    left_on='primary_supplier_id',\n    right_on='supplier_id',\n    how='left'\n)\n\n# Calculate reorder flags\ninventory_enriched['needs_reorder'] = (\n    inventory_enriched['days_of_supply'] < inventory_enriched['lead_time_days'] * 1.5\n).astype(int)\n\ninventory_enriched['critical_low'] = (\n    inventory_enriched['days_of_supply'] < inventory_enriched['lead_time_days']\n).astype(int)\n\n# Calculate suggested order quantity\ninventory_enriched['suggested_order_qty'] = np.where(\n    inventory_enriched['needs_reorder'] == 1,\n    inventory_enriched['daily_velocity'] * 30 - inventory_enriched['quantity_on_hand'],\n    0\n).clip(lower=0)\n\n# Aggregate by region for reporting\nregional_summary = inventory_enriched.groupby('region').agg({\n    'quantity_on_hand': 'sum',\n    'needs_reorder': 'sum',\n    'critical_low': 'sum',\n    'sku': 'count'\n}).reset_index()\nregional_summary.columns = ['region', 'total_inventory', 'items_need_reorder',\n                           'critical_items', 'total_skus']\n\n# Save outputs\ninventory_enriched.to_csv('inventory_analysis.csv', index=False)\nregional_summary.to_csv('regional_inventory_summary.csv', index=False)\n] PASSED [ 50%]
tests/test_py2dataiku/test_pipelines.py::TestAdvancedPipelines::test_advanced_pipeline_has_datasets[marketing_campaign-\nimport pandas as pd\nimport numpy as np\n\n# Load campaign data\ncampaigns = pd.read_csv('campaigns.csv')\nimpressions = pd.read_csv('ad_impressions.csv')\nclicks = pd.read_csv('ad_clicks.csv')\nconversions = pd.read_csv('conversions.csv')\ncustomers = pd.read_csv('customers.csv')\norders = pd.read_csv('orders.csv')\n\n# Parse dates\nimpressions['impression_time'] = pd.to_datetime(impressions['impression_time'])\nclicks['click_time'] = pd.to_datetime(clicks['click_time'])\nconversions['conversion_time'] = pd.to_datetime(conversions['conversion_time'])\n\n# Aggregate impressions by campaign\ncampaign_impressions = impressions.groupby('campaign_id').agg({\n    'impression_id': 'count',\n    'user_id': 'nunique'\n}).reset_index()\ncampaign_impressions.columns = ['campaign_id', 'total_impressions', 'unique_reach']\n\n# Aggregate clicks by campaign\ncampaign_clicks = clicks.groupby('campaign_id').agg({\n    'click_id': 'count',\n    'user_id': 'nunique'\n}).reset_index()\ncampaign_clicks.columns = ['campaign_id', 'total_clicks', 'unique_clickers']\n\n# Aggregate conversions\ncampaign_conversions = conversions.groupby('campaign_id').agg({\n    'conversion_id': 'count',\n    'conversion_value': 'sum',\n    'user_id': 'nunique'\n}).reset_index()\ncampaign_conversions.columns = ['campaign_id', 'total_conversions',\n                                'total_revenue', 'unique_converters']\n\n# Combine metrics\ncampaign_metrics = pd.merge(campaigns, campaign_impressions, on='campaign_id', how='left')\ncampaign_metrics = pd.merge(campaign_metrics, campaign_clicks, on='campaign_id', how='left')\ncampaign_metrics = pd.merge(campaign_metrics, campaign_conversions, on='campaign_id', how='left')\n\n# Fill missing values\nfor col in ['total_impressions', 'unique_reach', 'total_clicks', 'unique_clickers',\n            'total_conversions', 'total_revenue', 'unique_converters']:\n    campaign_metrics[col] = campaign_metrics[col].fillna(0)\n\n# Calculate rates\ncampaign_metrics['ctr'] = (\n    campaign_metrics['total_clicks'] / campaign_metrics['total_impressions'].replace(0, 1)\n) * 100\n\ncampaign_metrics['conversion_rate'] = (\n    campaign_metrics['total_conversions'] / campaign_metrics['total_clicks'].replace(0, 1)\n) * 100\n\ncampaign_metrics['cost_per_click'] = (\n    campaign_metrics['budget_spent'] / campaign_metrics['total_clicks'].replace(0, 1)\n)\n\ncampaign_metrics['cost_per_conversion'] = (\n    campaign_metrics['budget_spent'] / campaign_metrics['total_conversions'].replace(0, 1)\n)\n\ncampaign_metrics['roas'] = (\n    campaign_metrics['total_revenue'] / campaign_metrics['budget_spent'].replace(0, 1)\n)\n\n# Segment by performance\ncampaign_metrics['performance_tier'] = pd.cut(\n    campaign_metrics['roas'],\n    bins=[-float('inf'), 1, 2, 4, float('inf')],\n    labels=['Poor', 'Break-even', 'Good', 'Excellent']\n)\n\n# Save results\ncampaign_metrics.to_csv('campaign_performance.csv', index=False)\n] PASSED [ 50%]
tests/test_py2dataiku/test_pipelines.py::TestAdvancedPipelines::test_advanced_pipeline_has_datasets[healthcare_patient-\nimport pandas as pd\nimport numpy as np\n\n# Load healthcare data\npatients = pd.read_csv('patients.csv')\nencounters = pd.read_csv('encounters.csv')\ndiagnoses = pd.read_csv('diagnoses.csv')\nprocedures = pd.read_csv('procedures.csv')\nprescriptions = pd.read_csv('prescriptions.csv')\nlab_results = pd.read_csv('lab_results.csv')\n\n# Parse dates\nencounters['encounter_date'] = pd.to_datetime(encounters['encounter_date'])\npatients['birth_date'] = pd.to_datetime(patients['birth_date'])\n\n# Calculate patient age\npatients['age'] = (pd.Timestamp.now() - patients['birth_date']).dt.days // 365\n\n# Age groups\npatients['age_group'] = pd.cut(\n    patients['age'],\n    bins=[0, 18, 35, 50, 65, 100],\n    labels=['Pediatric', 'Young Adult', 'Adult', 'Middle Age', 'Senior']\n)\n\n# Join diagnoses to encounters\nencounters_with_dx = pd.merge(\n    encounters,\n    diagnoses[['encounter_id', 'diagnosis_code', 'diagnosis_description', 'is_primary']],\n    on='encounter_id',\n    how='left'\n)\n\n# Get primary diagnosis per encounter\nprimary_dx = encounters_with_dx[encounters_with_dx['is_primary'] == 1].copy()\n\n# Aggregate encounters per patient\npatient_encounter_summary = encounters.groupby('patient_id').agg({\n    'encounter_id': 'count',\n    'encounter_date': ['min', 'max'],\n    'total_charges': 'sum'\n}).reset_index()\npatient_encounter_summary.columns = [\n    'patient_id', 'total_encounters', 'first_encounter',\n    'last_encounter', 'total_charges'\n]\n\n# Count diagnoses per patient\npatient_dx_count = diagnoses.groupby(\n    diagnoses.merge(encounters[['encounter_id', 'patient_id']], on='encounter_id')['patient_id']\n).agg({\n    'diagnosis_code': 'nunique'\n}).reset_index()\npatient_dx_count.columns = ['patient_id', 'unique_diagnoses']\n\n# Count procedures per patient\npatient_proc_count = procedures.merge(\n    encounters[['encounter_id', 'patient_id']], on='encounter_id'\n).groupby('patient_id').agg({\n    'procedure_code': 'count'\n}).reset_index()\npatient_proc_count.columns = ['patient_id', 'total_procedures']\n\n# Combine patient profile\npatient_profile = pd.merge(patients, patient_encounter_summary, on='patient_id', how='left')\npatient_profile = pd.merge(patient_profile, patient_dx_count, on='patient_id', how='left')\npatient_profile = pd.merge(patient_profile, patient_proc_count, on='patient_id', how='left')\n\n# Fill missing values\npatient_profile['total_encounters'] = patient_profile['total_encounters'].fillna(0)\npatient_profile['unique_diagnoses'] = patient_profile['unique_diagnoses'].fillna(0)\npatient_profile['total_procedures'] = patient_profile['total_procedures'].fillna(0)\npatient_profile['total_charges'] = patient_profile['total_charges'].fillna(0)\n\n# Calculate risk score (simplified)\npatient_profile['complexity_score'] = (\n    patient_profile['unique_diagnoses'] * 0.3 +\n    patient_profile['total_procedures'] * 0.2 +\n    patient_profile['total_encounters'] * 0.1 +\n    (patient_profile['age'] / 10) * 0.4\n).round(2)\n\n# Identify high-risk patients\npatient_profile['high_risk'] = (patient_profile['complexity_score'] > 5).astype(int)\n\n# Save outputs\npatient_profile.to_csv('patient_profiles.csv', index=False)\n] PASSED [ 50%]
tests/test_py2dataiku/test_pipelines.py::TestAdvancedPipelines::test_advanced_pipeline_has_recipes[ecommerce_analytics-\nimport pandas as pd\nimport numpy as np\n\n# Load all data sources\ncustomers = pd.read_csv('customers.csv')\norders = pd.read_csv('orders.csv')\norder_items = pd.read_csv('order_items.csv')\nproducts = pd.read_csv('products.csv')\ncategories = pd.read_csv('categories.csv')\n\n# Clean customer data\ncustomers['email'] = customers['email'].str.lower().str.strip()\ncustomers['name'] = customers['name'].str.strip().str.title()\ncustomers = customers.drop_duplicates(subset=['email'])\n\n# Join order items with products\nitems_with_products = pd.merge(\n    order_items,\n    products[['product_id', 'product_name', 'category_id', 'unit_cost']],\n    on='product_id',\n    how='left'\n)\n\n# Add category info\nitems_enriched = pd.merge(\n    items_with_products,\n    categories[['category_id', 'category_name']],\n    on='category_id',\n    how='left'\n)\n\n# Calculate item metrics\nitems_enriched['line_total'] = items_enriched['quantity'] * items_enriched['unit_price']\nitems_enriched['profit'] = items_enriched['line_total'] - (items_enriched['quantity'] * items_enriched['unit_cost'])\n\n# Aggregate to order level\norder_summary = items_enriched.groupby('order_id').agg({\n    'line_total': 'sum',\n    'profit': 'sum',\n    'quantity': 'sum',\n    'product_id': 'nunique'\n}).reset_index()\n\norder_summary.columns = ['order_id', 'order_total', 'order_profit', 'total_items', 'unique_products']\n\n# Join with orders\norders_complete = pd.merge(orders, order_summary, on='order_id', how='left')\n\n# Parse dates\norders_complete['order_date'] = pd.to_datetime(orders_complete['order_date'])\n\n# Join with customers\ncustomer_orders = pd.merge(\n    orders_complete,\n    customers[['customer_id', 'name', 'email', 'signup_date', 'region']],\n    on='customer_id',\n    how='left'\n)\n\n# Calculate customer lifetime metrics\ncustomer_lifetime = customer_orders.groupby('customer_id').agg({\n    'order_id': 'count',\n    'order_total': 'sum',\n    'order_profit': 'sum',\n    'order_date': ['min', 'max']\n}).reset_index()\n\ncustomer_lifetime.columns = [\n    'customer_id', 'total_orders', 'lifetime_value',\n    'lifetime_profit', 'first_order', 'last_order'\n]\n\n# Calculate days since last order\ncustomer_lifetime['days_since_last_order'] = (\n    pd.Timestamp.now() - customer_lifetime['last_order']\n).dt.days\n\n# Segment customers\ncustomer_lifetime['value_segment'] = pd.cut(\n    customer_lifetime['lifetime_value'],\n    bins=[0, 100, 500, 2000, float('inf')],\n    labels=['Bronze', 'Silver', 'Gold', 'Platinum']\n)\n\n# Save all outputs\ncustomer_orders.to_csv('customer_orders_enriched.csv', index=False)\ncustomer_lifetime.to_csv('customer_lifetime_metrics.csv', index=False)\n] PASSED [ 50%]
tests/test_py2dataiku/test_pipelines.py::TestAdvancedPipelines::test_advanced_pipeline_has_recipes[financial_transactions-\nimport pandas as pd\nimport numpy as np\n\n# Load data\ntransactions = pd.read_csv('transactions.csv')\naccounts = pd.read_csv('accounts.csv')\nmerchants = pd.read_csv('merchants.csv')\nfraud_labels = pd.read_csv('fraud_labels.csv')\n\n# Parse transaction timestamps\ntransactions['transaction_time'] = pd.to_datetime(transactions['transaction_time'])\n\n# Clean and validate\ntransactions = transactions.dropna(subset=['account_id', 'amount'])\ntransactions['amount'] = transactions['amount'].abs()  # Ensure positive amounts\n\n# Add time features\ntransactions['hour'] = transactions['transaction_time'].dt.hour\ntransactions['day_of_week'] = transactions['transaction_time'].dt.dayofweek\ntransactions['is_weekend'] = transactions['day_of_week'].isin([5, 6]).astype(int)\ntransactions['is_night'] = transactions['hour'].between(22, 6).astype(int)\n\n# Join with account info\ntransactions_enriched = pd.merge(\n    transactions,\n    accounts[['account_id', 'account_type', 'credit_limit', 'customer_id', 'open_date']],\n    on='account_id',\n    how='left'\n)\n\n# Join with merchant info\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    merchants[['merchant_id', 'merchant_category', 'merchant_country', 'risk_score']],\n    on='merchant_id',\n    how='left'\n)\n\n# Calculate account-level aggregates\naccount_stats = transactions_enriched.groupby('account_id').agg({\n    'amount': ['mean', 'std', 'max', 'count'],\n    'merchant_id': 'nunique'\n}).reset_index()\naccount_stats.columns = ['account_id', 'avg_amount', 'std_amount', 'max_amount',\n                         'transaction_count', 'unique_merchants']\n\n# Join stats back\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    account_stats,\n    on='account_id',\n    how='left'\n)\n\n# Calculate deviation from normal\ntransactions_enriched['amount_zscore'] = (\n    (transactions_enriched['amount'] - transactions_enriched['avg_amount']) /\n    transactions_enriched['std_amount'].replace(0, 1)\n)\n\n# Flag suspicious transactions\ntransactions_enriched['high_amount_flag'] = (transactions_enriched['amount_zscore'] > 3).astype(int)\ntransactions_enriched['new_merchant_flag'] = 0  # Would need historical data\n\n# Join with fraud labels for training\nlabeled_transactions = pd.merge(\n    transactions_enriched,\n    fraud_labels[['transaction_id', 'is_fraud']],\n    on='transaction_id',\n    how='left'\n)\n\n# Fill unlabeled as not fraud\nlabeled_transactions['is_fraud'] = labeled_transactions['is_fraud'].fillna(0)\n\n# Save outputs\nlabeled_transactions.to_csv('transactions_processed.csv', index=False)\naccount_stats.to_csv('account_statistics.csv', index=False)\n] PASSED [ 50%]
tests/test_py2dataiku/test_pipelines.py::TestAdvancedPipelines::test_advanced_pipeline_has_recipes[supply_chain-\nimport pandas as pd\nimport numpy as np\n\n# Load data sources\ninventory = pd.read_csv('inventory.csv')\nsales_history = pd.read_csv('sales_history.csv')\nsuppliers = pd.read_csv('suppliers.csv')\nwarehouses = pd.read_csv('warehouses.csv')\npurchase_orders = pd.read_csv('purchase_orders.csv')\n\n# Clean data\ninventory = inventory.dropna(subset=['sku', 'warehouse_id'])\nsales_history['sale_date'] = pd.to_datetime(sales_history['sale_date'])\n\n# Calculate sales velocity (daily average over last 30 days)\nrecent_sales = sales_history[\n    sales_history['sale_date'] >= (pd.Timestamp.now() - pd.Timedelta(days=30))\n]\n\nsales_velocity = recent_sales.groupby(['sku', 'warehouse_id']).agg({\n    'quantity_sold': 'sum'\n}).reset_index()\nsales_velocity['daily_velocity'] = sales_velocity['quantity_sold'] / 30\n\n# Join inventory with velocity\ninventory_enriched = pd.merge(\n    inventory,\n    sales_velocity[['sku', 'warehouse_id', 'daily_velocity']],\n    on=['sku', 'warehouse_id'],\n    how='left'\n)\ninventory_enriched['daily_velocity'] = inventory_enriched['daily_velocity'].fillna(0)\n\n# Calculate days of supply\ninventory_enriched['days_of_supply'] = np.where(\n    inventory_enriched['daily_velocity'] > 0,\n    inventory_enriched['quantity_on_hand'] / inventory_enriched['daily_velocity'],\n    999  # Infinite supply if no velocity\n)\n\n# Join with warehouse info\ninventory_enriched = pd.merge(\n    inventory_enriched,\n    warehouses[['warehouse_id', 'warehouse_name', 'region', 'capacity']],\n    on='warehouse_id',\n    how='left'\n)\n\n# Join with supplier info\ninventory_enriched = pd.merge(\n    inventory_enriched,\n    suppliers[['supplier_id', 'supplier_name', 'lead_time_days', 'reliability_score']],\n    left_on='primary_supplier_id',\n    right_on='supplier_id',\n    how='left'\n)\n\n# Calculate reorder flags\ninventory_enriched['needs_reorder'] = (\n    inventory_enriched['days_of_supply'] < inventory_enriched['lead_time_days'] * 1.5\n).astype(int)\n\ninventory_enriched['critical_low'] = (\n    inventory_enriched['days_of_supply'] < inventory_enriched['lead_time_days']\n).astype(int)\n\n# Calculate suggested order quantity\ninventory_enriched['suggested_order_qty'] = np.where(\n    inventory_enriched['needs_reorder'] == 1,\n    inventory_enriched['daily_velocity'] * 30 - inventory_enriched['quantity_on_hand'],\n    0\n).clip(lower=0)\n\n# Aggregate by region for reporting\nregional_summary = inventory_enriched.groupby('region').agg({\n    'quantity_on_hand': 'sum',\n    'needs_reorder': 'sum',\n    'critical_low': 'sum',\n    'sku': 'count'\n}).reset_index()\nregional_summary.columns = ['region', 'total_inventory', 'items_need_reorder',\n                           'critical_items', 'total_skus']\n\n# Save outputs\ninventory_enriched.to_csv('inventory_analysis.csv', index=False)\nregional_summary.to_csv('regional_inventory_summary.csv', index=False)\n] PASSED [ 51%]
tests/test_py2dataiku/test_pipelines.py::TestAdvancedPipelines::test_advanced_pipeline_has_recipes[marketing_campaign-\nimport pandas as pd\nimport numpy as np\n\n# Load campaign data\ncampaigns = pd.read_csv('campaigns.csv')\nimpressions = pd.read_csv('ad_impressions.csv')\nclicks = pd.read_csv('ad_clicks.csv')\nconversions = pd.read_csv('conversions.csv')\ncustomers = pd.read_csv('customers.csv')\norders = pd.read_csv('orders.csv')\n\n# Parse dates\nimpressions['impression_time'] = pd.to_datetime(impressions['impression_time'])\nclicks['click_time'] = pd.to_datetime(clicks['click_time'])\nconversions['conversion_time'] = pd.to_datetime(conversions['conversion_time'])\n\n# Aggregate impressions by campaign\ncampaign_impressions = impressions.groupby('campaign_id').agg({\n    'impression_id': 'count',\n    'user_id': 'nunique'\n}).reset_index()\ncampaign_impressions.columns = ['campaign_id', 'total_impressions', 'unique_reach']\n\n# Aggregate clicks by campaign\ncampaign_clicks = clicks.groupby('campaign_id').agg({\n    'click_id': 'count',\n    'user_id': 'nunique'\n}).reset_index()\ncampaign_clicks.columns = ['campaign_id', 'total_clicks', 'unique_clickers']\n\n# Aggregate conversions\ncampaign_conversions = conversions.groupby('campaign_id').agg({\n    'conversion_id': 'count',\n    'conversion_value': 'sum',\n    'user_id': 'nunique'\n}).reset_index()\ncampaign_conversions.columns = ['campaign_id', 'total_conversions',\n                                'total_revenue', 'unique_converters']\n\n# Combine metrics\ncampaign_metrics = pd.merge(campaigns, campaign_impressions, on='campaign_id', how='left')\ncampaign_metrics = pd.merge(campaign_metrics, campaign_clicks, on='campaign_id', how='left')\ncampaign_metrics = pd.merge(campaign_metrics, campaign_conversions, on='campaign_id', how='left')\n\n# Fill missing values\nfor col in ['total_impressions', 'unique_reach', 'total_clicks', 'unique_clickers',\n            'total_conversions', 'total_revenue', 'unique_converters']:\n    campaign_metrics[col] = campaign_metrics[col].fillna(0)\n\n# Calculate rates\ncampaign_metrics['ctr'] = (\n    campaign_metrics['total_clicks'] / campaign_metrics['total_impressions'].replace(0, 1)\n) * 100\n\ncampaign_metrics['conversion_rate'] = (\n    campaign_metrics['total_conversions'] / campaign_metrics['total_clicks'].replace(0, 1)\n) * 100\n\ncampaign_metrics['cost_per_click'] = (\n    campaign_metrics['budget_spent'] / campaign_metrics['total_clicks'].replace(0, 1)\n)\n\ncampaign_metrics['cost_per_conversion'] = (\n    campaign_metrics['budget_spent'] / campaign_metrics['total_conversions'].replace(0, 1)\n)\n\ncampaign_metrics['roas'] = (\n    campaign_metrics['total_revenue'] / campaign_metrics['budget_spent'].replace(0, 1)\n)\n\n# Segment by performance\ncampaign_metrics['performance_tier'] = pd.cut(\n    campaign_metrics['roas'],\n    bins=[-float('inf'), 1, 2, 4, float('inf')],\n    labels=['Poor', 'Break-even', 'Good', 'Excellent']\n)\n\n# Save results\ncampaign_metrics.to_csv('campaign_performance.csv', index=False)\n] PASSED [ 51%]
tests/test_py2dataiku/test_pipelines.py::TestAdvancedPipelines::test_advanced_pipeline_has_recipes[healthcare_patient-\nimport pandas as pd\nimport numpy as np\n\n# Load healthcare data\npatients = pd.read_csv('patients.csv')\nencounters = pd.read_csv('encounters.csv')\ndiagnoses = pd.read_csv('diagnoses.csv')\nprocedures = pd.read_csv('procedures.csv')\nprescriptions = pd.read_csv('prescriptions.csv')\nlab_results = pd.read_csv('lab_results.csv')\n\n# Parse dates\nencounters['encounter_date'] = pd.to_datetime(encounters['encounter_date'])\npatients['birth_date'] = pd.to_datetime(patients['birth_date'])\n\n# Calculate patient age\npatients['age'] = (pd.Timestamp.now() - patients['birth_date']).dt.days // 365\n\n# Age groups\npatients['age_group'] = pd.cut(\n    patients['age'],\n    bins=[0, 18, 35, 50, 65, 100],\n    labels=['Pediatric', 'Young Adult', 'Adult', 'Middle Age', 'Senior']\n)\n\n# Join diagnoses to encounters\nencounters_with_dx = pd.merge(\n    encounters,\n    diagnoses[['encounter_id', 'diagnosis_code', 'diagnosis_description', 'is_primary']],\n    on='encounter_id',\n    how='left'\n)\n\n# Get primary diagnosis per encounter\nprimary_dx = encounters_with_dx[encounters_with_dx['is_primary'] == 1].copy()\n\n# Aggregate encounters per patient\npatient_encounter_summary = encounters.groupby('patient_id').agg({\n    'encounter_id': 'count',\n    'encounter_date': ['min', 'max'],\n    'total_charges': 'sum'\n}).reset_index()\npatient_encounter_summary.columns = [\n    'patient_id', 'total_encounters', 'first_encounter',\n    'last_encounter', 'total_charges'\n]\n\n# Count diagnoses per patient\npatient_dx_count = diagnoses.groupby(\n    diagnoses.merge(encounters[['encounter_id', 'patient_id']], on='encounter_id')['patient_id']\n).agg({\n    'diagnosis_code': 'nunique'\n}).reset_index()\npatient_dx_count.columns = ['patient_id', 'unique_diagnoses']\n\n# Count procedures per patient\npatient_proc_count = procedures.merge(\n    encounters[['encounter_id', 'patient_id']], on='encounter_id'\n).groupby('patient_id').agg({\n    'procedure_code': 'count'\n}).reset_index()\npatient_proc_count.columns = ['patient_id', 'total_procedures']\n\n# Combine patient profile\npatient_profile = pd.merge(patients, patient_encounter_summary, on='patient_id', how='left')\npatient_profile = pd.merge(patient_profile, patient_dx_count, on='patient_id', how='left')\npatient_profile = pd.merge(patient_profile, patient_proc_count, on='patient_id', how='left')\n\n# Fill missing values\npatient_profile['total_encounters'] = patient_profile['total_encounters'].fillna(0)\npatient_profile['unique_diagnoses'] = patient_profile['unique_diagnoses'].fillna(0)\npatient_profile['total_procedures'] = patient_profile['total_procedures'].fillna(0)\npatient_profile['total_charges'] = patient_profile['total_charges'].fillna(0)\n\n# Calculate risk score (simplified)\npatient_profile['complexity_score'] = (\n    patient_profile['unique_diagnoses'] * 0.3 +\n    patient_profile['total_procedures'] * 0.2 +\n    patient_profile['total_encounters'] * 0.1 +\n    (patient_profile['age'] / 10) * 0.4\n).round(2)\n\n# Identify high-risk patients\npatient_profile['high_risk'] = (patient_profile['complexity_score'] > 5).astype(int)\n\n# Save outputs\npatient_profile.to_csv('patient_profiles.csv', index=False)\n] PASSED [ 51%]
tests/test_py2dataiku/test_pipelines.py::TestAdvancedPipelines::test_ecommerce_has_multiple_recipe_types PASSED [ 51%]
tests/test_py2dataiku/test_pipelines.py::TestAdvancedPipelines::test_financial_transactions_complexity PASSED [ 51%]
tests/test_py2dataiku/test_pipelines.py::TestAdvancedPipelines::test_supply_chain_has_calculations PASSED [ 51%]
tests/test_py2dataiku/test_pipelines.py::TestPipelineVisualizations::test_basic_pipeline_svg_visualization PASSED [ 51%]
tests/test_py2dataiku/test_pipelines.py::TestPipelineVisualizations::test_intermediate_pipeline_svg_visualization PASSED [ 51%]
tests/test_py2dataiku/test_pipelines.py::TestPipelineVisualizations::test_advanced_pipeline_svg_visualization PASSED [ 51%]
tests/test_py2dataiku/test_pipelines.py::TestPipelineVisualizations::test_basic_pipeline_ascii_visualization PASSED [ 52%]
tests/test_py2dataiku/test_pipelines.py::TestPipelineVisualizations::test_intermediate_pipeline_ascii_visualization PASSED [ 52%]
tests/test_py2dataiku/test_pipelines.py::TestPipelineVisualizations::test_basic_pipeline_html_visualization PASSED [ 52%]
tests/test_py2dataiku/test_pipelines.py::TestPipelineVisualizations::test_basic_pipeline_plantuml_visualization PASSED [ 52%]
tests/test_py2dataiku/test_pipelines.py::TestPipelineVisualizations::test_basic_pipeline_mermaid_visualization PASSED [ 52%]
tests/test_py2dataiku/test_pipelines.py::TestPipelineSummaries::test_basic_pipeline_summary[cleaning-\nimport pandas as pd\n\n# Load raw data\ndf = pd.read_csv('raw_data.csv')\n\n# Remove rows with missing values\ndf = df.dropna()\n\n# Save cleaned data\ndf.to_csv('cleaned_data.csv', index=False)\n] PASSED [ 52%]
tests/test_py2dataiku/test_pipelines.py::TestPipelineSummaries::test_basic_pipeline_summary[column_transform-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('customers.csv')\n\n# Clean text columns\ndf['name'] = df['name'].str.strip()\ndf['name'] = df['name'].str.title()\ndf['email'] = df['email'].str.lower()\n\n# Save result\ndf.to_csv('customers_cleaned.csv', index=False)\n] PASSED [ 52%]
tests/test_py2dataiku/test_pipelines.py::TestPipelineSummaries::test_basic_pipeline_summary[filtering-\nimport pandas as pd\n\n# Load sales data\nsales = pd.read_csv('sales.csv')\n\n# Filter to high-value transactions\nhigh_value = sales[sales['amount'] > 1000]\n\n# Save filtered data\nhigh_value.to_csv('high_value_sales.csv', index=False)\n] PASSED [ 52%]
tests/test_py2dataiku/test_pipelines.py::TestPipelineSummaries::test_intermediate_pipeline_summary[customer_order_analysis-\nimport pandas as pd\n\n# Load data\ncustomers = pd.read_csv('customers.csv')\norders = pd.read_csv('orders.csv')\n\n# Clean customer data\ncustomers['name'] = customers['name'].str.strip().str.title()\ncustomers = customers.dropna(subset=['customer_id'])\n\n# Join customers with orders\ncustomer_orders = pd.merge(\n    customers,\n    orders,\n    on='customer_id',\n    how='left'\n)\n\n# Calculate order statistics per customer\ncustomer_summary = customer_orders.groupby('customer_id').agg({\n    'order_id': 'count',\n    'amount': 'sum'\n}).reset_index()\n\ncustomer_summary.columns = ['customer_id', 'order_count', 'total_amount']\n\n# Save result\ncustomer_summary.to_csv('customer_order_summary.csv', index=False)\n] PASSED [ 53%]
tests/test_py2dataiku/test_pipelines.py::TestPipelineSummaries::test_intermediate_pipeline_summary[sales_product_enrichment-\nimport pandas as pd\n\n# Load data\nsales = pd.read_csv('sales.csv')\nproducts = pd.read_csv('products.csv')\ncategories = pd.read_csv('categories.csv')\n\n# Join sales with products\nsales_with_products = pd.merge(\n    sales,\n    products[['product_id', 'product_name', 'category_id', 'unit_price']],\n    on='product_id',\n    how='inner'\n)\n\n# Join with categories\nsales_enriched = pd.merge(\n    sales_with_products,\n    categories[['category_id', 'category_name']],\n    on='category_id',\n    how='left'\n)\n\n# Calculate line total\nsales_enriched['line_total'] = sales_enriched['quantity'] * sales_enriched['unit_price']\n\n# Save enriched data\nsales_enriched.to_csv('sales_enriched.csv', index=False)\n] PASSED [ 53%]
tests/test_py2dataiku/test_pipelines.py::TestPipelineSummaries::test_intermediate_pipeline_summary[time_based_aggregation-\nimport pandas as pd\n\n# Load transaction data\ntransactions = pd.read_csv('transactions.csv')\n\n# Convert date column\ntransactions['transaction_date'] = pd.to_datetime(transactions['transaction_date'])\n\n# Extract date parts\ntransactions['year'] = transactions['transaction_date'].dt.year\ntransactions['month'] = transactions['transaction_date'].dt.month\ntransactions['day_of_week'] = transactions['transaction_date'].dt.dayofweek\n\n# Aggregate by month\nmonthly_summary = transactions.groupby(['year', 'month']).agg({\n    'amount': ['sum', 'mean', 'count'],\n    'customer_id': 'nunique'\n}).reset_index()\n\nmonthly_summary.columns = ['year', 'month', 'total_amount', 'avg_amount',\n                           'transaction_count', 'unique_customers']\n\n# Save result\nmonthly_summary.to_csv('monthly_summary.csv', index=False)\n] PASSED [ 53%]
tests/test_py2dataiku/test_pipelines.py::TestPipelineSummaries::test_advanced_pipeline_summary[ecommerce_analytics-\nimport pandas as pd\nimport numpy as np\n\n# Load all data sources\ncustomers = pd.read_csv('customers.csv')\norders = pd.read_csv('orders.csv')\norder_items = pd.read_csv('order_items.csv')\nproducts = pd.read_csv('products.csv')\ncategories = pd.read_csv('categories.csv')\n\n# Clean customer data\ncustomers['email'] = customers['email'].str.lower().str.strip()\ncustomers['name'] = customers['name'].str.strip().str.title()\ncustomers = customers.drop_duplicates(subset=['email'])\n\n# Join order items with products\nitems_with_products = pd.merge(\n    order_items,\n    products[['product_id', 'product_name', 'category_id', 'unit_cost']],\n    on='product_id',\n    how='left'\n)\n\n# Add category info\nitems_enriched = pd.merge(\n    items_with_products,\n    categories[['category_id', 'category_name']],\n    on='category_id',\n    how='left'\n)\n\n# Calculate item metrics\nitems_enriched['line_total'] = items_enriched['quantity'] * items_enriched['unit_price']\nitems_enriched['profit'] = items_enriched['line_total'] - (items_enriched['quantity'] * items_enriched['unit_cost'])\n\n# Aggregate to order level\norder_summary = items_enriched.groupby('order_id').agg({\n    'line_total': 'sum',\n    'profit': 'sum',\n    'quantity': 'sum',\n    'product_id': 'nunique'\n}).reset_index()\n\norder_summary.columns = ['order_id', 'order_total', 'order_profit', 'total_items', 'unique_products']\n\n# Join with orders\norders_complete = pd.merge(orders, order_summary, on='order_id', how='left')\n\n# Parse dates\norders_complete['order_date'] = pd.to_datetime(orders_complete['order_date'])\n\n# Join with customers\ncustomer_orders = pd.merge(\n    orders_complete,\n    customers[['customer_id', 'name', 'email', 'signup_date', 'region']],\n    on='customer_id',\n    how='left'\n)\n\n# Calculate customer lifetime metrics\ncustomer_lifetime = customer_orders.groupby('customer_id').agg({\n    'order_id': 'count',\n    'order_total': 'sum',\n    'order_profit': 'sum',\n    'order_date': ['min', 'max']\n}).reset_index()\n\ncustomer_lifetime.columns = [\n    'customer_id', 'total_orders', 'lifetime_value',\n    'lifetime_profit', 'first_order', 'last_order'\n]\n\n# Calculate days since last order\ncustomer_lifetime['days_since_last_order'] = (\n    pd.Timestamp.now() - customer_lifetime['last_order']\n).dt.days\n\n# Segment customers\ncustomer_lifetime['value_segment'] = pd.cut(\n    customer_lifetime['lifetime_value'],\n    bins=[0, 100, 500, 2000, float('inf')],\n    labels=['Bronze', 'Silver', 'Gold', 'Platinum']\n)\n\n# Save all outputs\ncustomer_orders.to_csv('customer_orders_enriched.csv', index=False)\ncustomer_lifetime.to_csv('customer_lifetime_metrics.csv', index=False)\n] PASSED [ 53%]
tests/test_py2dataiku/test_pipelines.py::TestPipelineSummaries::test_advanced_pipeline_summary[financial_transactions-\nimport pandas as pd\nimport numpy as np\n\n# Load data\ntransactions = pd.read_csv('transactions.csv')\naccounts = pd.read_csv('accounts.csv')\nmerchants = pd.read_csv('merchants.csv')\nfraud_labels = pd.read_csv('fraud_labels.csv')\n\n# Parse transaction timestamps\ntransactions['transaction_time'] = pd.to_datetime(transactions['transaction_time'])\n\n# Clean and validate\ntransactions = transactions.dropna(subset=['account_id', 'amount'])\ntransactions['amount'] = transactions['amount'].abs()  # Ensure positive amounts\n\n# Add time features\ntransactions['hour'] = transactions['transaction_time'].dt.hour\ntransactions['day_of_week'] = transactions['transaction_time'].dt.dayofweek\ntransactions['is_weekend'] = transactions['day_of_week'].isin([5, 6]).astype(int)\ntransactions['is_night'] = transactions['hour'].between(22, 6).astype(int)\n\n# Join with account info\ntransactions_enriched = pd.merge(\n    transactions,\n    accounts[['account_id', 'account_type', 'credit_limit', 'customer_id', 'open_date']],\n    on='account_id',\n    how='left'\n)\n\n# Join with merchant info\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    merchants[['merchant_id', 'merchant_category', 'merchant_country', 'risk_score']],\n    on='merchant_id',\n    how='left'\n)\n\n# Calculate account-level aggregates\naccount_stats = transactions_enriched.groupby('account_id').agg({\n    'amount': ['mean', 'std', 'max', 'count'],\n    'merchant_id': 'nunique'\n}).reset_index()\naccount_stats.columns = ['account_id', 'avg_amount', 'std_amount', 'max_amount',\n                         'transaction_count', 'unique_merchants']\n\n# Join stats back\ntransactions_enriched = pd.merge(\n    transactions_enriched,\n    account_stats,\n    on='account_id',\n    how='left'\n)\n\n# Calculate deviation from normal\ntransactions_enriched['amount_zscore'] = (\n    (transactions_enriched['amount'] - transactions_enriched['avg_amount']) /\n    transactions_enriched['std_amount'].replace(0, 1)\n)\n\n# Flag suspicious transactions\ntransactions_enriched['high_amount_flag'] = (transactions_enriched['amount_zscore'] > 3).astype(int)\ntransactions_enriched['new_merchant_flag'] = 0  # Would need historical data\n\n# Join with fraud labels for training\nlabeled_transactions = pd.merge(\n    transactions_enriched,\n    fraud_labels[['transaction_id', 'is_fraud']],\n    on='transaction_id',\n    how='left'\n)\n\n# Fill unlabeled as not fraud\nlabeled_transactions['is_fraud'] = labeled_transactions['is_fraud'].fillna(0)\n\n# Save outputs\nlabeled_transactions.to_csv('transactions_processed.csv', index=False)\naccount_stats.to_csv('account_statistics.csv', index=False)\n] PASSED [ 53%]
tests/test_py2dataiku/test_pipelines.py::TestPipelineExports::test_basic_pipeline_to_yaml PASSED [ 53%]
tests/test_py2dataiku/test_pipelines.py::TestPipelineExports::test_basic_pipeline_to_json PASSED [ 53%]
tests/test_py2dataiku/test_pipelines.py::TestPipelineExports::test_intermediate_pipeline_to_dict PASSED [ 53%]
tests/test_py2dataiku/test_pipelines.py::TestPipelineEdgeCases::test_empty_code PASSED [ 53%]
tests/test_py2dataiku/test_pipelines.py::TestPipelineEdgeCases::test_import_only_code PASSED [ 54%]
tests/test_py2dataiku/test_pipelines.py::TestPipelineEdgeCases::test_comment_only_code PASSED [ 54%]
tests/test_py2dataiku/test_pipelines.py::TestPipelineEdgeCases::test_mixed_operations PASSED [ 54%]
tests/test_py2dataiku/test_pipelines.py::TestRecipeValidation::test_recipe_has_inputs_and_outputs PASSED [ 54%]
tests/test_py2dataiku/test_pipelines.py::TestRecipeValidation::test_recipe_to_json_valid PASSED [ 54%]
tests/test_py2dataiku/test_pipelines.py::TestRecipeValidation::test_flow_validation PASSED [ 54%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesRegistry::test_processor_examples_dict_not_empty PASSED [ 54%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesRegistry::test_list_processor_examples_returns_list PASSED [ 54%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesRegistry::test_all_examples_are_strings PASSED [ 55%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesRegistry::test_get_processor_example_returns_code PASSED [ 55%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesRegistry::test_get_nonexistent_example_returns_empty PASSED [ 55%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesRegistry::test_minimum_processor_count PASSED [ 55%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[column_renamer-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Rename single column\ndf = df.rename(columns={'old_name': 'new_name'})\n\n# Rename multiple columns\ndf = df.rename(columns={\n    'cust_id': 'customer_id',\n    'prod_name': 'product_name',\n    'qty': 'quantity',\n    'amt': 'amount'\n})\n\ndf.to_csv('renamed.csv', index=False)\n] PASSED [ 55%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[column_copier-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Copy column with new name\ndf['customer_id_backup'] = df['customer_id']\ndf['amount_original'] = df['amount']\n\n# Copy with transformation\ndf['amount_copy'] = df['amount'].copy()\n\ndf.to_csv('copied.csv', index=False)\n] PASSED [ 55%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[column_deleter-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Delete single column\ndf = df.drop(columns=['unnecessary_column'])\n\n# Delete multiple columns\ndf = df.drop(columns=['temp1', 'temp2', 'debug_info'])\n\n# Delete using del\ndel df['another_column']\n\ndf.to_csv('reduced.csv', index=False)\n] PASSED [ 55%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[columns_selector-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Select specific columns\ndf = df[['customer_id', 'name', 'email', 'amount']]\n\n# Select using column list\ncols_to_keep = ['id', 'date', 'value']\ndf = df[cols_to_keep]\n\ndf.to_csv('selected.csv', index=False)\n] PASSED [ 55%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[column_reorder-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Reorder columns\nnew_order = ['customer_id', 'name', 'email', 'phone', 'address', 'created_date']\ndf = df[new_order]\n\n# Reorder with some columns first\npriority_cols = ['id', 'status']\nother_cols = [c for c in df.columns if c not in priority_cols]\ndf = df[priority_cols + other_cols]\n\ndf.to_csv('reordered.csv', index=False)\n] PASSED [ 55%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[columns_concatenator-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Concatenate columns into new column\ndf['full_name'] = df['first_name'] + ' ' + df['last_name']\n\n# Concatenate with separator\ndf['full_address'] = df['street'] + ', ' + df['city'] + ', ' + df['state'] + ' ' + df['zip']\n\ndf.to_csv('concatenated.csv', index=False)\n] PASSED [ 56%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[fill_empty_with_value-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Fill with constant value\ndf['category'] = df['category'].fillna('Unknown')\ndf['count'] = df['count'].fillna(0)\ndf['active'] = df['active'].fillna(False)\n\n# Fill multiple columns\ndf.fillna({'status': 'pending', 'priority': 'medium', 'score': 0}, inplace=True)\n\ndf.to_csv('filled.csv', index=False)\n] PASSED [ 56%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[remove_rows_on_empty-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Remove rows where any column is null\ndf = df.dropna()\n\n# Remove rows where specific columns are null\ndf = df.dropna(subset=['customer_id', 'email'])\n\n# Remove rows where all specified columns are null\ndf = df.dropna(subset=['phone', 'email'], how='all')\n\ndf.to_csv('no_nulls.csv', index=False)\n] PASSED [ 56%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[fill_empty_with_previous_next-\nimport pandas as pd\n\ndf = pd.read_csv('timeseries.csv')\ndf = df.sort_values('date')\n\n# Forward fill (use previous value)\ndf['value'] = df['value'].ffill()\n\n# Backward fill (use next value)\ndf['value'] = df['value'].bfill()\n\n# Forward fill with limit\ndf['metric'] = df['metric'].ffill(limit=3)\n\ndf.to_csv('filled_timeseries.csv', index=False)\n] PASSED [ 56%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[fill_empty_with_computed_value-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Fill with mean\ndf['score'] = df['score'].fillna(df['score'].mean())\n\n# Fill with median\ndf['age'] = df['age'].fillna(df['age'].median())\n\n# Fill with mode\ndf['category'] = df['category'].fillna(df['category'].mode()[0])\n\n# Fill with grouped mean\ndf['amount'] = df.groupby('category')['amount'].transform(\n    lambda x: x.fillna(x.mean())\n)\n\ndf.to_csv('computed_fill.csv', index=False)\n] PASSED [ 56%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[impute_with_ml-\nimport pandas as pd\nfrom sklearn.impute import KNNImputer\nimport numpy as np\n\ndf = pd.read_csv('data.csv')\n\n# KNN imputation for numeric columns\nnumeric_cols = df.select_dtypes(include=[np.number]).columns\nimputer = KNNImputer(n_neighbors=5)\ndf[numeric_cols] = imputer.fit_transform(df[numeric_cols])\n\ndf.to_csv('ml_imputed.csv', index=False)\n] PASSED [ 56%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[string_transformer_uppercase-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Convert to uppercase\ndf['name'] = df['name'].str.upper()\ndf['code'] = df['code'].str.upper()\n\ndf.to_csv('uppercase.csv', index=False)\n] PASSED [ 56%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[string_transformer_lowercase-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Convert to lowercase\ndf['email'] = df['email'].str.lower()\ndf['username'] = df['username'].str.lower()\n\ndf.to_csv('lowercase.csv', index=False)\n] PASSED [ 56%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[string_transformer_titlecase-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Convert to title case\ndf['name'] = df['name'].str.title()\ndf['city'] = df['city'].str.title()\n\ndf.to_csv('titlecase.csv', index=False)\n] PASSED [ 57%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[string_transformer_trim-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Trim whitespace from both sides\ndf['name'] = df['name'].str.strip()\n\n# Left trim only\ndf['code'] = df['code'].str.lstrip()\n\n# Right trim only\ndf['description'] = df['description'].str.rstrip()\n\ndf.to_csv('trimmed.csv', index=False)\n] PASSED [ 57%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[string_transformer_normalize_whitespace-\nimport pandas as pd\nimport re\n\ndf = pd.read_csv('data.csv')\n\n# Normalize whitespace (multiple spaces to single)\ndf['text'] = df['text'].str.replace(r'\\s+', ' ', regex=True)\n\n# Remove all whitespace\ndf['code'] = df['code'].str.replace(' ', '')\n\ndf.to_csv('normalized_whitespace.csv', index=False)\n] PASSED [ 57%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[tokenizer-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Split string into tokens\ndf['words'] = df['sentence'].str.split()\n\n# Split by specific delimiter\ndf['tags'] = df['tag_string'].str.split(',')\n\n# Split and expand into columns\nsplit_cols = df['full_name'].str.split(' ', expand=True)\ndf['first_name'] = split_cols[0]\ndf['last_name'] = split_cols[1]\n\ndf.to_csv('tokenized.csv', index=False)\n] PASSED [ 57%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[regexp_extractor-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Extract with regex pattern\ndf['area_code'] = df['phone'].str.extract(r'\\((\\d{3})\\)')\ndf['email_domain'] = df['email'].str.extract(r'@(.+)$')\n\n# Extract multiple groups\nextracted = df['address'].str.extract(r'(\\d+)\\s+(.+),\\s+(\\w+)')\ndf['street_num'] = extracted[0]\ndf['street_name'] = extracted[1]\ndf['city'] = extracted[2]\n\ndf.to_csv('extracted.csv', index=False)\n] PASSED [ 57%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[find_replace-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Simple find and replace\ndf['status'] = df['status'].str.replace('ACTV', 'Active')\ndf['status'] = df['status'].str.replace('INACTV', 'Inactive')\n\n# Replace with regex\ndf['phone'] = df['phone'].str.replace(r'[^\\d]', '', regex=True)\n\n# Multiple replacements\nreplacements = {'NY': 'New York', 'CA': 'California', 'TX': 'Texas'}\ndf['state'] = df['state'].replace(replacements)\n\ndf.to_csv('replaced.csv', index=False)\n] PASSED [ 57%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[split_column-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Split column into multiple\ndf[['first_name', 'last_name']] = df['full_name'].str.split(' ', n=1, expand=True)\n\n# Split by delimiter\ndf[['year', 'month', 'day']] = df['date_str'].str.split('-', expand=True)\n\n# Split and get specific part\ndf['domain'] = df['email'].str.split('@').str[1]\n\ndf.to_csv('split.csv', index=False)\n] PASSED [ 57%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[concat_columns-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Simple concatenation\ndf['full_name'] = df['first_name'] + ' ' + df['last_name']\n\n# Concatenation with multiple columns\ndf['full_address'] = (\n    df['street'] + ', ' +\n    df['city'] + ', ' +\n    df['state'] + ' ' +\n    df['zip'].astype(str)\n)\n\n# Using string format\ndf['display'] = df['id'].astype(str) + ': ' + df['name']\n\ndf.to_csv('concatenated.csv', index=False)\n] PASSED [ 57%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[html_stripper-\nimport pandas as pd\nimport re\n\ndf = pd.read_csv('data.csv')\n\n# Remove HTML tags\ndf['clean_text'] = df['html_content'].str.replace(r'<[^>]+>', '', regex=True)\n\n# Remove specific tags\ndf['no_script'] = df['content'].str.replace(r'<script[^>]*>.*?</script>', '', regex=True)\n\ndf.to_csv('stripped_html.csv', index=False)\n] PASSED [ 58%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[ngrammer-\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndf = pd.read_csv('text_data.csv')\n\n# Generate bigrams\nvectorizer = CountVectorizer(ngram_range=(2, 2), analyzer='word')\nbigrams = vectorizer.fit_transform(df['text'])\ndf['bigram_features'] = [' '.join(vectorizer.get_feature_names_out())] * len(df)\n\ndf.to_csv('ngrams.csv', index=False)\n] PASSED [ 58%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[text_simplifier-\nimport pandas as pd\nimport unicodedata\n\ndf = pd.read_csv('data.csv')\n\n# Remove accents\ndf['simplified'] = df['text'].apply(\n    lambda x: unicodedata.normalize('NFKD', str(x)).encode('ASCII', 'ignore').decode()\n)\n\n# Remove non-alphanumeric\ndf['clean'] = df['text'].str.replace(r'[^a-zA-Z0-9\\s]', '', regex=True)\n\ndf.to_csv('simplified.csv', index=False)\n] PASSED [ 58%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[stem_text-\nimport pandas as pd\nfrom nltk.stem import PorterStemmer\n\ndf = pd.read_csv('text_data.csv')\n\nstemmer = PorterStemmer()\ndf['stemmed'] = df['text'].apply(\n    lambda x: ' '.join([stemmer.stem(word) for word in str(x).split()])\n)\n\ndf.to_csv('stemmed.csv', index=False)\n] PASSED [ 58%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[lemmatize_text-\nimport pandas as pd\nfrom nltk.stem import WordNetLemmatizer\n\ndf = pd.read_csv('text_data.csv')\n\nlemmatizer = WordNetLemmatizer()\ndf['lemmatized'] = df['text'].apply(\n    lambda x: ' '.join([lemmatizer.lemmatize(word) for word in str(x).split()])\n)\n\ndf.to_csv('lemmatized.csv', index=False)\n] PASSED [ 58%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[url_parser-\nimport pandas as pd\nfrom urllib.parse import urlparse\n\ndf = pd.read_csv('data.csv')\n\n# Parse URL components\ndf['domain'] = df['url'].apply(lambda x: urlparse(str(x)).netloc)\ndf['path'] = df['url'].apply(lambda x: urlparse(str(x)).path)\ndf['scheme'] = df['url'].apply(lambda x: urlparse(str(x)).scheme)\n\ndf.to_csv('parsed_urls.csv', index=False)\n] PASSED [ 58%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[email_domain_extractor-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Extract email domain\ndf['email_domain'] = df['email'].str.split('@').str[1]\n\n# Extract email username\ndf['email_user'] = df['email'].str.split('@').str[0]\n\ndf.to_csv('email_parsed.csv', index=False)\n] PASSED [ 58%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[numerical_transformer_multiply-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Multiply by constant\ndf['amount_cents'] = df['amount'] * 100\ndf['doubled'] = df['value'] * 2\n\ndf.to_csv('multiplied.csv', index=False)\n] PASSED [ 58%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[numerical_transformer_divide-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Divide by constant\ndf['amount_thousands'] = df['amount'] / 1000\ndf['halved'] = df['value'] / 2\n\ndf.to_csv('divided.csv', index=False)\n] PASSED [ 58%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[numerical_transformer_add-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Add constant\ndf['adjusted_price'] = df['price'] + 10\ndf['score_bonus'] = df['score'] + 5\n\ndf.to_csv('added.csv', index=False)\n] PASSED [ 59%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[numerical_transformer_subtract-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Subtract constant\ndf['net_amount'] = df['gross_amount'] - df['tax']\ndf['adjusted'] = df['value'] - 100\n\ndf.to_csv('subtracted.csv', index=False)\n] PASSED [ 59%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[numerical_transformer_power-\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('data.csv')\n\n# Raise to power\ndf['squared'] = df['value'] ** 2\ndf['cubed'] = df['value'] ** 3\ndf['sqrt'] = np.sqrt(df['value'])\n\ndf.to_csv('powered.csv', index=False)\n] PASSED [ 59%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[numerical_transformer_log-\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('data.csv')\n\n# Logarithmic transformations\ndf['log_amount'] = np.log(df['amount'])\ndf['log10_amount'] = np.log10(df['amount'])\ndf['log1p_amount'] = np.log1p(df['amount'])\n\ndf.to_csv('logged.csv', index=False)\n] PASSED [ 59%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[round_column-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Round to decimal places\ndf['amount_rounded'] = df['amount'].round(2)\ndf['score_rounded'] = df['score'].round(0)\n\n# Round to significant figures\ndf['value_rounded'] = df['value'].round(-2)  # Round to nearest 100\n\ndf.to_csv('rounded.csv', index=False)\n] PASSED [ 59%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[abs_column-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Absolute value\ndf['abs_change'] = df['change'].abs()\ndf['abs_delta'] = df['delta'].abs()\n\ndf.to_csv('absolute.csv', index=False)\n] PASSED [ 59%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[clip_column-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Clip to range\ndf['clipped_score'] = df['score'].clip(lower=0, upper=100)\ndf['clipped_amount'] = df['amount'].clip(lower=0)  # Only lower bound\n\ndf.to_csv('clipped.csv', index=False)\n] PASSED [ 59%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[binner-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Create bins with pd.cut\ndf['age_group'] = pd.cut(\n    df['age'],\n    bins=[0, 18, 35, 50, 65, 100],\n    labels=['Child', 'Young Adult', 'Adult', 'Middle Age', 'Senior']\n)\n\n# Equal-width binning\ndf['score_bin'] = pd.cut(df['score'], bins=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n\n# Quantile-based binning\ndf['income_quartile'] = pd.qcut(df['income'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n\ndf.to_csv('binned.csv', index=False)\n] PASSED [ 59%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[normalizer-\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('data.csv')\n\n# Min-Max normalization\ndf['norm_minmax'] = (df['value'] - df['value'].min()) / (df['value'].max() - df['value'].min())\n\n# Z-score normalization\ndf['norm_zscore'] = (df['value'] - df['value'].mean()) / df['value'].std()\n\n# Robust scaling (using median and IQR)\nq1 = df['value'].quantile(0.25)\nq3 = df['value'].quantile(0.75)\ndf['norm_robust'] = (df['value'] - df['value'].median()) / (q3 - q1)\n\ndf.to_csv('normalized.csv', index=False)\n] PASSED [ 60%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[discretizer-\nimport pandas as pd\nfrom sklearn.preprocessing import KBinsDiscretizer\n\ndf = pd.read_csv('data.csv')\n\n# Discretize continuous variable\ndiscretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\ndf['value_discrete'] = discretizer.fit_transform(df[['value']])\n\ndf.to_csv('discretized.csv', index=False)\n] PASSED [ 60%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[type_setter-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Convert types\ndf['customer_id'] = df['customer_id'].astype(str)\ndf['amount'] = df['amount'].astype(float)\ndf['quantity'] = df['quantity'].astype(int)\ndf['is_active'] = df['is_active'].astype(bool)\n\ndf.to_csv('typed.csv', index=False)\n] PASSED [ 60%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[date_parser-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Parse dates\ndf['date'] = pd.to_datetime(df['date_string'])\ndf['timestamp'] = pd.to_datetime(df['ts_string'], format='%Y-%m-%d %H:%M:%S')\ndf['custom_date'] = pd.to_datetime(df['date_str'], format='%d/%m/%Y')\n\ndf.to_csv('parsed_dates.csv', index=False)\n] PASSED [ 60%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[date_formatter-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\ndf['date'] = pd.to_datetime(df['date'])\n\n# Format dates\ndf['date_formatted'] = df['date'].dt.strftime('%Y-%m-%d')\ndf['date_display'] = df['date'].dt.strftime('%B %d, %Y')\ndf['date_short'] = df['date'].dt.strftime('%m/%d/%y')\n\ndf.to_csv('formatted_dates.csv', index=False)\n] PASSED [ 60%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[boolean_converter-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Convert to boolean\ndf['is_active'] = df['status'].map({'active': True, 'inactive': False})\ndf['has_value'] = df['value'].notna()\ndf['is_positive'] = df['amount'] > 0\n\ndf.to_csv('boolean.csv', index=False)\n] PASSED [ 60%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[date_components_extractor-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\ndf['date'] = pd.to_datetime(df['date'])\n\n# Extract date components\ndf['year'] = df['date'].dt.year\ndf['month'] = df['date'].dt.month\ndf['day'] = df['date'].dt.day\ndf['day_of_week'] = df['date'].dt.dayofweek\ndf['day_of_year'] = df['date'].dt.dayofyear\ndf['week_of_year'] = df['date'].dt.isocalendar().week\ndf['quarter'] = df['date'].dt.quarter\ndf['hour'] = df['date'].dt.hour\ndf['minute'] = df['date'].dt.minute\n\ndf.to_csv('date_components.csv', index=False)\n] PASSED [ 60%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[date_diff_calculator-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\ndf['start_date'] = pd.to_datetime(df['start_date'])\ndf['end_date'] = pd.to_datetime(df['end_date'])\n\n# Calculate date differences\ndf['days_diff'] = (df['end_date'] - df['start_date']).dt.days\ndf['hours_diff'] = (df['end_date'] - df['start_date']).dt.total_seconds() / 3600\ndf['weeks_diff'] = df['days_diff'] / 7\n\n# Days since reference date\ndf['days_since_2020'] = (df['date'] - pd.Timestamp('2020-01-01')).dt.days\n\ndf.to_csv('date_diffs.csv', index=False)\n] PASSED [ 60%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[timezone_converter-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n\n# Convert timezone\ndf['timestamp_utc'] = df['timestamp'].dt.tz_localize('America/New_York').dt.tz_convert('UTC')\ndf['timestamp_pacific'] = df['timestamp_utc'].dt.tz_convert('America/Los_Angeles')\n\ndf.to_csv('timezone_converted.csv', index=False)\n] PASSED [ 60%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[filter_on_value-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Filter by exact value\ndf = df[df['status'] == 'active']\ndf = df[df['category'] != 'deprecated']\n\n# Filter by multiple values\ndf = df[df['region'].isin(['North', 'South', 'East'])]\n\ndf.to_csv('filtered_value.csv', index=False)\n] PASSED [ 61%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[filter_on_bad_type-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Filter rows with valid numeric values\ndf['amount_numeric'] = pd.to_numeric(df['amount'], errors='coerce')\ndf = df[df['amount_numeric'].notna()]\n\n# Filter rows with valid dates\ndf['date_parsed'] = pd.to_datetime(df['date_string'], errors='coerce')\ndf = df[df['date_parsed'].notna()]\n\ndf.to_csv('filtered_types.csv', index=False)\n] PASSED [ 61%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[filter_on_formula-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Complex filter expression\ndf = df[(df['amount'] > 100) & (df['status'] == 'active') | (df['priority'] == 'high')]\n\n# Filter using query\ndf = df.query('amount > 100 and status == "active"')\n\ndf.to_csv('filtered_formula.csv', index=False)\n] PASSED [ 61%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[filter_on_date_range-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\ndf['date'] = pd.to_datetime(df['date'])\n\n# Filter by date range\nstart_date = pd.Timestamp('2023-01-01')\nend_date = pd.Timestamp('2023-12-31')\ndf = df[(df['date'] >= start_date) & (df['date'] <= end_date)]\n\n# Filter last N days\nfrom datetime import datetime, timedelta\ncutoff = datetime.now() - timedelta(days=30)\ndf = df[df['date'] >= cutoff]\n\ndf.to_csv('filtered_dates.csv', index=False)\n] PASSED [ 61%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[filter_on_numeric_range-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Filter by numeric range\ndf = df[(df['amount'] >= 100) & (df['amount'] <= 1000)]\n\n# Filter with between\ndf = df[df['score'].between(50, 100)]\n\ndf.to_csv('filtered_numeric.csv', index=False)\n] PASSED [ 61%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[filter_on_multiple_values-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Filter on multiple values using isin\nallowed_statuses = ['active', 'pending', 'approved']\ndf = df[df['status'].isin(allowed_statuses)]\n\n# Exclude multiple values\nexcluded_categories = ['test', 'demo', 'sample']\ndf = df[~df['category'].isin(excluded_categories)]\n\ndf.to_csv('filtered_multiple.csv', index=False)\n] PASSED [ 61%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[flag_on_value-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Create flag based on value\ndf['is_premium'] = (df['tier'] == 'premium').astype(int)\ndf['is_active'] = (df['status'] == 'active').astype(int)\n\ndf.to_csv('flagged_value.csv', index=False)\n] PASSED [ 61%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[flag_on_formula-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Create flag based on formula\ndf['high_value'] = ((df['amount'] > 1000) & (df['quantity'] > 10)).astype(int)\ndf['needs_review'] = ((df['status'] == 'pending') | (df['age'] > 30)).astype(int)\n\ndf.to_csv('flagged_formula.csv', index=False)\n] PASSED [ 61%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[flag_on_bad_type-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Flag rows with type issues\ndf['invalid_amount'] = pd.to_numeric(df['amount'], errors='coerce').isna().astype(int)\ndf['invalid_date'] = pd.to_datetime(df['date'], errors='coerce').isna().astype(int)\n\ndf.to_csv('flagged_types.csv', index=False)\n] PASSED [ 62%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[flag_on_date_range-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\ndf['date'] = pd.to_datetime(df['date'])\n\n# Flag based on date range\ndf['is_recent'] = (df['date'] >= '2023-01-01').astype(int)\ndf['is_q1'] = ((df['date'].dt.month >= 1) & (df['date'].dt.month <= 3)).astype(int)\n\ndf.to_csv('flagged_dates.csv', index=False)\n] PASSED [ 62%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[flag_on_numeric_range-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Flag based on numeric range\ndf['in_range'] = df['score'].between(50, 100).astype(int)\ndf['outlier'] = ((df['value'] < df['value'].quantile(0.05)) |\n                  (df['value'] > df['value'].quantile(0.95))).astype(int)\n\ndf.to_csv('flagged_numeric.csv', index=False)\n] PASSED [ 62%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[remove_duplicates-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Remove complete duplicates\ndf = df.drop_duplicates()\n\n# Remove duplicates based on subset\ndf = df.drop_duplicates(subset=['customer_id', 'date'])\n\n# Keep first/last occurrence\ndf = df.drop_duplicates(subset=['email'], keep='first')\n\ndf.to_csv('deduplicated.csv', index=False)\n] PASSED [ 62%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[sort_rows-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Sort by single column\ndf = df.sort_values('date')\n\n# Sort by multiple columns\ndf = df.sort_values(['category', 'amount'], ascending=[True, False])\n\ndf.to_csv('sorted.csv', index=False)\n] PASSED [ 62%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[sample_rows-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Random sample\ndf_sample = df.sample(n=1000, random_state=42)\n\n# Percentage sample\ndf_sample_pct = df.sample(frac=0.1, random_state=42)\n\ndf_sample.to_csv('sampled.csv', index=False)\n] PASSED [ 62%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[shuffle_rows-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Shuffle rows\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\n\ndf.to_csv('shuffled.csv', index=False)\n] PASSED [ 62%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[create_column_with_grel-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# GREL-like expression (complex formula)\ndf['computed'] = df.apply(\n    lambda row: f"{row['first_name']} {row['last_name']} ({row['department']})",\n    axis=1\n)\n\n# Conditional expression\ndf['tier'] = df.apply(\n    lambda row: 'Premium' if row['amount'] > 1000 else 'Standard',\n    axis=1\n)\n\ndf.to_csv('grel_computed.csv', index=False)\n] PASSED [ 62%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[formula-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Simple formula\ndf['total'] = df['quantity'] * df['unit_price']\ndf['discount_amount'] = df['total'] * df['discount_rate']\ndf['final_amount'] = df['total'] - df['discount_amount']\n\n# Multi-column formula\ndf['weighted_score'] = (df['score1'] * 0.3 + df['score2'] * 0.5 + df['score3'] * 0.2)\n\ndf.to_csv('formula.csv', index=False)\n] PASSED [ 62%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[multi_column_formula-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Apply formula across multiple columns\nnumeric_cols = ['col1', 'col2', 'col3']\nfor col in numeric_cols:\n    df[f'{col}_normalized'] = (df[col] - df[col].mean()) / df[col].std()\n\ndf.to_csv('multi_formula.csv', index=False)\n] PASSED [ 63%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[hash_computer-\nimport pandas as pd\nimport hashlib\n\ndf = pd.read_csv('data.csv')\n\n# Compute hash\ndf['hash'] = df['email'].apply(\n    lambda x: hashlib.sha256(str(x).encode()).hexdigest()\n)\n\n# MD5 hash\ndf['md5'] = df['customer_id'].apply(\n    lambda x: hashlib.md5(str(x).encode()).hexdigest()\n)\n\ndf.to_csv('hashed.csv', index=False)\n] PASSED [ 63%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[uuid_generator-\nimport pandas as pd\nimport uuid\n\ndf = pd.read_csv('data.csv')\n\n# Generate UUID for each row\ndf['uuid'] = [str(uuid.uuid4()) for _ in range(len(df))]\n\ndf.to_csv('with_uuid.csv', index=False)\n] PASSED [ 63%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[merge_long_tail_values-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Merge rare categories into 'Other'\nvalue_counts = df['category'].value_counts()\nrare_categories = value_counts[value_counts < 10].index\ndf['category_merged'] = df['category'].apply(\n    lambda x: 'Other' if x in rare_categories else x\n)\n\ndf.to_csv('merged_categories.csv', index=False)\n] PASSED [ 63%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[categorical_encoder-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# One-hot encoding\ndf_encoded = pd.get_dummies(df, columns=['category', 'status'])\n\ndf_encoded.to_csv('encoded.csv', index=False)\n] PASSED [ 63%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[one_hot_encoder-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# One-hot encoding for specific columns\ndf_onehot = pd.get_dummies(df, columns=['color', 'size'], prefix=['color', 'size'])\n\ndf_onehot.to_csv('onehot.csv', index=False)\n] PASSED [ 63%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[label_encoder-\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndf = pd.read_csv('data.csv')\n\n# Label encoding\nle = LabelEncoder()\ndf['category_encoded'] = le.fit_transform(df['category'])\n\ndf.to_csv('label_encoded.csv', index=False)\n] PASSED [ 63%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[target_encoder-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Target encoding (mean encoding)\ntarget_means = df.groupby('category')['target'].mean()\ndf['category_target_encoded'] = df['category'].map(target_means)\n\ndf.to_csv('target_encoded.csv', index=False)\n] PASSED [ 63%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[geo_point_creator-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Create geo point string from lat/lon\ndf['geo_point'] = df.apply(\n    lambda row: f"POINT({row['longitude']} {row['latitude']})",\n    axis=1\n)\n\ndf.to_csv('geo_points.csv', index=False)\n] PASSED [ 64%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[geo_encoder-\nimport pandas as pd\n\ndf = pd.read_csv('addresses.csv')\n\n# Simulated geocoding (in practice, use a geocoding API)\n# This is a placeholder showing the pattern\ndf['latitude'] = 0.0\ndf['longitude'] = 0.0\n\ndf.to_csv('geocoded.csv', index=False)\n] PASSED [ 64%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[geo_distance_calculator-\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('locations.csv')\n\n# Haversine distance calculation\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371  # Earth's radius in km\n    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n    return 2 * R * np.arcsin(np.sqrt(a))\n\ndf['distance_km'] = haversine(\n    df['lat1'], df['lon1'],\n    df['lat2'], df['lon2']\n)\n\ndf.to_csv('distances.csv', index=False)\n] PASSED [ 64%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[array_splitter-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Split array string into rows (explode)\ndf['tags'] = df['tags_string'].str.split(',')\ndf_exploded = df.explode('tags')\n\ndf_exploded.to_csv('array_split.csv', index=False)\n] PASSED [ 64%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[array_joiner-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Join array elements into string\ndf['tags_joined'] = df['tags'].apply(lambda x: ','.join(x) if isinstance(x, list) else x)\n\ndf.to_csv('array_joined.csv', index=False)\n] PASSED [ 64%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[json_flattener-\nimport pandas as pd\nimport json\n\ndf = pd.read_csv('data.csv')\n\n# Flatten JSON column\ndf['json_data'] = df['json_string'].apply(json.loads)\ndf_flat = pd.json_normalize(df['json_data'])\ndf = pd.concat([df.drop('json_data', axis=1), df_flat], axis=1)\n\ndf.to_csv('flattened.csv', index=False)\n] PASSED [ 64%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[json_extractor-\nimport pandas as pd\nimport json\n\ndf = pd.read_csv('data.csv')\n\n# Extract specific JSON fields\ndef extract_json_field(json_str, field):\n    try:\n        data = json.loads(json_str)\n        return data.get(field)\n    except:\n        return None\n\ndf['name'] = df['json_data'].apply(lambda x: extract_json_field(x, 'name'))\ndf['value'] = df['json_data'].apply(lambda x: extract_json_field(x, 'value'))\n\ndf.to_csv('json_extracted.csv', index=False)\n] PASSED [ 64%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_converts_successfully[python_udf-\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Custom Python function\ndef custom_transform(row):\n    if row['type'] == 'A':\n        return row['value'] * 1.1\n    elif row['type'] == 'B':\n        return row['value'] * 0.9\n    else:\n        return row['value']\n\ndf['transformed'] = df.apply(custom_transform, axis=1)\n\ndf.to_csv('udf_result.csv', index=False)\n] PASSED [ 64%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_produces_valid_flow[column_renamer] PASSED [ 65%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_produces_valid_flow[column_copier] PASSED [ 65%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_produces_valid_flow[column_deleter] PASSED [ 65%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_produces_valid_flow[columns_selector] PASSED [ 65%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_produces_valid_flow[column_reorder] PASSED [ 65%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_produces_valid_flow[columns_concatenator] PASSED [ 65%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_produces_valid_flow[fill_empty_with_value] PASSED [ 65%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_produces_valid_flow[remove_rows_on_empty] PASSED [ 65%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_produces_valid_flow[fill_empty_with_previous_next] PASSED [ 65%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExamplesConversion::test_processor_produces_valid_flow[fill_empty_with_computed_value] PASSED [ 66%]
tests/test_py2dataiku/test_processor_examples.py::TestColumnManipulationProcessors::test_column_renamer_has_rename PASSED [ 66%]
tests/test_py2dataiku/test_processor_examples.py::TestColumnManipulationProcessors::test_column_copier_has_copy PASSED [ 66%]
tests/test_py2dataiku/test_processor_examples.py::TestColumnManipulationProcessors::test_column_deleter_has_drop PASSED [ 66%]
tests/test_py2dataiku/test_processor_examples.py::TestColumnManipulationProcessors::test_columns_selector_has_selection PASSED [ 66%]
tests/test_py2dataiku/test_processor_examples.py::TestMissingValueProcessors::test_fill_empty_with_value_has_fillna PASSED [ 66%]
tests/test_py2dataiku/test_processor_examples.py::TestMissingValueProcessors::test_remove_rows_on_empty_has_dropna PASSED [ 66%]
tests/test_py2dataiku/test_processor_examples.py::TestMissingValueProcessors::test_fill_empty_with_previous_next_has_ffill PASSED [ 66%]
tests/test_py2dataiku/test_processor_examples.py::TestStringTransformerProcessors::test_uppercase_has_upper PASSED [ 67%]
tests/test_py2dataiku/test_processor_examples.py::TestStringTransformerProcessors::test_lowercase_has_lower PASSED [ 67%]
tests/test_py2dataiku/test_processor_examples.py::TestStringTransformerProcessors::test_titlecase_has_title PASSED [ 67%]
tests/test_py2dataiku/test_processor_examples.py::TestStringTransformerProcessors::test_trim_has_strip PASSED [ 67%]
tests/test_py2dataiku/test_processor_examples.py::TestStringTransformerProcessors::test_tokenizer_has_split PASSED [ 67%]
tests/test_py2dataiku/test_processor_examples.py::TestStringTransformerProcessors::test_regexp_extractor_has_extract PASSED [ 67%]
tests/test_py2dataiku/test_processor_examples.py::TestStringTransformerProcessors::test_find_replace_has_replace PASSED [ 67%]
tests/test_py2dataiku/test_processor_examples.py::TestNumericTransformerProcessors::test_multiply_has_multiplication PASSED [ 67%]
tests/test_py2dataiku/test_processor_examples.py::TestNumericTransformerProcessors::test_divide_has_division PASSED [ 67%]
tests/test_py2dataiku/test_processor_examples.py::TestNumericTransformerProcessors::test_round_has_round PASSED [ 68%]
tests/test_py2dataiku/test_processor_examples.py::TestNumericTransformerProcessors::test_abs_has_abs PASSED [ 68%]
tests/test_py2dataiku/test_processor_examples.py::TestNumericTransformerProcessors::test_clip_has_clip PASSED [ 68%]
tests/test_py2dataiku/test_processor_examples.py::TestNumericTransformerProcessors::test_binner_has_cut PASSED [ 68%]
tests/test_py2dataiku/test_processor_examples.py::TestTypeConversionProcessors::test_type_setter_has_astype PASSED [ 68%]
tests/test_py2dataiku/test_processor_examples.py::TestTypeConversionProcessors::test_date_parser_has_to_datetime PASSED [ 68%]
tests/test_py2dataiku/test_processor_examples.py::TestTypeConversionProcessors::test_date_formatter_has_strftime PASSED [ 68%]
tests/test_py2dataiku/test_processor_examples.py::TestFilteringProcessors::test_filter_on_value_has_boolean_indexing PASSED [ 68%]
tests/test_py2dataiku/test_processor_examples.py::TestFilteringProcessors::test_filter_on_formula_has_complex_condition PASSED [ 69%]
tests/test_py2dataiku/test_processor_examples.py::TestFilteringProcessors::test_filter_on_date_range_has_date_comparison PASSED [ 69%]
tests/test_py2dataiku/test_processor_examples.py::TestFilteringProcessors::test_filter_on_numeric_range_has_range PASSED [ 69%]
tests/test_py2dataiku/test_processor_examples.py::TestFlaggingProcessors::test_flag_on_value_creates_flag PASSED [ 69%]
tests/test_py2dataiku/test_processor_examples.py::TestFlaggingProcessors::test_flag_on_formula_creates_flag PASSED [ 69%]
tests/test_py2dataiku/test_processor_examples.py::TestRowOperationProcessors::test_remove_duplicates_has_drop_duplicates PASSED [ 69%]
tests/test_py2dataiku/test_processor_examples.py::TestRowOperationProcessors::test_sort_rows_has_sort_values PASSED [ 69%]
tests/test_py2dataiku/test_processor_examples.py::TestRowOperationProcessors::test_sample_rows_has_sample PASSED [ 69%]
tests/test_py2dataiku/test_processor_examples.py::TestComputedColumnProcessors::test_formula_has_arithmetic PASSED [ 69%]
tests/test_py2dataiku/test_processor_examples.py::TestComputedColumnProcessors::test_grel_has_apply PASSED [ 70%]
tests/test_py2dataiku/test_processor_examples.py::TestCategoricalProcessors::test_categorical_encoder_has_dummies PASSED [ 70%]
tests/test_py2dataiku/test_processor_examples.py::TestCategoricalProcessors::test_merge_long_tail_has_value_counts PASSED [ 70%]
tests/test_py2dataiku/test_processor_examples.py::TestGeographicProcessors::test_geo_point_creator_has_point PASSED [ 70%]
tests/test_py2dataiku/test_processor_examples.py::TestArrayJsonProcessors::test_array_splitter_has_explode PASSED [ 70%]
tests/test_py2dataiku/test_processor_examples.py::TestArrayJsonProcessors::test_json_flattener_has_json PASSED [ 70%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorVisualization::test_svg_visualization[column_renamer] PASSED [ 70%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorVisualization::test_svg_visualization[column_copier] PASSED [ 70%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorVisualization::test_svg_visualization[column_deleter] PASSED [ 71%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorVisualization::test_svg_visualization[columns_selector] PASSED [ 71%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorVisualization::test_svg_visualization[column_reorder] PASSED [ 71%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorVisualization::test_svg_visualization[columns_concatenator] PASSED [ 71%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorVisualization::test_svg_visualization[fill_empty_with_value] PASSED [ 71%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorVisualization::test_svg_visualization[remove_rows_on_empty] PASSED [ 71%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorVisualization::test_svg_visualization[fill_empty_with_previous_next] PASSED [ 71%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorVisualization::test_svg_visualization[fill_empty_with_computed_value] PASSED [ 71%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorVisualization::test_mermaid_visualization[column_renamer] PASSED [ 72%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorVisualization::test_mermaid_visualization[column_copier] PASSED [ 72%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorVisualization::test_mermaid_visualization[column_deleter] PASSED [ 72%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorVisualization::test_mermaid_visualization[columns_selector] PASSED [ 72%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorVisualization::test_mermaid_visualization[column_reorder] PASSED [ 72%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorVisualization::test_mermaid_visualization[columns_concatenator] PASSED [ 72%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorVisualization::test_mermaid_visualization[fill_empty_with_value] PASSED [ 72%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorVisualization::test_mermaid_visualization[remove_rows_on_empty] PASSED [ 72%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorVisualization::test_mermaid_visualization[fill_empty_with_previous_next] PASSED [ 72%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorVisualization::test_mermaid_visualization[fill_empty_with_computed_value] PASSED [ 73%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExport::test_to_dict_export[column_renamer] PASSED [ 73%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExport::test_to_dict_export[column_copier] PASSED [ 73%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExport::test_to_dict_export[column_deleter] PASSED [ 73%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExport::test_to_dict_export[columns_selector] PASSED [ 73%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExport::test_to_dict_export[column_reorder] PASSED [ 73%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExport::test_to_dict_export[columns_concatenator] PASSED [ 73%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExport::test_to_dict_export[fill_empty_with_value] PASSED [ 73%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExport::test_to_dict_export[remove_rows_on_empty] PASSED [ 74%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExport::test_to_dict_export[fill_empty_with_previous_next] PASSED [ 74%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExport::test_to_dict_export[fill_empty_with_computed_value] PASSED [ 74%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExport::test_to_json_export[column_renamer] PASSED [ 74%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExport::test_to_json_export[column_copier] PASSED [ 74%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExport::test_to_json_export[column_deleter] PASSED [ 74%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExport::test_to_json_export[columns_selector] PASSED [ 74%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExport::test_to_json_export[column_reorder] PASSED [ 74%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExport::test_to_json_export[columns_concatenator] PASSED [ 74%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExport::test_to_json_export[fill_empty_with_value] PASSED [ 75%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExport::test_to_json_export[remove_rows_on_empty] PASSED [ 75%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExport::test_to_json_export[fill_empty_with_previous_next] PASSED [ 75%]
tests/test_py2dataiku/test_processor_examples.py::TestProcessorExport::test_to_json_export[fill_empty_with_computed_value] PASSED [ 75%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesRegistry::test_recipe_examples_dict_not_empty PASSED [ 75%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesRegistry::test_list_recipe_examples_returns_list PASSED [ 75%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesRegistry::test_all_examples_are_strings PASSED [ 75%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesRegistry::test_get_recipe_example_returns_code PASSED [ 75%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesRegistry::test_get_nonexistent_example_returns_empty PASSED [ 76%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[prepare-\nimport pandas as pd\nimport numpy as np\n\n# Load input data\ndf = pd.read_csv('raw_data.csv')\n\n# String transformations\ndf['name'] = df['name'].str.strip().str.title()\ndf['email'] = df['email'].str.lower()\n\n# Type conversions\ndf['age'] = df['age'].astype(int)\ndf['created_date'] = pd.to_datetime(df['created_date'])\n\n# Fill missing values\ndf['category'] = df['category'].fillna('Unknown')\ndf['score'] = df['score'].fillna(df['score'].mean())\n\n# Numeric transformations\ndf['score_normalized'] = (df['score'] - df['score'].min()) / (df['score'].max() - df['score'].min())\ndf['amount_rounded'] = df['amount'].round(2)\n\n# Filter bad data\ndf = df[df['age'] > 0]\ndf = df.dropna(subset=['customer_id'])\n\n# Save output\ndf.to_csv('cleaned_data.csv', index=False)\n] PASSED [ 76%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[sync-\nimport pandas as pd\n\n# Load source data\nsource_df = pd.read_csv('source_data.csv')\n\n# Simple copy/sync operation\ntarget_df = source_df.copy()\n\n# Save to target\ntarget_df.to_csv('target_data.csv', index=False)\n] PASSED [ 76%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[grouping-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('transactions.csv')\n\n# Group by single column\ncategory_summary = df.groupby('category').agg({\n    'amount': 'sum',\n    'quantity': 'mean',\n    'transaction_id': 'count'\n}).reset_index()\ncategory_summary.columns = ['category', 'total_amount', 'avg_quantity', 'transaction_count']\n\n# Save output\ncategory_summary.to_csv('category_summary.csv', index=False)\n] PASSED [ 76%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[grouping_multi-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('sales.csv')\n\n# Group by multiple columns with multiple aggregations\nregional_product_summary = df.groupby(['region', 'product_category']).agg({\n    'revenue': ['sum', 'mean', 'max', 'min'],\n    'units_sold': 'sum',\n    'customer_id': 'nunique',\n    'discount': 'std'\n}).reset_index()\n\n# Flatten column names\nregional_product_summary.columns = [\n    'region', 'product_category',\n    'total_revenue', 'avg_revenue', 'max_revenue', 'min_revenue',\n    'total_units', 'unique_customers', 'discount_std'\n]\n\n# Save output\nregional_product_summary.to_csv('regional_product_summary.csv', index=False)\n] PASSED [ 76%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[window-\nimport pandas as pd\n\n# Load time series data\ndf = pd.read_csv('daily_metrics.csv')\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values('date')\n\n# Rolling window calculations\ndf['rolling_7d_avg'] = df['value'].rolling(window=7).mean()\ndf['rolling_30d_sum'] = df['value'].rolling(window=30).sum()\ndf['rolling_7d_std'] = df['value'].rolling(window=7).std()\n\n# Cumulative calculations\ndf['cumulative_sum'] = df['value'].cumsum()\ndf['cumulative_max'] = df['value'].cummax()\ndf['cumulative_min'] = df['value'].cummin()\n\n# Save output\ndf.to_csv('metrics_with_windows.csv', index=False)\n] PASSED [ 76%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[window_grouped-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('user_activity.csv')\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values(['user_id', 'date'])\n\n# Window functions per group\ndf['user_running_total'] = df.groupby('user_id')['activity_count'].cumsum()\ndf['user_7d_avg'] = df.groupby('user_id')['activity_count'].transform(\n    lambda x: x.rolling(window=7, min_periods=1).mean()\n)\n\n# Expanding window\ndf['user_expanding_avg'] = df.groupby('user_id')['activity_count'].transform(\n    lambda x: x.expanding().mean()\n)\n\n# Lag and lead\ndf['prev_activity'] = df.groupby('user_id')['activity_count'].shift(1)\ndf['next_activity'] = df.groupby('user_id')['activity_count'].shift(-1)\n\n# Save output\ndf.to_csv('user_activity_windowed.csv', index=False)\n] PASSED [ 76%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[join_inner-\nimport pandas as pd\n\n# Load datasets\ncustomers = pd.read_csv('customers.csv')\norders = pd.read_csv('orders.csv')\n\n# Inner join\ncustomer_orders = pd.merge(\n    customers,\n    orders,\n    on='customer_id',\n    how='inner'\n)\n\n# Save output\ncustomer_orders.to_csv('customer_orders.csv', index=False)\n] PASSED [ 76%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[join_left-\nimport pandas as pd\n\n# Load datasets\ncustomers = pd.read_csv('customers.csv')\norders = pd.read_csv('orders.csv')\n\n# Left join - keep all customers\ncustomer_orders = pd.merge(\n    customers,\n    orders,\n    on='customer_id',\n    how='left'\n)\n\n# Save output\ncustomer_orders.to_csv('customer_orders_left.csv', index=False)\n] PASSED [ 76%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[join_right-\nimport pandas as pd\n\n# Load datasets\ncustomers = pd.read_csv('customers.csv')\norders = pd.read_csv('orders.csv')\n\n# Right join - keep all orders\ncustomer_orders = pd.merge(\n    customers,\n    orders,\n    on='customer_id',\n    how='right'\n)\n\n# Save output\ncustomer_orders.to_csv('customer_orders_right.csv', index=False)\n] PASSED [ 77%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[join_outer-\nimport pandas as pd\n\n# Load datasets\ncustomers = pd.read_csv('customers.csv')\norders = pd.read_csv('orders.csv')\n\n# Outer join - keep all records\ncustomer_orders = pd.merge(\n    customers,\n    orders,\n    on='customer_id',\n    how='outer'\n)\n\n# Save output\ncustomer_orders.to_csv('customer_orders_outer.csv', index=False)\n] PASSED [ 77%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[join_cross-\nimport pandas as pd\n\n# Load datasets\nproducts = pd.read_csv('products.csv')\nregions = pd.read_csv('regions.csv')\n\n# Cross join - all combinations\nproduct_regions = pd.merge(\n    products,\n    regions,\n    how='cross'\n)\n\n# Save output\nproduct_regions.to_csv('product_regions_cross.csv', index=False)\n] PASSED [ 77%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[join_multi_key-\nimport pandas as pd\n\n# Load datasets\nsales = pd.read_csv('sales.csv')\ntargets = pd.read_csv('targets.csv')\n\n# Join on multiple keys\nsales_vs_targets = pd.merge(\n    sales,\n    targets,\n    left_on=['region', 'product_id', 'quarter'],\n    right_on=['region_code', 'sku', 'fiscal_quarter'],\n    how='left'\n)\n\n# Save output\nsales_vs_targets.to_csv('sales_vs_targets.csv', index=False)\n] PASSED [ 77%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[fuzzy_join-\nimport pandas as pd\nfrom fuzzywuzzy import fuzz\n\n# Load datasets\ninternal_companies = pd.read_csv('internal_companies.csv')\nexternal_companies = pd.read_csv('external_companies.csv')\n\n# Fuzzy matching function\ndef fuzzy_match(row, df_to_match, col_name, threshold=80):\n    best_match = None\n    best_score = 0\n    for _, match_row in df_to_match.iterrows():\n        score = fuzz.ratio(str(row[col_name]).lower(), str(match_row[col_name]).lower())\n        if score > best_score and score >= threshold:\n            best_score = score\n            best_match = match_row\n    return best_match\n\n# Apply fuzzy matching\nmatches = []\nfor _, row in internal_companies.iterrows():\n    match = fuzzy_match(row, external_companies, 'company_name')\n    if match is not None:\n        matches.append({\n            'internal_id': row['id'],\n            'internal_name': row['company_name'],\n            'external_id': match['id'],\n            'external_name': match['company_name']\n        })\n\nfuzzy_joined = pd.DataFrame(matches)\nfuzzy_joined.to_csv('fuzzy_matched_companies.csv', index=False)\n] PASSED [ 77%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[geo_join-\nimport pandas as pd\nimport numpy as np\n\n# Load datasets\nstores = pd.read_csv('stores.csv')  # Has latitude, longitude\nregions = pd.read_csv('regions.csv')  # Has polygon boundaries\n\n# Calculate distance between points (Haversine formula)\ndef haversine_distance(lat1, lon1, lat2, lon2):\n    R = 6371  # Earth's radius in kilometers\n    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n# Join stores to nearest city center\ncities = pd.read_csv('city_centers.csv')\nstores_with_city = stores.copy()\nstores_with_city['nearest_city'] = stores.apply(\n    lambda row: cities.loc[\n        cities.apply(\n            lambda city: haversine_distance(\n                row['latitude'], row['longitude'],\n                city['lat'], city['lon']\n            ), axis=1\n        ).idxmin()\n    ]['city_name'], axis=1\n)\n\nstores_with_city.to_csv('stores_with_city.csv', index=False)\n] PASSED [ 77%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[stack-\nimport pandas as pd\n\n# Load multiple datasets\nq1_sales = pd.read_csv('sales_q1.csv')\nq2_sales = pd.read_csv('sales_q2.csv')\nq3_sales = pd.read_csv('sales_q3.csv')\nq4_sales = pd.read_csv('sales_q4.csv')\n\n# Add quarter identifier\nq1_sales['quarter'] = 'Q1'\nq2_sales['quarter'] = 'Q2'\nq3_sales['quarter'] = 'Q3'\nq4_sales['quarter'] = 'Q4'\n\n# Stack all quarters\nall_sales = pd.concat([q1_sales, q2_sales, q3_sales, q4_sales], ignore_index=True)\n\n# Save output\nall_sales.to_csv('all_sales.csv', index=False)\n] PASSED [ 77%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[stack_multi-\nimport pandas as pd\n\n# Load data from multiple sources\nweb_orders = pd.read_csv('web_orders.csv')\nmobile_orders = pd.read_csv('mobile_orders.csv')\nstore_orders = pd.read_csv('store_orders.csv')\n\n# Add source identifier\nweb_orders['source'] = 'web'\nmobile_orders['source'] = 'mobile'\nstore_orders['source'] = 'store'\n\n# Stack with different column alignment\nall_orders = pd.concat(\n    [web_orders, mobile_orders, store_orders],\n    ignore_index=True,\n    sort=False\n)\n\n# Save output\nall_orders.to_csv('all_orders.csv', index=False)\n] PASSED [ 77%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[split-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('all_data.csv')\n\n# Split by condition - filter\nactive_customers = df[df['status'] == 'active']\ninactive_customers = df[df['status'] == 'inactive']\n\n# Save outputs\nactive_customers.to_csv('active_customers.csv', index=False)\ninactive_customers.to_csv('inactive_customers.csv', index=False)\n] PASSED [ 78%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[split_multi-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('transactions.csv')\n\n# Split into multiple groups\nhigh_value = df[df['amount'] >= 1000]\nmedium_value = df[(df['amount'] >= 100) & (df['amount'] < 1000)]\nlow_value = df[df['amount'] < 100]\n\n# Save outputs\nhigh_value.to_csv('high_value_transactions.csv', index=False)\nmedium_value.to_csv('medium_value_transactions.csv', index=False)\nlow_value.to_csv('low_value_transactions.csv', index=False)\n] PASSED [ 78%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[sort-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('products.csv')\n\n# Sort by single column\nsorted_by_price = df.sort_values('price', ascending=True)\n\n# Save output\nsorted_by_price.to_csv('products_sorted.csv', index=False)\n] PASSED [ 78%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[sort_multi-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('employees.csv')\n\n# Sort by multiple columns\nsorted_employees = df.sort_values(\n    by=['department', 'salary', 'hire_date'],\n    ascending=[True, False, True]\n)\n\n# Save output\nsorted_employees.to_csv('employees_sorted.csv', index=False)\n] PASSED [ 78%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[distinct-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('raw_data.csv')\n\n# Remove complete duplicates\nunique_df = df.drop_duplicates()\n\n# Save output\nunique_df.to_csv('unique_data.csv', index=False)\n] PASSED [ 78%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[distinct_subset-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('customer_interactions.csv')\n\n# Remove duplicates based on specific columns (keep first)\nunique_customers = df.drop_duplicates(subset=['customer_id', 'email'], keep='first')\n\n# Remove duplicates keeping last\nlatest_interactions = df.drop_duplicates(subset=['customer_id'], keep='last')\n\n# Save outputs\nunique_customers.to_csv('unique_customers.csv', index=False)\nlatest_interactions.to_csv('latest_interactions.csv', index=False)\n] PASSED [ 78%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[top_n-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('sales.csv')\n\n# Get top 10 by sales amount\ntop_10_sales = df.nlargest(10, 'sales_amount')\n\n# Save output\ntop_10_sales.to_csv('top_10_sales.csv', index=False)\n] PASSED [ 78%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[top_n_grouped-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('products.csv')\n\n# Top N per group\ntop_3_per_category = df.groupby('category').apply(\n    lambda x: x.nlargest(3, 'revenue')\n).reset_index(drop=True)\n\n# Bottom N per group\nbottom_5_per_region = df.groupby('region').apply(\n    lambda x: x.nsmallest(5, 'price')\n).reset_index(drop=True)\n\n# Simple head/tail\nfirst_100 = df.head(100)\nlast_50 = df.tail(50)\n\n# Save outputs\ntop_3_per_category.to_csv('top_3_per_category.csv', index=False)\nbottom_5_per_region.to_csv('bottom_5_per_region.csv', index=False)\n] PASSED [ 78%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[pivot-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('sales.csv')\n\n# Pivot table\npivot_table = df.pivot_table(\n    values='amount',\n    index='product_category',\n    columns='month',\n    aggfunc='sum',\n    fill_value=0\n)\n\n# Reset index for export\npivot_table = pivot_table.reset_index()\n\n# Save output\npivot_table.to_csv('sales_pivot.csv', index=False)\n] PASSED [ 79%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[pivot_melt-\nimport pandas as pd\n\n# Load wide data\nwide_df = pd.read_csv('monthly_metrics.csv')\n\n# Melt (unpivot) from wide to long format\nlong_df = pd.melt(\n    wide_df,\n    id_vars=['product_id', 'product_name'],\n    value_vars=['jan', 'feb', 'mar', 'apr', 'may', 'jun'],\n    var_name='month',\n    value_name='sales'\n)\n\n# Save output\nlong_df.to_csv('monthly_metrics_long.csv', index=False)\n] PASSED [ 79%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[sampling-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('large_dataset.csv')\n\n# Random sample - percentage\nsample_10pct = df.sample(frac=0.1, random_state=42)\n\n# Random sample - fixed size\nsample_1000 = df.sample(n=1000, random_state=42)\n\n# Save outputs\nsample_10pct.to_csv('sample_10pct.csv', index=False)\nsample_1000.to_csv('sample_1000.csv', index=False)\n] PASSED [ 79%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[sampling_stratified-\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load data\ndf = pd.read_csv('customers.csv')\n\n# Stratified sampling\ntrain, test = train_test_split(\n    df,\n    test_size=0.2,\n    stratify=df['segment'],\n    random_state=42\n)\n\n# Save outputs\ntrain.to_csv('train_set.csv', index=False)\ntest.to_csv('test_set.csv', index=False)\n] PASSED [ 79%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[download-\nimport pandas as pd\nimport requests\n\n# Download from URL\nurl = 'https://api.example.com/data.csv'\nresponse = requests.get(url)\n\n# Parse CSV from response\nfrom io import StringIO\ndf = pd.read_csv(StringIO(response.text))\n\n# Save locally\ndf.to_csv('downloaded_data.csv', index=False)\n] PASSED [ 79%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[python-\nimport pandas as pd\nimport numpy as np\n\n# Load input data\ndf = pd.read_csv('input_data.csv')\n\n# Complex custom transformation that can't be done with visual recipes\ndef custom_scoring_algorithm(row):\n    base_score = row['revenue'] * 0.4\n    engagement_bonus = np.log1p(row['interactions']) * 10\n    recency_factor = 1 / (1 + row['days_since_last_purchase'] / 30)\n\n    score = base_score * engagement_bonus * recency_factor\n    return round(score, 2)\n\ndf['customer_score'] = df.apply(custom_scoring_algorithm, axis=1)\n\n# Custom aggregation logic\ndf['segment'] = pd.cut(\n    df['customer_score'],\n    bins=[-np.inf, 50, 150, 300, np.inf],\n    labels=['Bronze', 'Silver', 'Gold', 'Platinum']\n)\n\n# Save output\ndf.to_csv('scored_customers.csv', index=False)\n] PASSED [ 79%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[sql-\nimport pandas as pd\nimport sqlite3\n\n# Create in-memory database\nconn = sqlite3.connect(':memory:')\n\n# Load data into database\ncustomers = pd.read_csv('customers.csv')\norders = pd.read_csv('orders.csv')\n\ncustomers.to_sql('customers', conn, index=False)\norders.to_sql('orders', conn, index=False)\n\n# Execute SQL query\nquery = '''\nSELECT\n    c.customer_id,\n    c.name,\n    COUNT(o.order_id) as order_count,\n    SUM(o.amount) as total_spend,\n    AVG(o.amount) as avg_order_value\nFROM customers c\nLEFT JOIN orders o ON c.customer_id = o.customer_id\nGROUP BY c.customer_id, c.name\nHAVING COUNT(o.order_id) > 0\nORDER BY total_spend DESC\n'''\n\nresult = pd.read_sql(query, conn)\nconn.close()\n\n# Save output\nresult.to_csv('customer_order_summary.csv', index=False)\n] PASSED [ 79%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[r-\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\n# Load data (equivalent to R's read.csv)\ndf = pd.read_csv('data.csv')\n\n# Statistical analysis (R-style)\n# Linear regression\nslope, intercept, r_value, p_value, std_err = stats.linregress(df['x'], df['y'])\n\n# Add regression results\ndf['predicted'] = intercept + slope * df['x']\ndf['residual'] = df['y'] - df['predicted']\n\n# Save output\ndf.to_csv('regression_results.csv', index=False)\n] PASSED [ 79%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[prediction_scoring-\nimport pandas as pd\nimport pickle\n\n# Load test data\ntest_data = pd.read_csv('test_data.csv')\n\n# Load trained model\nwith open('trained_model.pkl', 'rb') as f:\n    model = pickle.load(f)\n\n# Prepare features\nfeatures = test_data[['feature1', 'feature2', 'feature3', 'feature4']]\n\n# Make predictions\ntest_data['prediction'] = model.predict(features)\ntest_data['probability'] = model.predict_proba(features)[:, 1]\n\n# Save output\ntest_data.to_csv('predictions.csv', index=False)\n] PASSED [ 79%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[clustering_scoring-\nimport pandas as pd\nimport pickle\n\n# Load data\ndf = pd.read_csv('customer_data.csv')\n\n# Load trained clustering model\nwith open('kmeans_model.pkl', 'rb') as f:\n    kmeans = pickle.load(f)\n\n# Prepare features\nfeatures = df[['recency', 'frequency', 'monetary']]\n\n# Assign cluster labels\ndf['cluster'] = kmeans.predict(features)\n\n# Calculate distance to cluster centers\ndf['distance_to_center'] = kmeans.transform(features).min(axis=1)\n\n# Save output\ndf.to_csv('clustered_customers.csv', index=False)\n] PASSED [ 80%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[evaluation-\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Load predictions and actuals\ndf = pd.read_csv('predictions.csv')\n\n# Extract predictions and actuals\ny_true = df['actual']\ny_pred = df['prediction']\n\n# Calculate metrics\nmetrics = {\n    'accuracy': accuracy_score(y_true, y_pred),\n    'precision': precision_score(y_true, y_pred, average='weighted'),\n    'recall': recall_score(y_true, y_pred, average='weighted'),\n    'f1': f1_score(y_true, y_pred, average='weighted')\n}\n\n# Create evaluation report\neval_df = pd.DataFrame([metrics])\neval_df.to_csv('model_evaluation.csv', index=False)\n\n# Confusion matrix\ncm = confusion_matrix(y_true, y_pred)\ncm_df = pd.DataFrame(cm)\ncm_df.to_csv('confusion_matrix.csv', index=False)\n] PASSED [ 80%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[upsert-\nimport pandas as pd\n\n# Load existing and new data\nexisting = pd.read_csv('existing_records.csv')\nupdates = pd.read_csv('new_records.csv')\n\n# Upsert logic: update existing records or insert new ones\nkey_column = 'record_id'\n\n# Get records to update (exist in both)\nto_update = updates[updates[key_column].isin(existing[key_column])]\n\n# Get records to insert (only in updates)\nto_insert = updates[~updates[key_column].isin(existing[key_column])]\n\n# Get unchanged records\nunchanged = existing[~existing[key_column].isin(updates[key_column])]\n\n# Combine: unchanged + updated + new\nresult = pd.concat([unchanged, to_update, to_insert], ignore_index=True)\n\n# Save output\nresult.to_csv('upserted_records.csv', index=False)\n] PASSED [ 80%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[generate_features-\nimport pandas as pd\nimport numpy as np\n\n# Load data\ndf = pd.read_csv('raw_features.csv')\n\n# Generate date features\ndf['date'] = pd.to_datetime(df['date'])\ndf['year'] = df['date'].dt.year\ndf['month'] = df['date'].dt.month\ndf['day'] = df['date'].dt.day\ndf['day_of_week'] = df['date'].dt.dayofweek\ndf['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\ndf['quarter'] = df['date'].dt.quarter\n\n# Generate numeric features\ndf['amount_log'] = np.log1p(df['amount'])\ndf['amount_squared'] = df['amount'] ** 2\ndf['amount_sqrt'] = np.sqrt(df['amount'].clip(lower=0))\n\n# Generate interaction features\ndf['amount_x_quantity'] = df['amount'] * df['quantity']\ndf['price_per_unit'] = df['amount'] / df['quantity'].replace(0, 1)\n\n# Save output\ndf.to_csv('generated_features.csv', index=False)\n] PASSED [ 80%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[pyspark-\nimport pandas as pd\n\n# Note: In Dataiku, this would run as a PySpark recipe\n# Here we show the pandas equivalent\n\n# Load large dataset\ndf = pd.read_csv('large_data.csv')\n\n# Transformations that would benefit from Spark\ndf['processed'] = df['text'].str.lower()\ngrouped = df.groupby(['category', 'region']).agg({\n    'amount': 'sum',\n    'count': 'sum'\n}).reset_index()\n\n# Save output\ngrouped.to_csv('spark_processed.csv', index=False)\n] PASSED [ 80%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_converts_successfully[hive-\nimport pandas as pd\n\n# Load data (simulating Hive table read)\nfact_sales = pd.read_csv('fact_sales.csv')\ndim_product = pd.read_csv('dim_product.csv')\ndim_date = pd.read_csv('dim_date.csv')\n\n# Join facts with dimensions (star schema query)\nresult = fact_sales.merge(\n    dim_product, on='product_id'\n).merge(\n    dim_date, on='date_id'\n)\n\n# Aggregate\nsummary = result.groupby(['year', 'month', 'category']).agg({\n    'amount': 'sum',\n    'quantity': 'sum'\n}).reset_index()\n\n# Save output\nsummary.to_csv('hive_result.csv', index=False)\n] PASSED [ 80%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[prepare-\nimport pandas as pd\nimport numpy as np\n\n# Load input data\ndf = pd.read_csv('raw_data.csv')\n\n# String transformations\ndf['name'] = df['name'].str.strip().str.title()\ndf['email'] = df['email'].str.lower()\n\n# Type conversions\ndf['age'] = df['age'].astype(int)\ndf['created_date'] = pd.to_datetime(df['created_date'])\n\n# Fill missing values\ndf['category'] = df['category'].fillna('Unknown')\ndf['score'] = df['score'].fillna(df['score'].mean())\n\n# Numeric transformations\ndf['score_normalized'] = (df['score'] - df['score'].min()) / (df['score'].max() - df['score'].min())\ndf['amount_rounded'] = df['amount'].round(2)\n\n# Filter bad data\ndf = df[df['age'] > 0]\ndf = df.dropna(subset=['customer_id'])\n\n# Save output\ndf.to_csv('cleaned_data.csv', index=False)\n] PASSED [ 80%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[sync-\nimport pandas as pd\n\n# Load source data\nsource_df = pd.read_csv('source_data.csv')\n\n# Simple copy/sync operation\ntarget_df = source_df.copy()\n\n# Save to target\ntarget_df.to_csv('target_data.csv', index=False)\n] PASSED [ 80%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[grouping-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('transactions.csv')\n\n# Group by single column\ncategory_summary = df.groupby('category').agg({\n    'amount': 'sum',\n    'quantity': 'mean',\n    'transaction_id': 'count'\n}).reset_index()\ncategory_summary.columns = ['category', 'total_amount', 'avg_quantity', 'transaction_count']\n\n# Save output\ncategory_summary.to_csv('category_summary.csv', index=False)\n] PASSED [ 81%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[grouping_multi-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('sales.csv')\n\n# Group by multiple columns with multiple aggregations\nregional_product_summary = df.groupby(['region', 'product_category']).agg({\n    'revenue': ['sum', 'mean', 'max', 'min'],\n    'units_sold': 'sum',\n    'customer_id': 'nunique',\n    'discount': 'std'\n}).reset_index()\n\n# Flatten column names\nregional_product_summary.columns = [\n    'region', 'product_category',\n    'total_revenue', 'avg_revenue', 'max_revenue', 'min_revenue',\n    'total_units', 'unique_customers', 'discount_std'\n]\n\n# Save output\nregional_product_summary.to_csv('regional_product_summary.csv', index=False)\n] PASSED [ 81%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[window-\nimport pandas as pd\n\n# Load time series data\ndf = pd.read_csv('daily_metrics.csv')\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values('date')\n\n# Rolling window calculations\ndf['rolling_7d_avg'] = df['value'].rolling(window=7).mean()\ndf['rolling_30d_sum'] = df['value'].rolling(window=30).sum()\ndf['rolling_7d_std'] = df['value'].rolling(window=7).std()\n\n# Cumulative calculations\ndf['cumulative_sum'] = df['value'].cumsum()\ndf['cumulative_max'] = df['value'].cummax()\ndf['cumulative_min'] = df['value'].cummin()\n\n# Save output\ndf.to_csv('metrics_with_windows.csv', index=False)\n] PASSED [ 81%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[window_grouped-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('user_activity.csv')\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values(['user_id', 'date'])\n\n# Window functions per group\ndf['user_running_total'] = df.groupby('user_id')['activity_count'].cumsum()\ndf['user_7d_avg'] = df.groupby('user_id')['activity_count'].transform(\n    lambda x: x.rolling(window=7, min_periods=1).mean()\n)\n\n# Expanding window\ndf['user_expanding_avg'] = df.groupby('user_id')['activity_count'].transform(\n    lambda x: x.expanding().mean()\n)\n\n# Lag and lead\ndf['prev_activity'] = df.groupby('user_id')['activity_count'].shift(1)\ndf['next_activity'] = df.groupby('user_id')['activity_count'].shift(-1)\n\n# Save output\ndf.to_csv('user_activity_windowed.csv', index=False)\n] PASSED [ 81%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[join_inner-\nimport pandas as pd\n\n# Load datasets\ncustomers = pd.read_csv('customers.csv')\norders = pd.read_csv('orders.csv')\n\n# Inner join\ncustomer_orders = pd.merge(\n    customers,\n    orders,\n    on='customer_id',\n    how='inner'\n)\n\n# Save output\ncustomer_orders.to_csv('customer_orders.csv', index=False)\n] PASSED [ 81%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[join_left-\nimport pandas as pd\n\n# Load datasets\ncustomers = pd.read_csv('customers.csv')\norders = pd.read_csv('orders.csv')\n\n# Left join - keep all customers\ncustomer_orders = pd.merge(\n    customers,\n    orders,\n    on='customer_id',\n    how='left'\n)\n\n# Save output\ncustomer_orders.to_csv('customer_orders_left.csv', index=False)\n] PASSED [ 81%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[join_right-\nimport pandas as pd\n\n# Load datasets\ncustomers = pd.read_csv('customers.csv')\norders = pd.read_csv('orders.csv')\n\n# Right join - keep all orders\ncustomer_orders = pd.merge(\n    customers,\n    orders,\n    on='customer_id',\n    how='right'\n)\n\n# Save output\ncustomer_orders.to_csv('customer_orders_right.csv', index=False)\n] PASSED [ 81%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[join_outer-\nimport pandas as pd\n\n# Load datasets\ncustomers = pd.read_csv('customers.csv')\norders = pd.read_csv('orders.csv')\n\n# Outer join - keep all records\ncustomer_orders = pd.merge(\n    customers,\n    orders,\n    on='customer_id',\n    how='outer'\n)\n\n# Save output\ncustomer_orders.to_csv('customer_orders_outer.csv', index=False)\n] PASSED [ 81%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[join_cross-\nimport pandas as pd\n\n# Load datasets\nproducts = pd.read_csv('products.csv')\nregions = pd.read_csv('regions.csv')\n\n# Cross join - all combinations\nproduct_regions = pd.merge(\n    products,\n    regions,\n    how='cross'\n)\n\n# Save output\nproduct_regions.to_csv('product_regions_cross.csv', index=False)\n] PASSED [ 81%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[join_multi_key-\nimport pandas as pd\n\n# Load datasets\nsales = pd.read_csv('sales.csv')\ntargets = pd.read_csv('targets.csv')\n\n# Join on multiple keys\nsales_vs_targets = pd.merge(\n    sales,\n    targets,\n    left_on=['region', 'product_id', 'quarter'],\n    right_on=['region_code', 'sku', 'fiscal_quarter'],\n    how='left'\n)\n\n# Save output\nsales_vs_targets.to_csv('sales_vs_targets.csv', index=False)\n] PASSED [ 82%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[fuzzy_join-\nimport pandas as pd\nfrom fuzzywuzzy import fuzz\n\n# Load datasets\ninternal_companies = pd.read_csv('internal_companies.csv')\nexternal_companies = pd.read_csv('external_companies.csv')\n\n# Fuzzy matching function\ndef fuzzy_match(row, df_to_match, col_name, threshold=80):\n    best_match = None\n    best_score = 0\n    for _, match_row in df_to_match.iterrows():\n        score = fuzz.ratio(str(row[col_name]).lower(), str(match_row[col_name]).lower())\n        if score > best_score and score >= threshold:\n            best_score = score\n            best_match = match_row\n    return best_match\n\n# Apply fuzzy matching\nmatches = []\nfor _, row in internal_companies.iterrows():\n    match = fuzzy_match(row, external_companies, 'company_name')\n    if match is not None:\n        matches.append({\n            'internal_id': row['id'],\n            'internal_name': row['company_name'],\n            'external_id': match['id'],\n            'external_name': match['company_name']\n        })\n\nfuzzy_joined = pd.DataFrame(matches)\nfuzzy_joined.to_csv('fuzzy_matched_companies.csv', index=False)\n] PASSED [ 82%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[geo_join-\nimport pandas as pd\nimport numpy as np\n\n# Load datasets\nstores = pd.read_csv('stores.csv')  # Has latitude, longitude\nregions = pd.read_csv('regions.csv')  # Has polygon boundaries\n\n# Calculate distance between points (Haversine formula)\ndef haversine_distance(lat1, lon1, lat2, lon2):\n    R = 6371  # Earth's radius in kilometers\n    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n# Join stores to nearest city center\ncities = pd.read_csv('city_centers.csv')\nstores_with_city = stores.copy()\nstores_with_city['nearest_city'] = stores.apply(\n    lambda row: cities.loc[\n        cities.apply(\n            lambda city: haversine_distance(\n                row['latitude'], row['longitude'],\n                city['lat'], city['lon']\n            ), axis=1\n        ).idxmin()\n    ]['city_name'], axis=1\n)\n\nstores_with_city.to_csv('stores_with_city.csv', index=False)\n] PASSED [ 82%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[stack-\nimport pandas as pd\n\n# Load multiple datasets\nq1_sales = pd.read_csv('sales_q1.csv')\nq2_sales = pd.read_csv('sales_q2.csv')\nq3_sales = pd.read_csv('sales_q3.csv')\nq4_sales = pd.read_csv('sales_q4.csv')\n\n# Add quarter identifier\nq1_sales['quarter'] = 'Q1'\nq2_sales['quarter'] = 'Q2'\nq3_sales['quarter'] = 'Q3'\nq4_sales['quarter'] = 'Q4'\n\n# Stack all quarters\nall_sales = pd.concat([q1_sales, q2_sales, q3_sales, q4_sales], ignore_index=True)\n\n# Save output\nall_sales.to_csv('all_sales.csv', index=False)\n] PASSED [ 82%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[stack_multi-\nimport pandas as pd\n\n# Load data from multiple sources\nweb_orders = pd.read_csv('web_orders.csv')\nmobile_orders = pd.read_csv('mobile_orders.csv')\nstore_orders = pd.read_csv('store_orders.csv')\n\n# Add source identifier\nweb_orders['source'] = 'web'\nmobile_orders['source'] = 'mobile'\nstore_orders['source'] = 'store'\n\n# Stack with different column alignment\nall_orders = pd.concat(\n    [web_orders, mobile_orders, store_orders],\n    ignore_index=True,\n    sort=False\n)\n\n# Save output\nall_orders.to_csv('all_orders.csv', index=False)\n] PASSED [ 82%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[split-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('all_data.csv')\n\n# Split by condition - filter\nactive_customers = df[df['status'] == 'active']\ninactive_customers = df[df['status'] == 'inactive']\n\n# Save outputs\nactive_customers.to_csv('active_customers.csv', index=False)\ninactive_customers.to_csv('inactive_customers.csv', index=False)\n] PASSED [ 82%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[split_multi-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('transactions.csv')\n\n# Split into multiple groups\nhigh_value = df[df['amount'] >= 1000]\nmedium_value = df[(df['amount'] >= 100) & (df['amount'] < 1000)]\nlow_value = df[df['amount'] < 100]\n\n# Save outputs\nhigh_value.to_csv('high_value_transactions.csv', index=False)\nmedium_value.to_csv('medium_value_transactions.csv', index=False)\nlow_value.to_csv('low_value_transactions.csv', index=False)\n] PASSED [ 82%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[sort-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('products.csv')\n\n# Sort by single column\nsorted_by_price = df.sort_values('price', ascending=True)\n\n# Save output\nsorted_by_price.to_csv('products_sorted.csv', index=False)\n] PASSED [ 82%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[sort_multi-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('employees.csv')\n\n# Sort by multiple columns\nsorted_employees = df.sort_values(\n    by=['department', 'salary', 'hire_date'],\n    ascending=[True, False, True]\n)\n\n# Save output\nsorted_employees.to_csv('employees_sorted.csv', index=False)\n] PASSED [ 83%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[distinct-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('raw_data.csv')\n\n# Remove complete duplicates\nunique_df = df.drop_duplicates()\n\n# Save output\nunique_df.to_csv('unique_data.csv', index=False)\n] PASSED [ 83%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[distinct_subset-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('customer_interactions.csv')\n\n# Remove duplicates based on specific columns (keep first)\nunique_customers = df.drop_duplicates(subset=['customer_id', 'email'], keep='first')\n\n# Remove duplicates keeping last\nlatest_interactions = df.drop_duplicates(subset=['customer_id'], keep='last')\n\n# Save outputs\nunique_customers.to_csv('unique_customers.csv', index=False)\nlatest_interactions.to_csv('latest_interactions.csv', index=False)\n] PASSED [ 83%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[top_n-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('sales.csv')\n\n# Get top 10 by sales amount\ntop_10_sales = df.nlargest(10, 'sales_amount')\n\n# Save output\ntop_10_sales.to_csv('top_10_sales.csv', index=False)\n] PASSED [ 83%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[top_n_grouped-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('products.csv')\n\n# Top N per group\ntop_3_per_category = df.groupby('category').apply(\n    lambda x: x.nlargest(3, 'revenue')\n).reset_index(drop=True)\n\n# Bottom N per group\nbottom_5_per_region = df.groupby('region').apply(\n    lambda x: x.nsmallest(5, 'price')\n).reset_index(drop=True)\n\n# Simple head/tail\nfirst_100 = df.head(100)\nlast_50 = df.tail(50)\n\n# Save outputs\ntop_3_per_category.to_csv('top_3_per_category.csv', index=False)\nbottom_5_per_region.to_csv('bottom_5_per_region.csv', index=False)\n] PASSED [ 83%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[pivot-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('sales.csv')\n\n# Pivot table\npivot_table = df.pivot_table(\n    values='amount',\n    index='product_category',\n    columns='month',\n    aggfunc='sum',\n    fill_value=0\n)\n\n# Reset index for export\npivot_table = pivot_table.reset_index()\n\n# Save output\npivot_table.to_csv('sales_pivot.csv', index=False)\n] PASSED [ 83%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[pivot_melt-\nimport pandas as pd\n\n# Load wide data\nwide_df = pd.read_csv('monthly_metrics.csv')\n\n# Melt (unpivot) from wide to long format\nlong_df = pd.melt(\n    wide_df,\n    id_vars=['product_id', 'product_name'],\n    value_vars=['jan', 'feb', 'mar', 'apr', 'may', 'jun'],\n    var_name='month',\n    value_name='sales'\n)\n\n# Save output\nlong_df.to_csv('monthly_metrics_long.csv', index=False)\n] PASSED [ 83%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[sampling-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('large_dataset.csv')\n\n# Random sample - percentage\nsample_10pct = df.sample(frac=0.1, random_state=42)\n\n# Random sample - fixed size\nsample_1000 = df.sample(n=1000, random_state=42)\n\n# Save outputs\nsample_10pct.to_csv('sample_10pct.csv', index=False)\nsample_1000.to_csv('sample_1000.csv', index=False)\n] PASSED [ 83%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[sampling_stratified-\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load data\ndf = pd.read_csv('customers.csv')\n\n# Stratified sampling\ntrain, test = train_test_split(\n    df,\n    test_size=0.2,\n    stratify=df['segment'],\n    random_state=42\n)\n\n# Save outputs\ntrain.to_csv('train_set.csv', index=False)\ntest.to_csv('test_set.csv', index=False)\n] PASSED [ 83%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[download-\nimport pandas as pd\nimport requests\n\n# Download from URL\nurl = 'https://api.example.com/data.csv'\nresponse = requests.get(url)\n\n# Parse CSV from response\nfrom io import StringIO\ndf = pd.read_csv(StringIO(response.text))\n\n# Save locally\ndf.to_csv('downloaded_data.csv', index=False)\n] PASSED [ 84%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[python-\nimport pandas as pd\nimport numpy as np\n\n# Load input data\ndf = pd.read_csv('input_data.csv')\n\n# Complex custom transformation that can't be done with visual recipes\ndef custom_scoring_algorithm(row):\n    base_score = row['revenue'] * 0.4\n    engagement_bonus = np.log1p(row['interactions']) * 10\n    recency_factor = 1 / (1 + row['days_since_last_purchase'] / 30)\n\n    score = base_score * engagement_bonus * recency_factor\n    return round(score, 2)\n\ndf['customer_score'] = df.apply(custom_scoring_algorithm, axis=1)\n\n# Custom aggregation logic\ndf['segment'] = pd.cut(\n    df['customer_score'],\n    bins=[-np.inf, 50, 150, 300, np.inf],\n    labels=['Bronze', 'Silver', 'Gold', 'Platinum']\n)\n\n# Save output\ndf.to_csv('scored_customers.csv', index=False)\n] PASSED [ 84%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[sql-\nimport pandas as pd\nimport sqlite3\n\n# Create in-memory database\nconn = sqlite3.connect(':memory:')\n\n# Load data into database\ncustomers = pd.read_csv('customers.csv')\norders = pd.read_csv('orders.csv')\n\ncustomers.to_sql('customers', conn, index=False)\norders.to_sql('orders', conn, index=False)\n\n# Execute SQL query\nquery = '''\nSELECT\n    c.customer_id,\n    c.name,\n    COUNT(o.order_id) as order_count,\n    SUM(o.amount) as total_spend,\n    AVG(o.amount) as avg_order_value\nFROM customers c\nLEFT JOIN orders o ON c.customer_id = o.customer_id\nGROUP BY c.customer_id, c.name\nHAVING COUNT(o.order_id) > 0\nORDER BY total_spend DESC\n'''\n\nresult = pd.read_sql(query, conn)\nconn.close()\n\n# Save output\nresult.to_csv('customer_order_summary.csv', index=False)\n] PASSED [ 84%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[r-\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\n# Load data (equivalent to R's read.csv)\ndf = pd.read_csv('data.csv')\n\n# Statistical analysis (R-style)\n# Linear regression\nslope, intercept, r_value, p_value, std_err = stats.linregress(df['x'], df['y'])\n\n# Add regression results\ndf['predicted'] = intercept + slope * df['x']\ndf['residual'] = df['y'] - df['predicted']\n\n# Save output\ndf.to_csv('regression_results.csv', index=False)\n] PASSED [ 84%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[prediction_scoring-\nimport pandas as pd\nimport pickle\n\n# Load test data\ntest_data = pd.read_csv('test_data.csv')\n\n# Load trained model\nwith open('trained_model.pkl', 'rb') as f:\n    model = pickle.load(f)\n\n# Prepare features\nfeatures = test_data[['feature1', 'feature2', 'feature3', 'feature4']]\n\n# Make predictions\ntest_data['prediction'] = model.predict(features)\ntest_data['probability'] = model.predict_proba(features)[:, 1]\n\n# Save output\ntest_data.to_csv('predictions.csv', index=False)\n] PASSED [ 84%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[clustering_scoring-\nimport pandas as pd\nimport pickle\n\n# Load data\ndf = pd.read_csv('customer_data.csv')\n\n# Load trained clustering model\nwith open('kmeans_model.pkl', 'rb') as f:\n    kmeans = pickle.load(f)\n\n# Prepare features\nfeatures = df[['recency', 'frequency', 'monetary']]\n\n# Assign cluster labels\ndf['cluster'] = kmeans.predict(features)\n\n# Calculate distance to cluster centers\ndf['distance_to_center'] = kmeans.transform(features).min(axis=1)\n\n# Save output\ndf.to_csv('clustered_customers.csv', index=False)\n] PASSED [ 84%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[evaluation-\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Load predictions and actuals\ndf = pd.read_csv('predictions.csv')\n\n# Extract predictions and actuals\ny_true = df['actual']\ny_pred = df['prediction']\n\n# Calculate metrics\nmetrics = {\n    'accuracy': accuracy_score(y_true, y_pred),\n    'precision': precision_score(y_true, y_pred, average='weighted'),\n    'recall': recall_score(y_true, y_pred, average='weighted'),\n    'f1': f1_score(y_true, y_pred, average='weighted')\n}\n\n# Create evaluation report\neval_df = pd.DataFrame([metrics])\neval_df.to_csv('model_evaluation.csv', index=False)\n\n# Confusion matrix\ncm = confusion_matrix(y_true, y_pred)\ncm_df = pd.DataFrame(cm)\ncm_df.to_csv('confusion_matrix.csv', index=False)\n] PASSED [ 84%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[upsert-\nimport pandas as pd\n\n# Load existing and new data\nexisting = pd.read_csv('existing_records.csv')\nupdates = pd.read_csv('new_records.csv')\n\n# Upsert logic: update existing records or insert new ones\nkey_column = 'record_id'\n\n# Get records to update (exist in both)\nto_update = updates[updates[key_column].isin(existing[key_column])]\n\n# Get records to insert (only in updates)\nto_insert = updates[~updates[key_column].isin(existing[key_column])]\n\n# Get unchanged records\nunchanged = existing[~existing[key_column].isin(updates[key_column])]\n\n# Combine: unchanged + updated + new\nresult = pd.concat([unchanged, to_update, to_insert], ignore_index=True)\n\n# Save output\nresult.to_csv('upserted_records.csv', index=False)\n] PASSED [ 84%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[generate_features-\nimport pandas as pd\nimport numpy as np\n\n# Load data\ndf = pd.read_csv('raw_features.csv')\n\n# Generate date features\ndf['date'] = pd.to_datetime(df['date'])\ndf['year'] = df['date'].dt.year\ndf['month'] = df['date'].dt.month\ndf['day'] = df['date'].dt.day\ndf['day_of_week'] = df['date'].dt.dayofweek\ndf['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\ndf['quarter'] = df['date'].dt.quarter\n\n# Generate numeric features\ndf['amount_log'] = np.log1p(df['amount'])\ndf['amount_squared'] = df['amount'] ** 2\ndf['amount_sqrt'] = np.sqrt(df['amount'].clip(lower=0))\n\n# Generate interaction features\ndf['amount_x_quantity'] = df['amount'] * df['quantity']\ndf['price_per_unit'] = df['amount'] / df['quantity'].replace(0, 1)\n\n# Save output\ndf.to_csv('generated_features.csv', index=False)\n] PASSED [ 85%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[pyspark-\nimport pandas as pd\n\n# Note: In Dataiku, this would run as a PySpark recipe\n# Here we show the pandas equivalent\n\n# Load large dataset\ndf = pd.read_csv('large_data.csv')\n\n# Transformations that would benefit from Spark\ndf['processed'] = df['text'].str.lower()\ngrouped = df.groupby(['category', 'region']).agg({\n    'amount': 'sum',\n    'count': 'sum'\n}).reset_index()\n\n# Save output\ngrouped.to_csv('spark_processed.csv', index=False)\n] PASSED [ 85%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_recipes[hive-\nimport pandas as pd\n\n# Load data (simulating Hive table read)\nfact_sales = pd.read_csv('fact_sales.csv')\ndim_product = pd.read_csv('dim_product.csv')\ndim_date = pd.read_csv('dim_date.csv')\n\n# Join facts with dimensions (star schema query)\nresult = fact_sales.merge(\n    dim_product, on='product_id'\n).merge(\n    dim_date, on='date_id'\n)\n\n# Aggregate\nsummary = result.groupby(['year', 'month', 'category']).agg({\n    'amount': 'sum',\n    'quantity': 'sum'\n}).reset_index()\n\n# Save output\nsummary.to_csv('hive_result.csv', index=False)\n] PASSED [ 85%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[prepare-\nimport pandas as pd\nimport numpy as np\n\n# Load input data\ndf = pd.read_csv('raw_data.csv')\n\n# String transformations\ndf['name'] = df['name'].str.strip().str.title()\ndf['email'] = df['email'].str.lower()\n\n# Type conversions\ndf['age'] = df['age'].astype(int)\ndf['created_date'] = pd.to_datetime(df['created_date'])\n\n# Fill missing values\ndf['category'] = df['category'].fillna('Unknown')\ndf['score'] = df['score'].fillna(df['score'].mean())\n\n# Numeric transformations\ndf['score_normalized'] = (df['score'] - df['score'].min()) / (df['score'].max() - df['score'].min())\ndf['amount_rounded'] = df['amount'].round(2)\n\n# Filter bad data\ndf = df[df['age'] > 0]\ndf = df.dropna(subset=['customer_id'])\n\n# Save output\ndf.to_csv('cleaned_data.csv', index=False)\n] PASSED [ 85%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[sync-\nimport pandas as pd\n\n# Load source data\nsource_df = pd.read_csv('source_data.csv')\n\n# Simple copy/sync operation\ntarget_df = source_df.copy()\n\n# Save to target\ntarget_df.to_csv('target_data.csv', index=False)\n] PASSED [ 85%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[grouping-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('transactions.csv')\n\n# Group by single column\ncategory_summary = df.groupby('category').agg({\n    'amount': 'sum',\n    'quantity': 'mean',\n    'transaction_id': 'count'\n}).reset_index()\ncategory_summary.columns = ['category', 'total_amount', 'avg_quantity', 'transaction_count']\n\n# Save output\ncategory_summary.to_csv('category_summary.csv', index=False)\n] PASSED [ 85%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[grouping_multi-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('sales.csv')\n\n# Group by multiple columns with multiple aggregations\nregional_product_summary = df.groupby(['region', 'product_category']).agg({\n    'revenue': ['sum', 'mean', 'max', 'min'],\n    'units_sold': 'sum',\n    'customer_id': 'nunique',\n    'discount': 'std'\n}).reset_index()\n\n# Flatten column names\nregional_product_summary.columns = [\n    'region', 'product_category',\n    'total_revenue', 'avg_revenue', 'max_revenue', 'min_revenue',\n    'total_units', 'unique_customers', 'discount_std'\n]\n\n# Save output\nregional_product_summary.to_csv('regional_product_summary.csv', index=False)\n] PASSED [ 85%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[window-\nimport pandas as pd\n\n# Load time series data\ndf = pd.read_csv('daily_metrics.csv')\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values('date')\n\n# Rolling window calculations\ndf['rolling_7d_avg'] = df['value'].rolling(window=7).mean()\ndf['rolling_30d_sum'] = df['value'].rolling(window=30).sum()\ndf['rolling_7d_std'] = df['value'].rolling(window=7).std()\n\n# Cumulative calculations\ndf['cumulative_sum'] = df['value'].cumsum()\ndf['cumulative_max'] = df['value'].cummax()\ndf['cumulative_min'] = df['value'].cummin()\n\n# Save output\ndf.to_csv('metrics_with_windows.csv', index=False)\n] PASSED [ 85%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[window_grouped-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('user_activity.csv')\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values(['user_id', 'date'])\n\n# Window functions per group\ndf['user_running_total'] = df.groupby('user_id')['activity_count'].cumsum()\ndf['user_7d_avg'] = df.groupby('user_id')['activity_count'].transform(\n    lambda x: x.rolling(window=7, min_periods=1).mean()\n)\n\n# Expanding window\ndf['user_expanding_avg'] = df.groupby('user_id')['activity_count'].transform(\n    lambda x: x.expanding().mean()\n)\n\n# Lag and lead\ndf['prev_activity'] = df.groupby('user_id')['activity_count'].shift(1)\ndf['next_activity'] = df.groupby('user_id')['activity_count'].shift(-1)\n\n# Save output\ndf.to_csv('user_activity_windowed.csv', index=False)\n] PASSED [ 86%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[join_inner-\nimport pandas as pd\n\n# Load datasets\ncustomers = pd.read_csv('customers.csv')\norders = pd.read_csv('orders.csv')\n\n# Inner join\ncustomer_orders = pd.merge(\n    customers,\n    orders,\n    on='customer_id',\n    how='inner'\n)\n\n# Save output\ncustomer_orders.to_csv('customer_orders.csv', index=False)\n] PASSED [ 86%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[join_left-\nimport pandas as pd\n\n# Load datasets\ncustomers = pd.read_csv('customers.csv')\norders = pd.read_csv('orders.csv')\n\n# Left join - keep all customers\ncustomer_orders = pd.merge(\n    customers,\n    orders,\n    on='customer_id',\n    how='left'\n)\n\n# Save output\ncustomer_orders.to_csv('customer_orders_left.csv', index=False)\n] PASSED [ 86%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[join_right-\nimport pandas as pd\n\n# Load datasets\ncustomers = pd.read_csv('customers.csv')\norders = pd.read_csv('orders.csv')\n\n# Right join - keep all orders\ncustomer_orders = pd.merge(\n    customers,\n    orders,\n    on='customer_id',\n    how='right'\n)\n\n# Save output\ncustomer_orders.to_csv('customer_orders_right.csv', index=False)\n] PASSED [ 86%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[join_outer-\nimport pandas as pd\n\n# Load datasets\ncustomers = pd.read_csv('customers.csv')\norders = pd.read_csv('orders.csv')\n\n# Outer join - keep all records\ncustomer_orders = pd.merge(\n    customers,\n    orders,\n    on='customer_id',\n    how='outer'\n)\n\n# Save output\ncustomer_orders.to_csv('customer_orders_outer.csv', index=False)\n] PASSED [ 86%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[join_cross-\nimport pandas as pd\n\n# Load datasets\nproducts = pd.read_csv('products.csv')\nregions = pd.read_csv('regions.csv')\n\n# Cross join - all combinations\nproduct_regions = pd.merge(\n    products,\n    regions,\n    how='cross'\n)\n\n# Save output\nproduct_regions.to_csv('product_regions_cross.csv', index=False)\n] PASSED [ 86%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[join_multi_key-\nimport pandas as pd\n\n# Load datasets\nsales = pd.read_csv('sales.csv')\ntargets = pd.read_csv('targets.csv')\n\n# Join on multiple keys\nsales_vs_targets = pd.merge(\n    sales,\n    targets,\n    left_on=['region', 'product_id', 'quarter'],\n    right_on=['region_code', 'sku', 'fiscal_quarter'],\n    how='left'\n)\n\n# Save output\nsales_vs_targets.to_csv('sales_vs_targets.csv', index=False)\n] PASSED [ 86%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[fuzzy_join-\nimport pandas as pd\nfrom fuzzywuzzy import fuzz\n\n# Load datasets\ninternal_companies = pd.read_csv('internal_companies.csv')\nexternal_companies = pd.read_csv('external_companies.csv')\n\n# Fuzzy matching function\ndef fuzzy_match(row, df_to_match, col_name, threshold=80):\n    best_match = None\n    best_score = 0\n    for _, match_row in df_to_match.iterrows():\n        score = fuzz.ratio(str(row[col_name]).lower(), str(match_row[col_name]).lower())\n        if score > best_score and score >= threshold:\n            best_score = score\n            best_match = match_row\n    return best_match\n\n# Apply fuzzy matching\nmatches = []\nfor _, row in internal_companies.iterrows():\n    match = fuzzy_match(row, external_companies, 'company_name')\n    if match is not None:\n        matches.append({\n            'internal_id': row['id'],\n            'internal_name': row['company_name'],\n            'external_id': match['id'],\n            'external_name': match['company_name']\n        })\n\nfuzzy_joined = pd.DataFrame(matches)\nfuzzy_joined.to_csv('fuzzy_matched_companies.csv', index=False)\n] PASSED [ 86%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[geo_join-\nimport pandas as pd\nimport numpy as np\n\n# Load datasets\nstores = pd.read_csv('stores.csv')  # Has latitude, longitude\nregions = pd.read_csv('regions.csv')  # Has polygon boundaries\n\n# Calculate distance between points (Haversine formula)\ndef haversine_distance(lat1, lon1, lat2, lon2):\n    R = 6371  # Earth's radius in kilometers\n    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n# Join stores to nearest city center\ncities = pd.read_csv('city_centers.csv')\nstores_with_city = stores.copy()\nstores_with_city['nearest_city'] = stores.apply(\n    lambda row: cities.loc[\n        cities.apply(\n            lambda city: haversine_distance(\n                row['latitude'], row['longitude'],\n                city['lat'], city['lon']\n            ), axis=1\n        ).idxmin()\n    ]['city_name'], axis=1\n)\n\nstores_with_city.to_csv('stores_with_city.csv', index=False)\n] PASSED [ 86%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[stack-\nimport pandas as pd\n\n# Load multiple datasets\nq1_sales = pd.read_csv('sales_q1.csv')\nq2_sales = pd.read_csv('sales_q2.csv')\nq3_sales = pd.read_csv('sales_q3.csv')\nq4_sales = pd.read_csv('sales_q4.csv')\n\n# Add quarter identifier\nq1_sales['quarter'] = 'Q1'\nq2_sales['quarter'] = 'Q2'\nq3_sales['quarter'] = 'Q3'\nq4_sales['quarter'] = 'Q4'\n\n# Stack all quarters\nall_sales = pd.concat([q1_sales, q2_sales, q3_sales, q4_sales], ignore_index=True)\n\n# Save output\nall_sales.to_csv('all_sales.csv', index=False)\n] PASSED [ 87%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[stack_multi-\nimport pandas as pd\n\n# Load data from multiple sources\nweb_orders = pd.read_csv('web_orders.csv')\nmobile_orders = pd.read_csv('mobile_orders.csv')\nstore_orders = pd.read_csv('store_orders.csv')\n\n# Add source identifier\nweb_orders['source'] = 'web'\nmobile_orders['source'] = 'mobile'\nstore_orders['source'] = 'store'\n\n# Stack with different column alignment\nall_orders = pd.concat(\n    [web_orders, mobile_orders, store_orders],\n    ignore_index=True,\n    sort=False\n)\n\n# Save output\nall_orders.to_csv('all_orders.csv', index=False)\n] PASSED [ 87%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[split-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('all_data.csv')\n\n# Split by condition - filter\nactive_customers = df[df['status'] == 'active']\ninactive_customers = df[df['status'] == 'inactive']\n\n# Save outputs\nactive_customers.to_csv('active_customers.csv', index=False)\ninactive_customers.to_csv('inactive_customers.csv', index=False)\n] PASSED [ 87%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[split_multi-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('transactions.csv')\n\n# Split into multiple groups\nhigh_value = df[df['amount'] >= 1000]\nmedium_value = df[(df['amount'] >= 100) & (df['amount'] < 1000)]\nlow_value = df[df['amount'] < 100]\n\n# Save outputs\nhigh_value.to_csv('high_value_transactions.csv', index=False)\nmedium_value.to_csv('medium_value_transactions.csv', index=False)\nlow_value.to_csv('low_value_transactions.csv', index=False)\n] PASSED [ 87%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[sort-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('products.csv')\n\n# Sort by single column\nsorted_by_price = df.sort_values('price', ascending=True)\n\n# Save output\nsorted_by_price.to_csv('products_sorted.csv', index=False)\n] PASSED [ 87%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[sort_multi-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('employees.csv')\n\n# Sort by multiple columns\nsorted_employees = df.sort_values(\n    by=['department', 'salary', 'hire_date'],\n    ascending=[True, False, True]\n)\n\n# Save output\nsorted_employees.to_csv('employees_sorted.csv', index=False)\n] PASSED [ 87%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[distinct-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('raw_data.csv')\n\n# Remove complete duplicates\nunique_df = df.drop_duplicates()\n\n# Save output\nunique_df.to_csv('unique_data.csv', index=False)\n] PASSED [ 87%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[distinct_subset-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('customer_interactions.csv')\n\n# Remove duplicates based on specific columns (keep first)\nunique_customers = df.drop_duplicates(subset=['customer_id', 'email'], keep='first')\n\n# Remove duplicates keeping last\nlatest_interactions = df.drop_duplicates(subset=['customer_id'], keep='last')\n\n# Save outputs\nunique_customers.to_csv('unique_customers.csv', index=False)\nlatest_interactions.to_csv('latest_interactions.csv', index=False)\n] PASSED [ 87%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[top_n-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('sales.csv')\n\n# Get top 10 by sales amount\ntop_10_sales = df.nlargest(10, 'sales_amount')\n\n# Save output\ntop_10_sales.to_csv('top_10_sales.csv', index=False)\n] PASSED [ 88%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[top_n_grouped-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('products.csv')\n\n# Top N per group\ntop_3_per_category = df.groupby('category').apply(\n    lambda x: x.nlargest(3, 'revenue')\n).reset_index(drop=True)\n\n# Bottom N per group\nbottom_5_per_region = df.groupby('region').apply(\n    lambda x: x.nsmallest(5, 'price')\n).reset_index(drop=True)\n\n# Simple head/tail\nfirst_100 = df.head(100)\nlast_50 = df.tail(50)\n\n# Save outputs\ntop_3_per_category.to_csv('top_3_per_category.csv', index=False)\nbottom_5_per_region.to_csv('bottom_5_per_region.csv', index=False)\n] PASSED [ 88%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[pivot-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('sales.csv')\n\n# Pivot table\npivot_table = df.pivot_table(\n    values='amount',\n    index='product_category',\n    columns='month',\n    aggfunc='sum',\n    fill_value=0\n)\n\n# Reset index for export\npivot_table = pivot_table.reset_index()\n\n# Save output\npivot_table.to_csv('sales_pivot.csv', index=False)\n] PASSED [ 88%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[pivot_melt-\nimport pandas as pd\n\n# Load wide data\nwide_df = pd.read_csv('monthly_metrics.csv')\n\n# Melt (unpivot) from wide to long format\nlong_df = pd.melt(\n    wide_df,\n    id_vars=['product_id', 'product_name'],\n    value_vars=['jan', 'feb', 'mar', 'apr', 'may', 'jun'],\n    var_name='month',\n    value_name='sales'\n)\n\n# Save output\nlong_df.to_csv('monthly_metrics_long.csv', index=False)\n] PASSED [ 88%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[sampling-\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('large_dataset.csv')\n\n# Random sample - percentage\nsample_10pct = df.sample(frac=0.1, random_state=42)\n\n# Random sample - fixed size\nsample_1000 = df.sample(n=1000, random_state=42)\n\n# Save outputs\nsample_10pct.to_csv('sample_10pct.csv', index=False)\nsample_1000.to_csv('sample_1000.csv', index=False)\n] PASSED [ 88%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[sampling_stratified-\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load data\ndf = pd.read_csv('customers.csv')\n\n# Stratified sampling\ntrain, test = train_test_split(\n    df,\n    test_size=0.2,\n    stratify=df['segment'],\n    random_state=42\n)\n\n# Save outputs\ntrain.to_csv('train_set.csv', index=False)\ntest.to_csv('test_set.csv', index=False)\n] PASSED [ 88%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[download-\nimport pandas as pd\nimport requests\n\n# Download from URL\nurl = 'https://api.example.com/data.csv'\nresponse = requests.get(url)\n\n# Parse CSV from response\nfrom io import StringIO\ndf = pd.read_csv(StringIO(response.text))\n\n# Save locally\ndf.to_csv('downloaded_data.csv', index=False)\n] PASSED [ 88%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[python-\nimport pandas as pd\nimport numpy as np\n\n# Load input data\ndf = pd.read_csv('input_data.csv')\n\n# Complex custom transformation that can't be done with visual recipes\ndef custom_scoring_algorithm(row):\n    base_score = row['revenue'] * 0.4\n    engagement_bonus = np.log1p(row['interactions']) * 10\n    recency_factor = 1 / (1 + row['days_since_last_purchase'] / 30)\n\n    score = base_score * engagement_bonus * recency_factor\n    return round(score, 2)\n\ndf['customer_score'] = df.apply(custom_scoring_algorithm, axis=1)\n\n# Custom aggregation logic\ndf['segment'] = pd.cut(\n    df['customer_score'],\n    bins=[-np.inf, 50, 150, 300, np.inf],\n    labels=['Bronze', 'Silver', 'Gold', 'Platinum']\n)\n\n# Save output\ndf.to_csv('scored_customers.csv', index=False)\n] PASSED [ 88%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[sql-\nimport pandas as pd\nimport sqlite3\n\n# Create in-memory database\nconn = sqlite3.connect(':memory:')\n\n# Load data into database\ncustomers = pd.read_csv('customers.csv')\norders = pd.read_csv('orders.csv')\n\ncustomers.to_sql('customers', conn, index=False)\norders.to_sql('orders', conn, index=False)\n\n# Execute SQL query\nquery = '''\nSELECT\n    c.customer_id,\n    c.name,\n    COUNT(o.order_id) as order_count,\n    SUM(o.amount) as total_spend,\n    AVG(o.amount) as avg_order_value\nFROM customers c\nLEFT JOIN orders o ON c.customer_id = o.customer_id\nGROUP BY c.customer_id, c.name\nHAVING COUNT(o.order_id) > 0\nORDER BY total_spend DESC\n'''\n\nresult = pd.read_sql(query, conn)\nconn.close()\n\n# Save output\nresult.to_csv('customer_order_summary.csv', index=False)\n] PASSED [ 88%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[r-\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\n# Load data (equivalent to R's read.csv)\ndf = pd.read_csv('data.csv')\n\n# Statistical analysis (R-style)\n# Linear regression\nslope, intercept, r_value, p_value, std_err = stats.linregress(df['x'], df['y'])\n\n# Add regression results\ndf['predicted'] = intercept + slope * df['x']\ndf['residual'] = df['y'] - df['predicted']\n\n# Save output\ndf.to_csv('regression_results.csv', index=False)\n] PASSED [ 89%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[prediction_scoring-\nimport pandas as pd\nimport pickle\n\n# Load test data\ntest_data = pd.read_csv('test_data.csv')\n\n# Load trained model\nwith open('trained_model.pkl', 'rb') as f:\n    model = pickle.load(f)\n\n# Prepare features\nfeatures = test_data[['feature1', 'feature2', 'feature3', 'feature4']]\n\n# Make predictions\ntest_data['prediction'] = model.predict(features)\ntest_data['probability'] = model.predict_proba(features)[:, 1]\n\n# Save output\ntest_data.to_csv('predictions.csv', index=False)\n] PASSED [ 89%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[clustering_scoring-\nimport pandas as pd\nimport pickle\n\n# Load data\ndf = pd.read_csv('customer_data.csv')\n\n# Load trained clustering model\nwith open('kmeans_model.pkl', 'rb') as f:\n    kmeans = pickle.load(f)\n\n# Prepare features\nfeatures = df[['recency', 'frequency', 'monetary']]\n\n# Assign cluster labels\ndf['cluster'] = kmeans.predict(features)\n\n# Calculate distance to cluster centers\ndf['distance_to_center'] = kmeans.transform(features).min(axis=1)\n\n# Save output\ndf.to_csv('clustered_customers.csv', index=False)\n] PASSED [ 89%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[evaluation-\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Load predictions and actuals\ndf = pd.read_csv('predictions.csv')\n\n# Extract predictions and actuals\ny_true = df['actual']\ny_pred = df['prediction']\n\n# Calculate metrics\nmetrics = {\n    'accuracy': accuracy_score(y_true, y_pred),\n    'precision': precision_score(y_true, y_pred, average='weighted'),\n    'recall': recall_score(y_true, y_pred, average='weighted'),\n    'f1': f1_score(y_true, y_pred, average='weighted')\n}\n\n# Create evaluation report\neval_df = pd.DataFrame([metrics])\neval_df.to_csv('model_evaluation.csv', index=False)\n\n# Confusion matrix\ncm = confusion_matrix(y_true, y_pred)\ncm_df = pd.DataFrame(cm)\ncm_df.to_csv('confusion_matrix.csv', index=False)\n] PASSED [ 89%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[upsert-\nimport pandas as pd\n\n# Load existing and new data\nexisting = pd.read_csv('existing_records.csv')\nupdates = pd.read_csv('new_records.csv')\n\n# Upsert logic: update existing records or insert new ones\nkey_column = 'record_id'\n\n# Get records to update (exist in both)\nto_update = updates[updates[key_column].isin(existing[key_column])]\n\n# Get records to insert (only in updates)\nto_insert = updates[~updates[key_column].isin(existing[key_column])]\n\n# Get unchanged records\nunchanged = existing[~existing[key_column].isin(updates[key_column])]\n\n# Combine: unchanged + updated + new\nresult = pd.concat([unchanged, to_update, to_insert], ignore_index=True)\n\n# Save output\nresult.to_csv('upserted_records.csv', index=False)\n] PASSED [ 89%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[generate_features-\nimport pandas as pd\nimport numpy as np\n\n# Load data\ndf = pd.read_csv('raw_features.csv')\n\n# Generate date features\ndf['date'] = pd.to_datetime(df['date'])\ndf['year'] = df['date'].dt.year\ndf['month'] = df['date'].dt.month\ndf['day'] = df['date'].dt.day\ndf['day_of_week'] = df['date'].dt.dayofweek\ndf['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\ndf['quarter'] = df['date'].dt.quarter\n\n# Generate numeric features\ndf['amount_log'] = np.log1p(df['amount'])\ndf['amount_squared'] = df['amount'] ** 2\ndf['amount_sqrt'] = np.sqrt(df['amount'].clip(lower=0))\n\n# Generate interaction features\ndf['amount_x_quantity'] = df['amount'] * df['quantity']\ndf['price_per_unit'] = df['amount'] / df['quantity'].replace(0, 1)\n\n# Save output\ndf.to_csv('generated_features.csv', index=False)\n] PASSED [ 89%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[pyspark-\nimport pandas as pd\n\n# Note: In Dataiku, this would run as a PySpark recipe\n# Here we show the pandas equivalent\n\n# Load large dataset\ndf = pd.read_csv('large_data.csv')\n\n# Transformations that would benefit from Spark\ndf['processed'] = df['text'].str.lower()\ngrouped = df.groupby(['category', 'region']).agg({\n    'amount': 'sum',\n    'count': 'sum'\n}).reset_index()\n\n# Save output\ngrouped.to_csv('spark_processed.csv', index=False)\n] PASSED [ 89%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExamplesConversion::test_recipe_produces_datasets[hive-\nimport pandas as pd\n\n# Load data (simulating Hive table read)\nfact_sales = pd.read_csv('fact_sales.csv')\ndim_product = pd.read_csv('dim_product.csv')\ndim_date = pd.read_csv('dim_date.csv')\n\n# Join facts with dimensions (star schema query)\nresult = fact_sales.merge(\n    dim_product, on='product_id'\n).merge(\n    dim_date, on='date_id'\n)\n\n# Aggregate\nsummary = result.groupby(['year', 'month', 'category']).agg({\n    'amount': 'sum',\n    'quantity': 'sum'\n}).reset_index()\n\n# Save output\nsummary.to_csv('hive_result.csv', index=False)\n] PASSED [ 89%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeVisualization::test_svg_visualization[prepare] PASSED [ 90%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeVisualization::test_svg_visualization[sync] PASSED [ 90%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeVisualization::test_svg_visualization[grouping] PASSED [ 90%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeVisualization::test_svg_visualization[grouping_multi] PASSED [ 90%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeVisualization::test_svg_visualization[window] PASSED [ 90%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeVisualization::test_mermaid_visualization[prepare] PASSED [ 90%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeVisualization::test_mermaid_visualization[sync] PASSED [ 90%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeVisualization::test_mermaid_visualization[grouping] PASSED [ 90%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeVisualization::test_mermaid_visualization[grouping_multi] PASSED [ 90%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeVisualization::test_mermaid_visualization[window] PASSED [ 91%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeVisualization::test_ascii_visualization[prepare] PASSED [ 91%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeVisualization::test_ascii_visualization[sync] PASSED [ 91%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeVisualization::test_ascii_visualization[grouping] PASSED [ 91%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeVisualization::test_ascii_visualization[grouping_multi] PASSED [ 91%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeVisualization::test_ascii_visualization[window] PASSED [ 91%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExport::test_to_dict_export[prepare] PASSED [ 91%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExport::test_to_dict_export[sync] PASSED [ 91%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExport::test_to_dict_export[grouping] PASSED [ 92%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExport::test_to_dict_export[grouping_multi] PASSED [ 92%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExport::test_to_dict_export[window] PASSED [ 92%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExport::test_to_json_export[prepare] PASSED [ 92%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExport::test_to_json_export[sync] PASSED [ 92%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExport::test_to_json_export[grouping] PASSED [ 92%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExport::test_to_json_export[grouping_multi] PASSED [ 92%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeExport::test_to_json_export[window] PASSED [ 92%]
tests/test_py2dataiku/test_recipe_examples.py::TestVisualRecipeExamples::test_prepare_example_has_transformations PASSED [ 93%]
tests/test_py2dataiku/test_recipe_examples.py::TestVisualRecipeExamples::test_grouping_example_has_groupby PASSED [ 93%]
tests/test_py2dataiku/test_recipe_examples.py::TestVisualRecipeExamples::test_join_inner_example_has_merge PASSED [ 93%]
tests/test_py2dataiku/test_recipe_examples.py::TestVisualRecipeExamples::test_stack_example_has_concat PASSED [ 93%]
tests/test_py2dataiku/test_recipe_examples.py::TestVisualRecipeExamples::test_window_example_has_rolling PASSED [ 93%]
tests/test_py2dataiku/test_recipe_examples.py::TestVisualRecipeExamples::test_distinct_example_has_drop_duplicates PASSED [ 93%]
tests/test_py2dataiku/test_recipe_examples.py::TestVisualRecipeExamples::test_sort_example_has_sort_values PASSED [ 93%]
tests/test_py2dataiku/test_recipe_examples.py::TestVisualRecipeExamples::test_pivot_example_has_pivot PASSED [ 93%]
tests/test_py2dataiku/test_recipe_examples.py::TestVisualRecipeExamples::test_sampling_example_has_sample PASSED [ 93%]
tests/test_py2dataiku/test_recipe_examples.py::TestJoinTypeExamples::test_inner_join PASSED [ 94%]
tests/test_py2dataiku/test_recipe_examples.py::TestJoinTypeExamples::test_left_join PASSED [ 94%]
tests/test_py2dataiku/test_recipe_examples.py::TestJoinTypeExamples::test_right_join PASSED [ 94%]
tests/test_py2dataiku/test_recipe_examples.py::TestJoinTypeExamples::test_outer_join PASSED [ 94%]
tests/test_py2dataiku/test_recipe_examples.py::TestJoinTypeExamples::test_cross_join PASSED [ 94%]
tests/test_py2dataiku/test_recipe_examples.py::TestCodeRecipeExamples::test_python_recipe_example PASSED [ 94%]
tests/test_py2dataiku/test_recipe_examples.py::TestCodeRecipeExamples::test_sql_recipe_example PASSED [ 94%]
tests/test_py2dataiku/test_recipe_examples.py::TestMLRecipeExamples::test_prediction_scoring_example PASSED [ 94%]
tests/test_py2dataiku/test_recipe_examples.py::TestMLRecipeExamples::test_clustering_scoring_example PASSED [ 95%]
tests/test_py2dataiku/test_recipe_examples.py::TestMLRecipeExamples::test_evaluation_example PASSED [ 95%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeMetadata::test_metadata_exists_for_prepare PASSED [ 95%]
tests/test_py2dataiku/test_recipe_examples.py::TestRecipeMetadata::test_metadata_has_expected_fields PASSED [ 95%]
tests/test_py2dataiku/test_visualizers.py::TestThemes::test_default_theme PASSED [ 95%]
tests/test_py2dataiku/test_visualizers.py::TestThemes::test_dark_theme PASSED [ 95%]
tests/test_py2dataiku/test_visualizers.py::TestThemes::test_recipe_colors PASSED [ 95%]
tests/test_py2dataiku/test_visualizers.py::TestThemes::test_unknown_recipe_type PASSED [ 95%]
tests/test_py2dataiku/test_visualizers.py::TestIcons::test_unicode_icons PASSED [ 95%]
tests/test_py2dataiku/test_visualizers.py::TestIcons::test_labels PASSED [ 96%]
tests/test_py2dataiku/test_visualizers.py::TestIcons::test_ascii_icons PASSED [ 96%]
tests/test_py2dataiku/test_visualizers.py::TestIcons::test_unknown_icon PASSED [ 96%]
tests/test_py2dataiku/test_visualizers.py::TestLayoutEngine::test_simple_layout PASSED [ 96%]
tests/test_py2dataiku/test_visualizers.py::TestLayoutEngine::test_layer_assignment PASSED [ 96%]
tests/test_py2dataiku/test_visualizers.py::TestLayoutEngine::test_position_bounds PASSED [ 96%]
tests/test_py2dataiku/test_visualizers.py::TestLayoutEngine::test_canvas_size PASSED [ 96%]
tests/test_py2dataiku/test_visualizers.py::TestLayoutEngine::test_edges PASSED [ 96%]
tests/test_py2dataiku/test_visualizers.py::TestSVGVisualizer::test_render_simple_flow PASSED [ 97%]
tests/test_py2dataiku/test_visualizers.py::TestSVGVisualizer::test_svg_contains_nodes PASSED [ 97%]
tests/test_py2dataiku/test_visualizers.py::TestSVGVisualizer::test_svg_contains_recipes PASSED [ 97%]
tests/test_py2dataiku/test_visualizers.py::TestSVGVisualizer::test_svg_with_custom_theme PASSED [ 97%]
tests/test_py2dataiku/test_visualizers.py::TestSVGVisualizer::test_svg_defs PASSED [ 97%]
tests/test_py2dataiku/test_visualizers.py::TestASCIIVisualizer::test_render_simple_flow PASSED [ 97%]
tests/test_py2dataiku/test_visualizers.py::TestASCIIVisualizer::test_ascii_contains_nodes PASSED [ 97%]
tests/test_py2dataiku/test_visualizers.py::TestASCIIVisualizer::test_ascii_contains_legend PASSED [ 97%]
tests/test_py2dataiku/test_visualizers.py::TestASCIIVisualizer::test_compact_render PASSED [ 97%]
tests/test_py2dataiku/test_visualizers.py::TestPlantUMLVisualizer::test_render_simple_flow PASSED [ 98%]
tests/test_py2dataiku/test_visualizers.py::TestPlantUMLVisualizer::test_plantuml_contains_nodes PASSED [ 98%]
tests/test_py2dataiku/test_visualizers.py::TestPlantUMLVisualizer::test_plantuml_contains_arrows PASSED [ 98%]
tests/test_py2dataiku/test_visualizers.py::TestPlantUMLVisualizer::test_plantuml_styling PASSED [ 98%]
tests/test_py2dataiku/test_visualizers.py::TestHTMLVisualizer::test_render_simple_flow PASSED [ 98%]
tests/test_py2dataiku/test_visualizers.py::TestHTMLVisualizer::test_html_contains_javascript PASSED [ 98%]
tests/test_py2dataiku/test_visualizers.py::TestHTMLVisualizer::test_html_contains_theme PASSED [ 98%]
tests/test_py2dataiku/test_visualizers.py::TestHTMLVisualizer::test_html_interactive_features PASSED [ 98%]
tests/test_py2dataiku/test_visualizers.py::TestVisualizationIntegration::test_visualize_flow_function PASSED [ 99%]
tests/test_py2dataiku/test_visualizers.py::TestVisualizationIntegration::test_flow_visualization_methods PASSED [ 99%]
tests/test_py2dataiku/test_visualizers.py::TestVisualizationIntegration::test_flow_visualize_method PASSED [ 99%]
tests/test_py2dataiku/test_visualizers.py::TestVisualizationIntegration::test_complex_flow_visualization PASSED [ 99%]
tests/test_py2dataiku/test_visualizers.py::TestVisualizationIntegration::test_unknown_format_raises_error PASSED [ 99%]
tests/test_py2dataiku/test_visualizers.py::TestEdgeCases::test_empty_flow PASSED [ 99%]
tests/test_py2dataiku/test_visualizers.py::TestEdgeCases::test_single_dataset_flow PASSED [ 99%]
tests/test_py2dataiku/test_visualizers.py::TestEdgeCases::test_long_names PASSED [ 99%]
tests/test_py2dataiku/test_visualizers.py::TestEdgeCases::test_special_characters_in_names PASSED [100%]

============================= 843 passed in 2.06s ==============================
