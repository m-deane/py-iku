{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# py-iku: Advanced Features\n",
    "\n",
    "This notebook covers advanced py-iku features:\n",
    "\n",
    "1. **Plugin Architecture**: Extend with custom recipe/processor mappings\n",
    "2. **Command Line Interface**: Use py-iku from the terminal\n",
    "3. **DSS Export**: Export directly to Dataiku DSS project format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Plugin Architecture\n",
    "\n",
    "py-iku's plugin system allows you to:\n",
    "- Register custom pandas method handlers\n",
    "- Map custom functions to Dataiku recipes\n",
    "- Create reusable extension packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2dataiku.plugins.registry import (\n",
    "    PluginRegistry,\n",
    "    register_recipe_handler,\n",
    "    register_processor_handler,\n",
    "    Plugin\n",
    ")\n",
    "from py2dataiku.models.dataiku_recipe import RecipeType\n",
    "from py2dataiku.models.prepare_step import ProcessorType\n",
    "from py2dataiku.models.transformation import Transformation, TransformationType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Registering Custom Recipe Mappings\n",
    "\n",
    "Map custom pandas methods to Dataiku recipe types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register a custom method to map to a recipe type\n",
    "PluginRegistry.register_recipe_mapping(\n",
    "    pandas_method='my_custom_merge',\n",
    "    recipe_type=RecipeType.JOIN\n",
    ")\n",
    "\n",
    "# Now when py-iku sees df.my_custom_merge(), it will create a JOIN recipe\n",
    "print(f\"Registered recipe mappings: {len(PluginRegistry._recipe_mappings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Using Decorators\n",
    "\n",
    "Use decorators for cleaner registration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register a custom method handler using decorator\n",
    "@register_recipe_handler('special_aggregate')\n",
    "def handle_special_aggregate(context):\n",
    "    \"\"\"Custom handler for a special aggregation method.\"\"\"\n",
    "    return Transformation(\n",
    "        transformation_type=TransformationType.AGGREGATION,\n",
    "        source_dataframe=context.dataframes.get('df'),\n",
    "        target_dataframe='result',\n",
    "        parameters={'custom': True, 'method': 'special_aggregate'},\n",
    "        source_line=context.current_line,\n",
    "        notes=['Custom aggregation via plugin']\n",
    "    )\n",
    "\n",
    "print(\"Custom handler registered!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Creating a Plugin Class\n",
    "\n",
    "For larger extensions, create a Plugin class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAnalyticsPlugin(Plugin):\n",
    "    \"\"\"Custom plugin for analytics operations.\"\"\"\n",
    "    \n",
    "    name = \"my_analytics\"\n",
    "    version = \"1.0.0\"\n",
    "    \n",
    "    def register(self):\n",
    "        \"\"\"Register all handlers when plugin loads.\"\"\"\n",
    "        # Register recipe mappings\n",
    "        PluginRegistry.register_recipe_mapping(\n",
    "            'time_series_analysis',\n",
    "            RecipeType.PYTHON\n",
    "        )\n",
    "        PluginRegistry.register_recipe_mapping(\n",
    "            'anomaly_detection',\n",
    "            RecipeType.PYTHON\n",
    "        )\n",
    "        \n",
    "        # Register processor mappings\n",
    "        PluginRegistry.register_processor_mapping(\n",
    "            'normalize_ts',\n",
    "            ProcessorType.NORMALIZER\n",
    "        )\n",
    "        \n",
    "        print(f\"Plugin '{self.name}' v{self.version} registered!\")\n",
    "\n",
    "# Load the plugin\n",
    "plugin = MyAnalyticsPlugin()\n",
    "plugin.register()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Checking Registered Plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all registered mappings\n",
    "print(\"Recipe Mappings:\")\n",
    "for method, recipe_type in PluginRegistry._recipe_mappings.items():\n",
    "    print(f\"  {method} -> {recipe_type.value}\")\n",
    "\n",
    "print(\"\\nProcessor Mappings:\")\n",
    "for method, processor_type in PluginRegistry._processor_mappings.items():\n",
    "    print(f\"  {method} -> {processor_type.value}\")\n",
    "\n",
    "print(\"\\nMethod Handlers:\")\n",
    "for method in PluginRegistry._method_handlers.keys():\n",
    "    print(f\"  {method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Command Line Interface (CLI)\n",
    "\n",
    "py-iku provides a CLI for terminal usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 CLI Commands\n",
    "\n",
    "```bash\n",
    "# Convert Python code to Dataiku flow\n",
    "py2dataiku convert script.py -o flow.json\n",
    "py2dataiku convert script.py -f yaml -o flow.yaml\n",
    "\n",
    "# Visualize a flow\n",
    "py2dataiku viz script.py -f ascii\n",
    "py2dataiku viz script.py -f svg -o diagram.svg\n",
    "py2dataiku viz script.py -f html -o interactive.html\n",
    "\n",
    "# Analyze code and show transformations\n",
    "py2dataiku analyze script.py\n",
    "py2dataiku analyze script.py -f json\n",
    "\n",
    "# Export to Dataiku DSS project\n",
    "py2dataiku export script.py -o ./my_project\n",
    "py2dataiku export script.py -o project.zip --zip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample script to use with CLI\n",
    "sample_script = '''\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "df = df.dropna()\n",
    "summary = df.groupby('category')['amount'].sum().reset_index()\n",
    "summary.to_csv('summary.csv', index=False)\n",
    "'''\n",
    "\n",
    "with open('sample_script.py', 'w') as f:\n",
    "    f.write(sample_script)\n",
    "print(\"Created sample_script.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CLI commands from notebook\n",
    "!python -m py2dataiku.cli convert sample_script.py 2>/dev/null | head -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with CLI\n",
    "!python -m py2dataiku.cli viz sample_script.py -f ascii 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze with CLI\n",
    "!python -m py2dataiku.cli analyze sample_script.py 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Using CLI Programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2dataiku.cli import create_parser, cmd_convert, cmd_analyze\n",
    "import sys\n",
    "from io import StringIO\n",
    "\n",
    "# Create a parser\n",
    "parser = create_parser()\n",
    "\n",
    "# Parse arguments programmatically\n",
    "args = parser.parse_args(['convert', 'sample_script.py', '-f', 'yaml'])\n",
    "print(f\"Command: {args.command}\")\n",
    "print(f\"Input: {args.input}\")\n",
    "print(f\"Format: {args.format}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DSS Project Export\n",
    "\n",
    "Export flows directly to Dataiku DSS project format for import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2dataiku.exporters.dss_exporter import DSSExporter\n",
    "from py2dataiku import convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a flow\n",
    "etl_code = '''\n",
    "import pandas as pd\n",
    "\n",
    "# Load raw data\n",
    "customers = pd.read_csv('customers.csv')\n",
    "orders = pd.read_csv('orders.csv')\n",
    "\n",
    "# Clean customer data\n",
    "customers['name'] = customers['name'].str.strip().str.title()\n",
    "customers['email'] = customers['email'].str.lower()\n",
    "customers = customers.dropna(subset=['customer_id'])\n",
    "\n",
    "# Join with orders\n",
    "customer_orders = pd.merge(customers, orders, on='customer_id', how='left')\n",
    "\n",
    "# Calculate metrics\n",
    "customer_summary = customer_orders.groupby('customer_id').agg({\n",
    "    'order_id': 'count',\n",
    "    'amount': 'sum',\n",
    "    'name': 'first'\n",
    "}).reset_index()\n",
    "customer_summary.columns = ['customer_id', 'order_count', 'total_amount', 'name']\n",
    "\n",
    "# Save output\n",
    "customer_summary.to_csv('customer_metrics.csv', index=False)\n",
    "'''\n",
    "\n",
    "flow = convert(etl_code)\n",
    "print(flow.get_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Export to Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create exporter\n",
    "exporter = DSSExporter(flow, project_key='CUSTOMER_ANALYTICS')\n",
    "\n",
    "# Export to directory\n",
    "output_path = exporter.export('./dss_project')\n",
    "print(f\"Exported to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the exported structure\n",
    "!find ./dss_project -type f | head -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View project.json\n",
    "!cat ./dss_project/project.json 2>/dev/null | head -30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Export as ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export as ZIP file for easy import to Dataiku\n",
    "zip_path = exporter.export('./dss_export', create_zip=True)\n",
    "print(f\"Created ZIP: {zip_path}\")\n",
    "\n",
    "# Check file size\n",
    "import os\n",
    "size = os.path.getsize(zip_path)\n",
    "print(f\"ZIP size: {size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Get API Bundle\n",
    "\n",
    "Get the export as a Python dictionary for API-based import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get API bundle (dict format)\n",
    "bundle = exporter.get_api_bundle()\n",
    "\n",
    "print(f\"Bundle keys: {list(bundle.keys())}\")\n",
    "print(f\"\\nProject info:\")\n",
    "print(f\"  Key: {bundle['project']['projectKey']}\")\n",
    "print(f\"  Name: {bundle['project']['name']}\")\n",
    "\n",
    "print(f\"\\nDatasets: {len(bundle['datasets'])}\")\n",
    "for ds in bundle['datasets'][:3]:\n",
    "    print(f\"  - {ds['name']} ({ds['type']})\")\n",
    "\n",
    "print(f\"\\nRecipes: {len(bundle['recipes'])}\")\n",
    "for r in bundle['recipes'][:3]:\n",
    "    print(f\"  - {r['name']} ({r['type']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Examining Exported Recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# View a recipe definition\n",
    "if bundle['recipes']:\n",
    "    recipe = bundle['recipes'][0]\n",
    "    print(f\"Recipe: {recipe['name']}\")\n",
    "    print(json.dumps(recipe, indent=2)[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Complete Example: End-to-End Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete workflow: Code -> Analysis -> Visualization -> Export\n",
    "\n",
    "ml_pipeline = '''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('ml_dataset.csv')\n",
    "\n",
    "# Feature engineering with NumPy\n",
    "df['log_amount'] = np.log1p(df['amount'])\n",
    "df['amount_clipped'] = np.clip(df['amount'], 0, 10000)\n",
    "df['is_high'] = np.where(df['amount'] > 1000, 1, 0)\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df[['feature1', 'feature2']] = imputer.fit_transform(df[['feature1', 'feature2']])\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "df[['feature1', 'feature2']] = scaler.fit_transform(df[['feature1', 'feature2']])\n",
    "\n",
    "# Split data\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save processed data\n",
    "X_train.to_csv('train_features.csv', index=False)\n",
    "X_test.to_csv('test_features.csv', index=False)\n",
    "'''\n",
    "\n",
    "# 1. Convert\n",
    "flow = convert(ml_pipeline)\n",
    "print(\"=\" * 60)\n",
    "print(\"FLOW SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(flow.get_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Visualize\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FLOW DIAGRAM\")\n",
    "print(\"=\" * 60)\n",
    "print(flow.visualize(format='ascii'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Export\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DSS EXPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "exporter = DSSExporter(flow, project_key='ML_PIPELINE')\n",
    "zip_path = exporter.export('./ml_pipeline_export', create_zip=True)\n",
    "print(f\"\\nExported to: {zip_path}\")\n",
    "print(\"\\nThis ZIP can be imported directly into Dataiku DSS!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tips and Best Practices\n",
    "\n",
    "### Code Organization\n",
    "- Keep data transformations in separate, focused scripts\n",
    "- Use clear variable names that indicate data state\n",
    "- Add comments that describe the business logic\n",
    "\n",
    "### Plugin Development\n",
    "- Create plugins for domain-specific transformations\n",
    "- Register handlers early in application startup\n",
    "- Use type hints in handler functions\n",
    "\n",
    "### DSS Integration\n",
    "- Test exports in a development DSS instance first\n",
    "- Use meaningful project keys\n",
    "- Review recipe configurations before import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "for path in ['sample_script.py', 'dss_project', 'dss_export', 'ml_pipeline_export', \n",
    "             'flow_diagram.svg', 'flow_interactive.html']:\n",
    "    if os.path.isfile(path):\n",
    "        os.remove(path)\n",
    "    elif os.path.isdir(path):\n",
    "        shutil.rmtree(path)\n",
    "\n",
    "print(\"Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This concludes the py-iku tutorial series. You've learned:\n",
    "\n",
    "1. **Basic Usage**: Converting pandas code to Dataiku flows\n",
    "2. **NumPy Support**: Using NumPy functions in transformations\n",
    "3. **Scikit-learn**: Converting ML pipelines\n",
    "4. **Visualizations**: Multiple output formats\n",
    "5. **Advanced Features**: Plugins, CLI, and DSS export\n",
    "\n",
    "For more information, see the documentation and examples in the repository."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
