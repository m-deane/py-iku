{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# py-iku: Advanced Features\n",
    "\n",
    "This notebook covers advanced py-iku features:\n",
    "\n",
    "1. **Plugin Architecture**: Extend with custom recipe/processor mappings\n",
    "2. **Command Line Interface**: Use py-iku from the terminal\n",
    "3. **DSS Export**: Export directly to Dataiku DSS project format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Plugin Architecture\n",
    "\n",
    "py-iku's plugin system allows you to:\n",
    "- Register custom pandas method handlers\n",
    "- Map custom functions to Dataiku recipes\n",
    "- Create reusable extension packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2dataiku.plugins.registry import (\n",
    "    PluginRegistry,\n",
    "    register_recipe_handler,\n",
    "    register_processor_handler,\n",
    "    Plugin\n",
    ")\n",
    "from py2dataiku.models.dataiku_recipe import RecipeType\n",
    "from py2dataiku.models.prepare_step import ProcessorType\n",
    "from py2dataiku.models.transformation import Transformation, TransformationType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Registering Custom Recipe Mappings\n",
    "\n",
    "Map custom pandas methods to Dataiku recipe types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered recipe mappings: 1\n"
     ]
    }
   ],
   "source": [
    "# Register a custom method to map to a recipe type\n",
    "PluginRegistry.register_recipe_mapping(\n",
    "    pandas_method='my_custom_merge',\n",
    "    recipe_type=RecipeType.JOIN\n",
    ")\n",
    "\n",
    "# Now when py-iku sees df.my_custom_merge(), it will create a JOIN recipe\n",
    "print(f\"Registered recipe mappings: {len(PluginRegistry._recipe_mappings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Using Decorators\n",
    "\n",
    "Use decorators for cleaner registration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom handler registered!\n"
     ]
    }
   ],
   "source": [
    "# Register a custom method handler using decorator\n",
    "@register_recipe_handler('special_aggregate')\n",
    "def handle_special_aggregate(context):\n",
    "    \"\"\"Custom handler for a special aggregation method.\"\"\"\n",
    "    return Transformation(\n",
    "        transformation_type=TransformationType.AGGREGATION,\n",
    "        source_dataframe=context.dataframes.get('df'),\n",
    "        target_dataframe='result',\n",
    "        parameters={'custom': True, 'method': 'special_aggregate'},\n",
    "        source_line=context.current_line,\n",
    "        notes=['Custom aggregation via plugin']\n",
    "    )\n",
    "\n",
    "print(\"Custom handler registered!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Creating a Plugin Class\n",
    "\n",
    "For larger extensions, create a Plugin class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plugin 'my_analytics' v1.0.0 registered!\n"
     ]
    }
   ],
   "source": [
    "class MyAnalyticsPlugin(Plugin):\n",
    "    \"\"\"Custom plugin for analytics operations.\"\"\"\n",
    "    \n",
    "    name = \"my_analytics\"\n",
    "    version = \"1.0.0\"\n",
    "    \n",
    "    def register(self):\n",
    "        \"\"\"Register all handlers when plugin loads.\"\"\"\n",
    "        # Register recipe mappings\n",
    "        PluginRegistry.register_recipe_mapping(\n",
    "            'time_series_analysis',\n",
    "            RecipeType.PYTHON\n",
    "        )\n",
    "        PluginRegistry.register_recipe_mapping(\n",
    "            'anomaly_detection',\n",
    "            RecipeType.PYTHON\n",
    "        )\n",
    "        \n",
    "        # Register processor mappings\n",
    "        PluginRegistry.register_processor_mapping(\n",
    "            'normalize_ts',\n",
    "            ProcessorType.NORMALIZER\n",
    "        )\n",
    "        \n",
    "        print(f\"Plugin '{self.name}' v{self.version} registered!\")\n",
    "\n",
    "# Load the plugin\n",
    "plugin = MyAnalyticsPlugin()\n",
    "plugin.register()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Checking Registered Plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe Mappings:\n",
      "  my_custom_merge -> join\n",
      "  time_series_analysis -> python\n",
      "  anomaly_detection -> python\n",
      "\n",
      "Processor Mappings:\n",
      "  normalize_ts -> Normalizer\n",
      "\n",
      "Method Handlers:\n"
     ]
    }
   ],
   "source": [
    "# View all registered mappings\n",
    "print(\"Recipe Mappings:\")\n",
    "for method, recipe_type in PluginRegistry._recipe_mappings.items():\n",
    "    print(f\"  {method} -> {recipe_type.value}\")\n",
    "\n",
    "print(\"\\nProcessor Mappings:\")\n",
    "for method, processor_type in PluginRegistry._processor_mappings.items():\n",
    "    print(f\"  {method} -> {processor_type.value}\")\n",
    "\n",
    "print(\"\\nMethod Handlers:\")\n",
    "for method in PluginRegistry._method_handlers.keys():\n",
    "    print(f\"  {method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Command Line Interface (CLI)\n",
    "\n",
    "py-iku provides a CLI for terminal usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 CLI Commands\n",
    "\n",
    "```bash\n",
    "# Convert Python code to Dataiku flow\n",
    "py2dataiku convert script.py -o flow.json\n",
    "py2dataiku convert script.py -f yaml -o flow.yaml\n",
    "\n",
    "# Visualize a flow\n",
    "py2dataiku viz script.py -f ascii\n",
    "py2dataiku viz script.py -f svg -o diagram.svg\n",
    "py2dataiku viz script.py -f html -o interactive.html\n",
    "\n",
    "# Analyze code and show transformations\n",
    "py2dataiku analyze script.py\n",
    "py2dataiku analyze script.py -f json\n",
    "\n",
    "# Export to Dataiku DSS project\n",
    "py2dataiku export script.py -o ./my_project\n",
    "py2dataiku export script.py -o project.zip --zip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created sample_script.py\n"
     ]
    }
   ],
   "source": [
    "# Create a sample script to use with CLI\n",
    "sample_script = '''\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "df = df.dropna()\n",
    "summary = df.groupby('category')['amount'].sum().reset_index()\n",
    "summary.to_csv('summary.csv', index=False)\n",
    "'''\n",
    "\n",
    "with open('sample_script.py', 'w') as f:\n",
    "    f.write(sample_script)\n",
    "print(\"Created sample_script.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"flow_name\": \"converted_flow\",\n",
      "  \"generated_from\": null,\n",
      "  \"generation_timestamp\": \"2026-01-13T20:33:02.313439\",\n",
      "  \"total_recipes\": 2,\n",
      "  \"total_datasets\": 4,\n",
      "  \"datasets\": [\n",
      "    {\n",
      "      \"name\": \"df\",\n",
      "      \"type\": \"input\",\n",
      "      \"schema\": [],\n",
      "      \"source_variable\": \"df\",\n",
      "      \"source_line\": 4,\n",
      "      \"notes\": []\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"df_prepared\",\n",
      "      \"type\": \"intermediate\",\n",
      "      \"schema\": [],\n",
      "      \"source_variable\": null,\n"
     ]
    }
   ],
   "source": [
    "# Run CLI commands from notebook\n",
    "!python -m py2dataiku.cli convert sample_script.py 2>/dev/null | head -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "                          DATAIKU FLOW: converted_flow  \n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "                              â”‚ ğŸ“Š df             â”‚\n",
      "                             â”‚    [INPUT]        â”‚\n",
      "                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "                              â”‚ ğŸ“Š                â”‚\n",
      "                             â”‚    [INPUT]        â”‚\n",
      "                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "                                       â”‚\n",
      "                                       â–¼\n",
      "\n",
      "                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "                               â”‚   âš™ PREPARE   â”‚\n",
      "                                â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚\n",
      "                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "                               â”‚   Î£ GROUPING  â”‚\n",
      "                                â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚\n",
      "                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "                                       â”‚\n",
      "                                       â–¼\n",
      "\n",
      "                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "                              â”‚ ğŸ“Š df_prepared    â”‚\n",
      "                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "                              â”‚ ğŸ“Š _chain_step_0  â”‚\n",
      "                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "Legend: ğŸ“Š Dataset  âš™ Prepare  â‹ˆ Join  Î£ Grouping  â‘‚ Split  â‡… Sort\n"
     ]
    }
   ],
   "source": [
    "# Visualize with CLI\n",
    "!python -m py2dataiku.cli viz sample_script.py -f ascii 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 4 transformation(s):\n",
      "\n",
      "1. read_data\n",
      "   Target: df\n",
      "   Line: 4\n",
      "\n",
      "2. drop_na\n",
      "   Source: df\n",
      "   Target: df\n",
      "   Suggested Processor: RemoveRowsOnEmpty\n",
      "   Line: 5\n",
      "\n",
      "3. groupby\n",
      "   Target: _chain_step_0\n",
      "   Suggested Recipe: grouping\n",
      "   Line: 6\n",
      "\n",
      "4. write_data\n",
      "   Source: summary\n",
      "   Line: 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze with CLI\n",
    "!python -m py2dataiku.cli analyze sample_script.py 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Using CLI Programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command: convert\n",
      "Input: sample_script.py\n",
      "Format: yaml\n"
     ]
    }
   ],
   "source": [
    "from py2dataiku.cli import create_parser, cmd_convert, cmd_analyze\n",
    "import sys\n",
    "from io import StringIO\n",
    "\n",
    "# Create a parser\n",
    "parser = create_parser()\n",
    "\n",
    "# Parse arguments programmatically\n",
    "args = parser.parse_args(['convert', 'sample_script.py', '-f', 'yaml'])\n",
    "print(f\"Command: {args.command}\")\n",
    "print(f\"Input: {args.input}\")\n",
    "print(f\"Format: {args.format}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DSS Project Export\n",
    "\n",
    "Export flows directly to Dataiku DSS project format for import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2dataiku.exporters.dss_exporter import DSSExporter\n",
    "from py2dataiku import convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow: converted_flow\n",
      "Source: unknown\n",
      "Generated: 2026-01-13T20:33:03.256114\n",
      "\n",
      "Datasets: 5\n",
      "  - Input: 2\n",
      "  - Intermediate: 3\n",
      "  - Output: 0\n",
      "\n",
      "Recipes: 3\n",
      "  - grouping: 1\n",
      "  - join: 1\n",
      "  - prepare: 1\n"
     ]
    }
   ],
   "source": [
    "# Create a flow\n",
    "etl_code = '''\n",
    "import pandas as pd\n",
    "\n",
    "# Load raw data\n",
    "customers = pd.read_csv('customers.csv')\n",
    "orders = pd.read_csv('orders.csv')\n",
    "\n",
    "# Clean customer data\n",
    "customers['name'] = customers['name'].str.strip().str.title()\n",
    "customers['email'] = customers['email'].str.lower()\n",
    "customers = customers.dropna(subset=['customer_id'])\n",
    "\n",
    "# Join with orders\n",
    "customer_orders = pd.merge(customers, orders, on='customer_id', how='left')\n",
    "\n",
    "# Calculate metrics\n",
    "customer_summary = customer_orders.groupby('customer_id').agg({\n",
    "    'order_id': 'count',\n",
    "    'amount': 'sum',\n",
    "    'name': 'first'\n",
    "}).reset_index()\n",
    "customer_summary.columns = ['customer_id', 'order_count', 'total_amount', 'name']\n",
    "\n",
    "# Save output\n",
    "customer_summary.to_csv('customer_metrics.csv', index=False)\n",
    "'''\n",
    "\n",
    "flow = convert(etl_code)\n",
    "print(flow.get_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Export to Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported to: ./dss_project/CUSTOMER_ANALYTICS\n"
     ]
    }
   ],
   "source": [
    "# Create exporter\n",
    "exporter = DSSExporter(flow, project_key='CUSTOMER_ANALYTICS')\n",
    "\n",
    "# Export to directory\n",
    "output_path = exporter.export('./dss_project')\n",
    "print(f\"Exported to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dss_project/CUSTOMER_ANALYTICS/project.json\n",
      "./dss_project/CUSTOMER_ANALYTICS/datasets/_chain_step_0.json\n",
      "./dss_project/CUSTOMER_ANALYTICS/datasets/orders.json\n",
      "./dss_project/CUSTOMER_ANALYTICS/datasets/customers.json\n",
      "./dss_project/CUSTOMER_ANALYTICS/datasets/customer_orders.json\n",
      "./dss_project/CUSTOMER_ANALYTICS/datasets/customers_prepared.json\n",
      "./dss_project/CUSTOMER_ANALYTICS/params.json\n",
      "./dss_project/CUSTOMER_ANALYTICS/README.md\n",
      "./dss_project/CUSTOMER_ANALYTICS/recipes/prepare_1.json\n",
      "./dss_project/CUSTOMER_ANALYTICS/recipes/grouping_3.json\n",
      "./dss_project/CUSTOMER_ANALYTICS/recipes/join_2.json\n",
      "./dss_project/CUSTOMER_ANALYTICS/flow/zones.json\n"
     ]
    }
   ],
   "source": [
    "# View the exported structure\n",
    "!find ./dss_project -type f | head -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View project.json\n",
    "!cat ./dss_project/project.json 2>/dev/null | head -30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Export as ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ZIP: ./dss_export/CUSTOMER_ANALYTICS.zip\n",
      "ZIP size: 7352 bytes\n"
     ]
    }
   ],
   "source": [
    "# Export as ZIP file for easy import to Dataiku\n",
    "zip_path = exporter.export('./dss_export', create_zip=True)\n",
    "print(f\"Created ZIP: {zip_path}\")\n",
    "\n",
    "# Check file size\n",
    "import os\n",
    "size = os.path.getsize(zip_path)\n",
    "print(f\"ZIP size: {size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Get API Bundle\n",
    "\n",
    "Get the export as a Python dictionary for API-based import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bundle keys: ['projectKey', 'projectName', 'datasets', 'recipes', 'metadata']\n",
      "\n",
      "Project info:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'project'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBundle keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(bundle.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mProject info:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mbundle\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproject\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mprojectKey\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbundle[\u001b[33m'\u001b[39m\u001b[33mproject\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDatasets: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(bundle[\u001b[33m'\u001b[39m\u001b[33mdatasets\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'project'"
     ]
    }
   ],
   "source": [
    "# Get API bundle (dict format)\n",
    "bundle = exporter.get_api_bundle()\n",
    "\n",
    "print(f\"Bundle keys: {list(bundle.keys())}\")\n",
    "print(f\"\\nProject info:\")\n",
    "print(f\"  Key: {bundle['project']['projectKey']}\")\n",
    "print(f\"  Name: {bundle['project']['name']}\")\n",
    "\n",
    "print(f\"\\nDatasets: {len(bundle['datasets'])}\")\n",
    "for ds in bundle['datasets'][:3]:\n",
    "    print(f\"  - {ds['name']} ({ds['type']})\")\n",
    "\n",
    "print(f\"\\nRecipes: {len(bundle['recipes'])}\")\n",
    "for r in bundle['recipes'][:3]:\n",
    "    print(f\"  - {r['name']} ({r['type']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Examining Exported Recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe: prepare_1\n",
      "{\n",
      "  \"name\": \"prepare_1\",\n",
      "  \"projectKey\": \"CUSTOMER_ANALYTICS\",\n",
      "  \"type\": \"shaker\",\n",
      "  \"inputs\": {\n",
      "    \"main\": {\n",
      "      \"items\": [\n",
      "        {\n",
      "          \"ref\": \"customers\",\n",
      "          \"deps\": []\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"outputs\": {\n",
      "    \"main\": {\n",
      "      \"items\": [\n",
      "        {\n",
      "          \"ref\": \"customers_prepared\",\n",
      "          \"deps\": []\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"versionTag\": {\n",
      "    \"versionNumber\": 1,\n",
      "    \"lastModifiedBy\": {\n",
      "      \"login\": \"py2dataiku\"\n",
      "    },\n",
      "    \"lastModifiedOn\": 1768336384014\n",
      "  },\n",
      "  \"creationTag\": {\n",
      "    \"versionNumber\": 0,\n",
      "    \"lastModifiedBy\": {\n",
      "      \"login\": \"py2dataiku\"\n",
      "    },\n",
      "    \"lastModifiedOn\": 1768336384014\n",
      "  },\n",
      "  \"tags\": [],\n",
      "  \"customMeta\": {\n",
      "    \"kpiByLabels\": {}\n",
      "  },\n",
      "  \"params\": {\n",
      "    \"mode\": \"BATCH\",\n",
      "    \"steps\": [\n",
      "      {\n",
      "        \"metaType\": \"PROCESSOR\",\n",
      "        \"type\": \"RemoveRowsOnEmpty\",\n",
      "        \"disabled\": false,\n",
      "        \"params\": {\n",
      "          \"columns\": [\n",
      "            \"customer_id\"\n",
      "          ],\n",
      "          \"keep\": false\n",
      "        }\n",
      "      }\n",
      "    ],\n",
      "    \"ma\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# View a recipe definition\n",
    "if bundle['recipes']:\n",
    "    recipe = bundle['recipes'][0]\n",
    "    print(f\"Recipe: {recipe['name']}\")\n",
    "    print(json.dumps(recipe, indent=2)[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Complete Example: End-to-End Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FLOW SUMMARY\n",
      "============================================================\n",
      "Flow: converted_flow\n",
      "Source: unknown\n",
      "Generated: 2026-01-13T20:33:25.586929\n",
      "\n",
      "Datasets: 2\n",
      "  - Input: 1\n",
      "  - Intermediate: 1\n",
      "  - Output: 0\n",
      "\n",
      "Recipes: 1\n",
      "  - split: 1\n"
     ]
    }
   ],
   "source": [
    "# Complete workflow: Code -> Analysis -> Visualization -> Export\n",
    "\n",
    "ml_pipeline = '''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('ml_dataset.csv')\n",
    "\n",
    "# Feature engineering with NumPy\n",
    "df['log_amount'] = np.log1p(df['amount'])\n",
    "df['amount_clipped'] = np.clip(df['amount'], 0, 10000)\n",
    "df['is_high'] = np.where(df['amount'] > 1000, 1, 0)\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df[['feature1', 'feature2']] = imputer.fit_transform(df[['feature1', 'feature2']])\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "df[['feature1', 'feature2']] = scaler.fit_transform(df[['feature1', 'feature2']])\n",
    "\n",
    "# Split data\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save processed data\n",
    "X_train.to_csv('train_features.csv', index=False)\n",
    "X_test.to_csv('test_features.csv', index=False)\n",
    "'''\n",
    "\n",
    "# 1. Convert\n",
    "flow = convert(ml_pipeline)\n",
    "print(\"=\" * 60)\n",
    "print(\"FLOW SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(flow.get_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FLOW DIAGRAM\n",
      "============================================================\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "                          DATAIKU FLOW: converted_flow  \n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "                              â”‚ ğŸ“Š df             â”‚\n",
      "                             â”‚    [INPUT]        â”‚\n",
      "                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "                                       â”‚\n",
      "                                       â–¼\n",
      "\n",
      "                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "                               â”‚   â‘‚ SPLIT     â”‚\n",
      "                                â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚\n",
      "                                â”‚ filter       â”‚\n",
      "                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "                                       â”‚\n",
      "                                       â–¼\n",
      "\n",
      "                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "                              â”‚ ğŸ“Š y              â”‚\n",
      "                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "Legend: ğŸ“Š Dataset  âš™ Prepare  â‹ˆ Join  Î£ Grouping  â‘‚ Split  â‡… Sort\n"
     ]
    }
   ],
   "source": [
    "# 2. Visualize\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FLOW DIAGRAM\")\n",
    "print(\"=\" * 60)\n",
    "print(flow.visualize(format='ascii'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DSS EXPORT\n",
      "============================================================\n",
      "\n",
      "Exported to: ./ml_pipeline_export/ML_PIPELINE.zip\n",
      "\n",
      "This ZIP can be imported directly into Dataiku DSS!\n"
     ]
    }
   ],
   "source": [
    "# 3. Export\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DSS EXPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "exporter = DSSExporter(flow, project_key='ML_PIPELINE')\n",
    "zip_path = exporter.export('./ml_pipeline_export', create_zip=True)\n",
    "print(f\"\\nExported to: {zip_path}\")\n",
    "print(\"\\nThis ZIP can be imported directly into Dataiku DSS!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tips and Best Practices\n",
    "\n",
    "### Code Organization\n",
    "- Keep data transformations in separate, focused scripts\n",
    "- Use clear variable names that indicate data state\n",
    "- Add comments that describe the business logic\n",
    "\n",
    "### Plugin Development\n",
    "- Create plugins for domain-specific transformations\n",
    "- Register handlers early in application startup\n",
    "- Use type hints in handler functions\n",
    "\n",
    "### DSS Integration\n",
    "- Test exports in a development DSS instance first\n",
    "- Use meaningful project keys\n",
    "- Review recipe configurations before import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup complete!\n"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "for path in ['sample_script.py', 'dss_project', 'dss_export', 'ml_pipeline_export', \n",
    "             'flow_diagram.svg', 'flow_interactive.html']:\n",
    "    if os.path.isfile(path):\n",
    "        os.remove(path)\n",
    "    elif os.path.isdir(path):\n",
    "        shutil.rmtree(path)\n",
    "\n",
    "print(\"Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This concludes the py-iku tutorial series. You've learned:\n",
    "\n",
    "1. **Basic Usage**: Converting pandas code to Dataiku flows\n",
    "2. **NumPy Support**: Using NumPy functions in transformations\n",
    "3. **Scikit-learn**: Converting ML pipelines\n",
    "4. **Visualizations**: Multiple output formats\n",
    "5. **Advanced Features**: Plugins, CLI, and DSS export\n",
    "\n",
    "For more information, see the documentation and examples in the repository."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-iku",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
