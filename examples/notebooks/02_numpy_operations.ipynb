{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# py-iku: NumPy Operations Support\n",
    "\n",
    "This notebook demonstrates how py-iku converts NumPy operations to Dataiku DSS recipes and processors.\n",
    "\n",
    "## Supported NumPy Operations\n",
    "\n",
    "py-iku supports 30+ NumPy functions across these categories:\n",
    "- Mathematical: log, exp, sqrt, power, abs, round\n",
    "- Conditional: where, isnan, isinf, isfinite\n",
    "- Aggregations: sum, mean, std, var, min, max, median\n",
    "- Array operations: concatenate, vstack, hstack, sort, unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2dataiku import convert\n",
    "from py2dataiku.parser.ast_analyzer import CodeAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mathematical Transformations\n",
    "\n",
    "NumPy math functions are converted to Dataiku Prepare recipe steps with formula processors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_code = '''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('metrics.csv')\n",
    "\n",
    "# Logarithmic transformations\n",
    "df['log_value'] = np.log(df['value'])\n",
    "df['log10_value'] = np.log10(df['value'])\n",
    "df['log1p_value'] = np.log1p(df['value'])  # log(1 + x)\n",
    "\n",
    "# Exponential\n",
    "df['exp_rate'] = np.exp(df['rate'])\n",
    "\n",
    "# Power functions\n",
    "df['sqrt_value'] = np.sqrt(df['value'])\n",
    "df['squared'] = np.power(df['value'], 2)\n",
    "df['cubed'] = np.power(df['value'], 3)\n",
    "\n",
    "df.to_csv('transformed_metrics.csv', index=False)\n",
    "'''\n",
    "\n",
    "# Analyze the transformations\n",
    "analyzer = CodeAnalyzer()\n",
    "transformations = analyzer.analyze(math_code)\n",
    "\n",
    "print(\"Detected Transformations:\")\n",
    "for t in transformations:\n",
    "    print(f\"  - {t.transformation_type.value}: {t.parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to flow\n",
    "flow = convert(math_code)\n",
    "print(flow.get_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rounding and Clipping\n",
    "\n",
    "NumPy's rounding and clipping functions map to Dataiku's ROUND_COLUMN and CLIP_COLUMN processors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_clip_code = '''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('scores.csv')\n",
    "\n",
    "# Rounding operations\n",
    "df['score_rounded'] = np.round(df['score'], 2)\n",
    "df['score_floor'] = np.floor(df['score'])\n",
    "df['score_ceil'] = np.ceil(df['score'])\n",
    "\n",
    "# Clipping (constraining values)\n",
    "df['score_clipped'] = np.clip(df['score'], 0, 100)  # Between 0 and 100\n",
    "df['positive_only'] = np.clip(df['value'], 0, None)  # Minimum 0\n",
    "\n",
    "# Absolute value\n",
    "df['abs_diff'] = np.abs(df['actual'] - df['predicted'])\n",
    "\n",
    "df.to_csv('processed_scores.csv', index=False)\n",
    "'''\n",
    "\n",
    "flow = convert(round_clip_code)\n",
    "print(flow.visualize(format='ascii'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conditional Operations\n",
    "\n",
    "`np.where()` is converted to Dataiku's formula-based column creation with if/else logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_code = '''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('transactions.csv')\n",
    "\n",
    "# Conditional column creation with np.where\n",
    "df['category'] = np.where(df['amount'] > 1000, 'high_value', 'standard')\n",
    "df['is_positive'] = np.where(df['balance'] > 0, 'positive', 'negative')\n",
    "\n",
    "# NaN checking\n",
    "df['has_missing'] = np.isnan(df['optional_field'])\n",
    "df['is_valid'] = np.isfinite(df['ratio'])\n",
    "\n",
    "# Replace NaN values\n",
    "df['cleaned_ratio'] = np.nan_to_num(df['ratio'], nan=0.0)\n",
    "\n",
    "df.to_csv('categorized_transactions.csv', index=False)\n",
    "'''\n",
    "\n",
    "# Analyze\n",
    "analyzer = CodeAnalyzer()\n",
    "transformations = analyzer.analyze(conditional_code)\n",
    "\n",
    "print(\"Conditional Transformations:\")\n",
    "for t in transformations:\n",
    "    if 'where' in str(t.notes).lower() or 'is' in str(t.parameters).lower():\n",
    "        print(f\"  - {t.transformation_type.value}\")\n",
    "        print(f\"    Parameters: {t.parameters}\")\n",
    "        print(f\"    Notes: {t.notes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Aggregation Functions\n",
    "\n",
    "NumPy aggregations are detected and can inform Grouping recipe creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_code = '''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('sales.csv')\n",
    "\n",
    "# Calculate statistics using NumPy\n",
    "total_sales = np.sum(df['amount'])\n",
    "avg_sale = np.mean(df['amount'])\n",
    "std_sale = np.std(df['amount'])\n",
    "median_sale = np.median(df['amount'])\n",
    "\n",
    "# Percentiles\n",
    "p25 = np.percentile(df['amount'], 25)\n",
    "p75 = np.percentile(df['amount'], 75)\n",
    "p90 = np.percentile(df['amount'], 90)\n",
    "\n",
    "# Min/Max\n",
    "min_sale = np.min(df['amount'])\n",
    "max_sale = np.max(df['amount'])\n",
    "\n",
    "print(f\"Statistics computed\")\n",
    "'''\n",
    "\n",
    "flow = convert(agg_code)\n",
    "print(flow.get_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering Pipeline\n",
    "\n",
    "A complete feature engineering pipeline combining NumPy and pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_code = '''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('raw_features.csv')\n",
    "\n",
    "# Log transform skewed features\n",
    "df['log_income'] = np.log1p(df['income'])\n",
    "df['log_transactions'] = np.log1p(df['num_transactions'])\n",
    "\n",
    "# Normalize features\n",
    "df['income_normalized'] = (df['income'] - np.mean(df['income'])) / np.std(df['income'])\n",
    "\n",
    "# Clip outliers\n",
    "df['income_clipped'] = np.clip(df['income'], \n",
    "                                np.percentile(df['income'], 1),\n",
    "                                np.percentile(df['income'], 99))\n",
    "\n",
    "# Handle special values\n",
    "df['ratio_clean'] = np.nan_to_num(df['ratio'], nan=0, posinf=1, neginf=-1)\n",
    "df['is_valid'] = np.isfinite(df['score']).astype(int)\n",
    "\n",
    "# Create categorical flags\n",
    "df['high_income'] = np.where(df['income'] > 100000, 1, 0)\n",
    "df['active_user'] = np.where(df['num_transactions'] > 10, 1, 0)\n",
    "\n",
    "# Save engineered features\n",
    "df.to_csv('engineered_features.csv', index=False)\n",
    "'''\n",
    "\n",
    "flow = convert(feature_code)\n",
    "print(\"Feature Engineering Flow:\")\n",
    "print(flow.visualize(format='ascii'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Array Operations\n",
    "\n",
    "NumPy array operations like concatenate and unique are also supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_code = '''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load multiple datasets\n",
    "df1 = pd.read_csv('data_2022.csv')\n",
    "df2 = pd.read_csv('data_2023.csv')\n",
    "\n",
    "# Vertical stack (concatenate)\n",
    "combined = np.vstack([df1.values, df2.values])\n",
    "\n",
    "# Get unique values\n",
    "unique_categories = np.unique(df1['category'].values)\n",
    "\n",
    "# Sort array\n",
    "sorted_values = np.sort(df1['value'].values)\n",
    "\n",
    "print(\"Array operations complete\")\n",
    "'''\n",
    "\n",
    "analyzer = CodeAnalyzer()\n",
    "transformations = analyzer.analyze(array_code)\n",
    "\n",
    "print(\"Array Operations Detected:\")\n",
    "for t in transformations:\n",
    "    print(f\"  - {t.transformation_type.value}: {t.notes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Supported NumPy Functions Reference\n",
    "\n",
    "Here's a complete list of supported NumPy functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_functions = {\n",
    "    \"Mathematical\": [\n",
    "        \"np.log\", \"np.log10\", \"np.log2\", \"np.log1p\",\n",
    "        \"np.exp\", \"np.expm1\",\n",
    "        \"np.sqrt\", \"np.cbrt\", \"np.square\", \"np.power\",\n",
    "        \"np.abs\", \"np.absolute\"\n",
    "    ],\n",
    "    \"Rounding\": [\n",
    "        \"np.round\", \"np.around\", \"np.rint\",\n",
    "        \"np.floor\", \"np.ceil\", \"np.trunc\"\n",
    "    ],\n",
    "    \"Clipping\": [\"np.clip\"],\n",
    "    \"Conditional\": [\n",
    "        \"np.where\",\n",
    "        \"np.isnan\", \"np.isinf\", \"np.isfinite\",\n",
    "        \"np.nan_to_num\", \"np.nanmean\", \"np.nansum\", \"np.nanstd\"\n",
    "    ],\n",
    "    \"Aggregation\": [\n",
    "        \"np.sum\", \"np.mean\", \"np.std\", \"np.var\",\n",
    "        \"np.min\", \"np.max\", \"np.median\",\n",
    "        \"np.percentile\", \"np.quantile\"\n",
    "    ],\n",
    "    \"Array Operations\": [\n",
    "        \"np.concatenate\", \"np.vstack\", \"np.hstack\", \"np.stack\",\n",
    "        \"np.sort\", \"np.argsort\", \"np.unique\"\n",
    "    ],\n",
    "    \"Reshaping\": [\n",
    "        \"np.reshape\", \"np.flatten\", \"np.ravel\", \"np.transpose\"\n",
    "    ],\n",
    "    \"Creation\": [\n",
    "        \"np.zeros\", \"np.ones\", \"np.full\", \"np.empty\",\n",
    "        \"np.arange\", \"np.linspace\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, functions in numpy_functions.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for func in functions:\n",
    "        print(f\"  - {func}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- See `03_sklearn_pipelines.ipynb` for scikit-learn ML pipeline support\n",
    "- See `04_visualizations.ipynb` for visualization options"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
