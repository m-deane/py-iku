{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# py-iku: Scikit-learn Pipeline Support\n",
    "\n",
    "This notebook demonstrates how py-iku converts scikit-learn ML pipelines to Dataiku DSS recipes.\n",
    "\n",
    "## Supported sklearn Components\n",
    "\n",
    "- **Scalers**: StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler, Normalizer\n",
    "- **Encoders**: LabelEncoder, OneHotEncoder, OrdinalEncoder, LabelBinarizer\n",
    "- **Imputers**: SimpleImputer, KNNImputer, IterativeImputer\n",
    "- **Feature Selection**: PCA, TruncatedSVD, SelectKBest, SelectFromModel\n",
    "- **Utilities**: train_test_split, Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2dataiku import convert\n",
    "from py2dataiku.parser.ast_analyzer import CodeAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Scaling\n",
    "\n",
    "Scikit-learn scalers are converted to Dataiku's Normalizer processor in Prepare recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_code = '''\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "df = pd.read_csv('features.csv')\n",
    "\n",
    "# Standard scaling (z-score normalization)\n",
    "scaler = StandardScaler()\n",
    "df[['feature1', 'feature2']] = scaler.fit_transform(df[['feature1', 'feature2']])\n",
    "\n",
    "# Min-Max scaling (0-1 range)\n",
    "minmax = MinMaxScaler()\n",
    "df[['feature3']] = minmax.fit_transform(df[['feature3']])\n",
    "\n",
    "# Robust scaling (handles outliers)\n",
    "robust = RobustScaler()\n",
    "df[['feature4']] = robust.fit_transform(df[['feature4']])\n",
    "\n",
    "df.to_csv('scaled_features.csv', index=False)\n",
    "'''\n",
    "\n",
    "# Analyze the transformations\n",
    "analyzer = CodeAnalyzer()\n",
    "transformations = analyzer.analyze(scaling_code)\n",
    "\n",
    "print(\"Scaling Transformations Detected:\")\n",
    "for t in transformations:\n",
    "    if 'scal' in str(t.notes).lower() or 'scal' in str(t.parameters).lower():\n",
    "        print(f\"  - Type: {t.transformation_type.value}\")\n",
    "        print(f\"    Parameters: {t.parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to flow\n",
    "flow = convert(scaling_code)\n",
    "print(flow.get_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Categorical Encoding\n",
    "\n",
    "Label and one-hot encoding are converted to Dataiku's categorical encoding processors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_code = '''\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "df = pd.read_csv('categorical_data.csv')\n",
    "\n",
    "# Label encoding for ordinal categories\n",
    "le = LabelEncoder()\n",
    "df['category_encoded'] = le.fit_transform(df['category'])\n",
    "\n",
    "# One-hot encoding for nominal categories\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "encoded = ohe.fit_transform(df[['region']])\n",
    "\n",
    "# Ordinal encoding with custom order\n",
    "oe = OrdinalEncoder(categories=[['low', 'medium', 'high']])\n",
    "df['priority_encoded'] = oe.fit_transform(df[['priority']])\n",
    "\n",
    "df.to_csv('encoded_data.csv', index=False)\n",
    "'''\n",
    "\n",
    "flow = convert(encoding_code)\n",
    "print(flow.visualize(format='ascii'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Missing Value Imputation\n",
    "\n",
    "Scikit-learn imputers map to Dataiku's fill empty processors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputation_code = '''\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "df = pd.read_csv('data_with_missing.csv')\n",
    "\n",
    "# Mean imputation\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "df[['numeric_col']] = mean_imputer.fit_transform(df[['numeric_col']])\n",
    "\n",
    "# Median imputation\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "df[['income']] = median_imputer.fit_transform(df[['income']])\n",
    "\n",
    "# Most frequent (mode) imputation\n",
    "mode_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[['category']] = mode_imputer.fit_transform(df[['category']])\n",
    "\n",
    "# Constant value imputation\n",
    "const_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "df[['optional_field']] = const_imputer.fit_transform(df[['optional_field']])\n",
    "\n",
    "# KNN-based imputation (requires Python recipe in Dataiku)\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "df[['feature1', 'feature2']] = knn_imputer.fit_transform(df[['feature1', 'feature2']])\n",
    "\n",
    "df.to_csv('imputed_data.csv', index=False)\n",
    "'''\n",
    "\n",
    "analyzer = CodeAnalyzer()\n",
    "transformations = analyzer.analyze(imputation_code)\n",
    "\n",
    "print(\"Imputation Transformations:\")\n",
    "for t in transformations:\n",
    "    if 'impute' in str(t.notes).lower() or 'impute' in str(t.parameters).lower():\n",
    "        print(f\"  - {t.transformation_type.value}\")\n",
    "        print(f\"    Strategy: {t.parameters.get('strategy', 'N/A')}\")\n",
    "        print(f\"    Notes: {t.notes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train-Test Split\n",
    "\n",
    "`train_test_split` is converted to a Dataiku Split recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_code = '''\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('ml_dataset.csv')\n",
    "\n",
    "# Split into training and test sets\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Save splits\n",
    "X_train.to_csv('train_features.csv', index=False)\n",
    "X_test.to_csv('test_features.csv', index=False)\n",
    "'''\n",
    "\n",
    "flow = convert(split_code)\n",
    "print(\"Train-Test Split Flow:\")\n",
    "print(flow.visualize(format='ascii'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Selection\n",
    "\n",
    "Dimensionality reduction and feature selection components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection_code = '''\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "df = pd.read_csv('high_dim_data.csv')\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# PCA for dimensionality reduction\n",
    "pca = PCA(n_components=10)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# TruncatedSVD for sparse data\n",
    "svd = TruncatedSVD(n_components=5)\n",
    "X_svd = svd.fit_transform(X)\n",
    "\n",
    "# Select K best features\n",
    "selector = SelectKBest(score_func=f_classif, k=20)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "# Save reduced features\n",
    "pd.DataFrame(X_pca).to_csv('pca_features.csv', index=False)\n",
    "'''\n",
    "\n",
    "analyzer = CodeAnalyzer()\n",
    "transformations = analyzer.analyze(feature_selection_code)\n",
    "\n",
    "print(\"Feature Selection Operations:\")\n",
    "for t in transformations:\n",
    "    if 'pca' in str(t.notes).lower() or 'select' in str(t.notes).lower():\n",
    "        print(f\"  - {t.transformation_type.value}\")\n",
    "        print(f\"    Parameters: {t.parameters}\")\n",
    "        print(f\"    Notes: {t.notes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete ML Pipeline\n",
    "\n",
    "A full ML preprocessing pipeline combining multiple sklearn components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline_code = '''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load raw data\n",
    "df = pd.read_csv('raw_ml_data.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X.select_dtypes(include=[np.number]))\n",
    "\n",
    "# Encode categorical target\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# Reduce dimensionality\n",
    "pca = PCA(n_components=0.95)  # Keep 95% variance\n",
    "X_reduced = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_reduced, y_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Save processed data\n",
    "pd.DataFrame(X_train).to_csv('X_train.csv', index=False)\n",
    "pd.DataFrame(X_test).to_csv('X_test.csv', index=False)\n",
    "pd.DataFrame(y_train).to_csv('y_train.csv', index=False)\n",
    "pd.DataFrame(y_test).to_csv('y_test.csv', index=False)\n",
    "'''\n",
    "\n",
    "flow = convert(full_pipeline_code)\n",
    "print(\"Complete ML Pipeline:\")\n",
    "print(flow.get_summary())\n",
    "print(\"\\n\" + flow.visualize(format='ascii'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. sklearn Pipeline Object\n",
    "\n",
    "sklearn's Pipeline object is also recognized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_pipeline_code = '''\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "X = df.drop('target', axis=1)\n",
    "\n",
    "# Create sklearn Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=10))\n",
    "])\n",
    "\n",
    "# Fit and transform\n",
    "X_processed = pipeline.fit_transform(X)\n",
    "\n",
    "pd.DataFrame(X_processed).to_csv('processed_data.csv', index=False)\n",
    "'''\n",
    "\n",
    "analyzer = CodeAnalyzer()\n",
    "transformations = analyzer.analyze(sklearn_pipeline_code)\n",
    "\n",
    "print(\"sklearn Pipeline Detected:\")\n",
    "for t in transformations:\n",
    "    if 'pipeline' in str(t.notes).lower():\n",
    "        print(f\"  - {t.transformation_type.value}\")\n",
    "        print(f\"    Notes: {t.notes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Dataiku Recipe Mappings\n",
    "\n",
    "How sklearn components map to Dataiku recipes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = {\n",
    "    \"sklearn Component\": [\n",
    "        \"StandardScaler\",\n",
    "        \"MinMaxScaler\",\n",
    "        \"RobustScaler\",\n",
    "        \"LabelEncoder\",\n",
    "        \"OneHotEncoder\",\n",
    "        \"SimpleImputer(mean)\",\n",
    "        \"SimpleImputer(median)\",\n",
    "        \"SimpleImputer(constant)\",\n",
    "        \"KNNImputer\",\n",
    "        \"train_test_split\",\n",
    "        \"PCA\",\n",
    "        \"SelectKBest\",\n",
    "        \"Pipeline\"\n",
    "    ],\n",
    "    \"Dataiku Recipe/Processor\": [\n",
    "        \"Prepare > Normalizer (Z_SCORE)\",\n",
    "        \"Prepare > Normalizer (MIN_MAX)\",\n",
    "        \"Prepare > Normalizer (ROBUST)\",\n",
    "        \"Prepare > CategoricalEncoder\",\n",
    "        \"Prepare > CategoricalEncoder (ONE_HOT)\",\n",
    "        \"Prepare > FillEmptyWithComputedValue (MEAN)\",\n",
    "        \"Prepare > FillEmptyWithComputedValue (MEDIAN)\",\n",
    "        \"Prepare > FillEmptyWithValue\",\n",
    "        \"Python Recipe (advanced imputation)\",\n",
    "        \"Split Recipe\",\n",
    "        \"Python Recipe (dimensionality reduction)\",\n",
    "        \"Python Recipe (feature selection)\",\n",
    "        \"Python Recipe (multi-step pipeline)\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame(mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- See `04_visualizations.ipynb` for visualization formats\n",
    "- See `05_advanced_features.ipynb` for plugins and DSS export"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
