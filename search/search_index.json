{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"py-iku","text":"<p>Convert Python data processing code to Dataiku DSS recipes and flows.</p> <p>py-iku analyzes your Python code (pandas, numpy, scikit-learn) and generates equivalent Dataiku DSS recipe configurations, flow structures, and visual diagrams.</p>"},{"location":"#two-analysis-modes","title":"Two Analysis Modes","text":"LLM-based (Recommended)Rule-based (Offline) <p>Uses AI (Anthropic Claude or OpenAI GPT) to understand code semantics and produce accurate Dataiku mappings.</p> <pre><code>from py2dataiku import convert_with_llm\n\nflow = convert_with_llm(\"\"\"\nimport pandas as pd\ndf = pd.read_csv('sales.csv')\ndf = df.dropna(subset=['amount'])\nresult = df.groupby('region').agg({'amount': 'sum'})\nresult.to_csv('summary.csv')\n\"\"\", provider=\"anthropic\")\n\nprint(flow.visualize(format=\"ascii\"))\n</code></pre> <p>Uses AST pattern matching for fast, deterministic conversion without API calls.</p> <pre><code>from py2dataiku import convert\n\nflow = convert(\"\"\"\nimport pandas as pd\ndf = pd.read_csv('sales.csv')\ndf = df.dropna(subset=['amount'])\nresult = df.groupby('region').agg({'amount': 'sum'})\nresult.to_csv('summary.csv')\n\"\"\")\n\nprint(flow.visualize(format=\"ascii\"))\n</code></pre>"},{"location":"#features","title":"Features","text":"<ul> <li>37 recipe types - Visual, code, ML, and plugin recipes</li> <li>122 processor types - Complete Dataiku Prepare recipe processor coverage</li> <li>5 visualization formats - SVG, HTML, ASCII, Mermaid, PlantUML</li> <li>Round-trip serialization - JSON, YAML, and dict export/import</li> <li>DAG analysis - Topological sort, cycle detection, column lineage</li> <li>DSS project export - Generate Dataiku-importable project bundles</li> <li>Plugin system - Extend with custom recipe/processor handlers</li> <li>Scenario &amp; metrics - Automation triggers, data quality checks</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Installation</li> <li>Quick Start Guide</li> <li>API Reference</li> <li>GitHub Repository</li> </ul>"},{"location":"api/","title":"py-iku API Reference","text":"<p>API documentation for py-iku v0.3.0 - Convert Python data processing code to Dataiku DSS recipes and flows.</p>"},{"location":"api/#modules","title":"Modules","text":"Module Description Core Functions Top-level convenience functions: <code>convert()</code>, <code>convert_with_llm()</code>, <code>convert_file()</code> Py2Dataiku Class Main converter class with hybrid LLM + rule-based approach Models Core data models: <code>DataikuFlow</code>, <code>DataikuRecipe</code>, <code>DataikuDataset</code>, <code>PrepareStep</code> Enums All enum types: <code>RecipeType</code>, <code>ProcessorType</code>, <code>DatasetType</code>, and 25+ more LLM Providers LLM integration: <code>AnthropicProvider</code>, <code>OpenAIProvider</code>, <code>LLMCodeAnalyzer</code> Visualizers Visualization engines: SVG, HTML, ASCII, Mermaid, PlantUML Exporters DSS project export: <code>DSSExporter</code>, <code>DSSProjectConfig</code> Plugin System Extension system: <code>PluginRegistry</code>, decorators, custom handlers Configuration Config system: <code>Py2DataikuConfig</code>, file discovery, environment variables Exceptions Exception hierarchy: <code>Py2DataikuError</code> and subclasses Graph DAG operations: <code>FlowGraph</code>, topological sort, cycle detection Scenarios &amp; Metrics Automation: scenarios, triggers, metrics, checks, data quality rules MLOps ML operations: API endpoints, model versions, drift detection Recipe Settings Typed settings: <code>RecipeSettings</code> ABC with 12 subclasses"},{"location":"api/#quick-start","title":"Quick Start","text":"<pre><code>from py2dataiku import convert, convert_with_llm\n\n# Rule-based conversion (fast, offline)\nflow = convert(\"\"\"\nimport pandas as pd\ndf = pd.read_csv('data.csv')\ndf = df.dropna()\nresult = df.groupby('category').agg({'amount': 'sum'})\n\"\"\")\n\n# LLM-based conversion (more accurate)\nflow = convert_with_llm(code, provider=\"anthropic\")\n\n# Inspect the result\nprint(flow.get_summary())\nprint(flow.visualize(format=\"ascii\"))\n\n# Export\nflow.to_json()\nflow.to_yaml()\n</code></pre>"},{"location":"api/#package-structure","title":"Package Structure","text":"<pre><code>py2dataiku\n\u251c\u2500\u2500 convert()                    # Rule-based conversion\n\u251c\u2500\u2500 convert_with_llm()           # LLM-based conversion\n\u251c\u2500\u2500 convert_file()               # File-based rule conversion\n\u251c\u2500\u2500 convert_file_with_llm()      # File-based LLM conversion\n\u251c\u2500\u2500 Py2Dataiku                   # Main converter class\n\u251c\u2500\u2500 models\n\u2502   \u251c\u2500\u2500 DataikuFlow              # Flow container\n\u2502   \u251c\u2500\u2500 DataikuRecipe            # Recipe node\n\u2502   \u251c\u2500\u2500 DataikuDataset           # Dataset node\n\u2502   \u251c\u2500\u2500 PrepareStep              # Processor step\n\u2502   \u251c\u2500\u2500 FlowGraph                # DAG representation\n\u2502   \u251c\u2500\u2500 RecipeSettings           # Typed settings (ABC)\n\u2502   \u251c\u2500\u2500 DataikuScenario          # Automation\n\u2502   \u251c\u2500\u2500 DataikuMetric            # Metrics\n\u2502   \u2514\u2500\u2500 APIEndpoint              # MLOps\n\u251c\u2500\u2500 llm\n\u2502   \u251c\u2500\u2500 LLMCodeAnalyzer          # AI analysis\n\u2502   \u251c\u2500\u2500 AnthropicProvider        # Claude integration\n\u2502   \u2514\u2500\u2500 OpenAIProvider           # GPT integration\n\u251c\u2500\u2500 visualizers\n\u2502   \u251c\u2500\u2500 SVGVisualizer            # Pixel-accurate\n\u2502   \u251c\u2500\u2500 HTMLVisualizer           # Interactive\n\u2502   \u251c\u2500\u2500 ASCIIVisualizer          # Terminal\n\u2502   \u251c\u2500\u2500 MermaidVisualizer        # GitHub/Notion\n\u2502   \u2514\u2500\u2500 PlantUMLVisualizer       # Documentation\n\u251c\u2500\u2500 exporters\n\u2502   \u2514\u2500\u2500 DSSExporter              # DSS project export\n\u251c\u2500\u2500 plugins\n\u2502   \u2514\u2500\u2500 PluginRegistry           # Extension system\n\u251c\u2500\u2500 config\n\u2502   \u2514\u2500\u2500 Py2DataikuConfig         # Configuration\n\u2514\u2500\u2500 exceptions\n    \u2514\u2500\u2500 Py2DataikuError          # Error hierarchy\n</code></pre>"},{"location":"api/configuration/","title":"Configuration","text":"<p>Configuration system for py-iku with file-based and environment variable support.</p>"},{"location":"api/configuration/#py2dataikuconfig","title":"Py2DataikuConfig","text":"<p>Configuration dataclass with all library settings.</p> <pre><code>from py2dataiku import Py2DataikuConfig, load_config, find_config_file\n</code></pre>"},{"location":"api/configuration/#fields","title":"Fields","text":"Field Type Default Description <code>default_provider</code> <code>str</code> <code>\"anthropic\"</code> Default LLM provider <code>default_model</code> <code>Optional[str]</code> <code>None</code> Default model name <code>api_key</code> <code>Optional[str]</code> <code>None</code> API key <code>project_key</code> <code>str</code> <code>\"MY_PROJECT\"</code> DSS project key <code>flow_name</code> <code>str</code> <code>\"converted_flow\"</code> Default flow name <code>optimize</code> <code>bool</code> <code>True</code> Enable flow optimization <code>optimization_level</code> <code>int</code> <code>1</code> 0=none, 1=basic, 2=aggressive <code>dataset_prefix</code> <code>str</code> <code>\"\"</code> Prefix for dataset names <code>dataset_suffix</code> <code>str</code> <code>\"\"</code> Suffix for dataset names <code>recipe_prefix</code> <code>str</code> <code>\"\"</code> Prefix for recipe names <code>recipe_suffix</code> <code>str</code> <code>\"\"</code> Suffix for recipe names <code>default_format</code> <code>str</code> <code>\"svg\"</code> Default visualization format <code>default_connection</code> <code>str</code> <code>\"Filesystem\"</code> Default data connection <code>extra</code> <code>Dict[str, Any]</code> <code>{}</code> Additional settings"},{"location":"api/configuration/#methods","title":"Methods","text":"<pre><code>config.to_dict()                      # -&gt; Dict[str, Any]\nPy2DataikuConfig.from_dict(data)      # -&gt; Py2DataikuConfig (classmethod)\n</code></pre>"},{"location":"api/configuration/#load_config","title":"load_config()","text":"<p>Load configuration from file or defaults.</p> <pre><code>def load_config(\n    config_path: Optional[str] = None,\n    auto_discover: bool = True,\n) -&gt; Py2DataikuConfig\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>config_path</code> <code>Optional[str]</code> <code>None</code> Explicit path to config file <code>auto_discover</code> <code>bool</code> <code>True</code> Search standard locations <p>Returns: <code>Py2DataikuConfig</code></p> <p>Behavior: 1. If <code>config_path</code> is provided, loads from that file 2. If <code>auto_discover=True</code>, searches for config files (see <code>find_config_file()</code>) 3. Falls back to default values 4. Environment variables override file values</p>"},{"location":"api/configuration/#find_config_file","title":"find_config_file()","text":"<p>Search for configuration files in standard locations.</p> <pre><code>def find_config_file(start_dir: Optional[str] = None) -&gt; Optional[Path]\n</code></pre> <p>Search order: 1. <code>py2dataiku.toml</code> (current directory, walking up to root) 2. <code>.py2dataikurc</code> (current directory, walking up to root) 3. <code>.py2dataiku.yaml</code> / <code>.py2dataiku.yml</code> (current directory, walking up to root) 4. <code>~/.config/py2dataiku/config.toml</code> 5. <code>~/.py2dataikurc</code></p> <p>Returns: <code>Optional[Path]</code> - Path to first config file found, or <code>None</code></p>"},{"location":"api/configuration/#config-file-formats","title":"Config File Formats","text":""},{"location":"api/configuration/#toml-py2dataikutoml","title":"TOML (<code>py2dataiku.toml</code>)","text":"<pre><code>[py2dataiku]\ndefault_provider = \"anthropic\"\ndefault_model = \"claude-sonnet-4-20250514\"\nproject_key = \"SALES_PIPELINE\"\noptimize = true\noptimization_level = 2\ndefault_format = \"svg\"\ndefault_connection = \"postgresql_prod\"\ndataset_prefix = \"ds_\"\nrecipe_prefix = \"r_\"\n</code></pre>"},{"location":"api/configuration/#yaml-py2dataikuyaml","title":"YAML (<code>.py2dataiku.yaml</code>)","text":"<pre><code>py2dataiku:\n  default_provider: anthropic\n  project_key: SALES_PIPELINE\n  optimize: true\n  optimization_level: 2\n  default_format: svg\n</code></pre>"},{"location":"api/configuration/#environment-variables","title":"Environment Variables","text":"<p>Environment variables override file-based configuration:</p> Variable Config Field Description <code>PY2DATAIKU_PROVIDER</code> <code>default_provider</code> LLM provider <code>PY2DATAIKU_MODEL</code> <code>default_model</code> Model name <code>ANTHROPIC_API_KEY</code> <code>api_key</code> Anthropic API key <code>OPENAI_API_KEY</code> <code>api_key</code> OpenAI API key <code>PY2DATAIKU_PROJECT_KEY</code> <code>project_key</code> DSS project key <code>PY2DATAIKU_OPTIMIZE</code> <code>optimize</code> Enable optimization"},{"location":"api/core-functions/","title":"Core Functions","text":"<p>Top-level convenience functions for converting Python code to Dataiku DSS flows.</p> <p>All functions are importable directly from <code>py2dataiku</code>:</p> <pre><code>from py2dataiku import convert, convert_with_llm, convert_file, convert_file_with_llm\n</code></pre>"},{"location":"api/core-functions/#convert","title":"<code>convert()</code>","text":"<p>Convert Python code to a Dataiku flow using rule-based AST analysis.</p> <pre><code>def convert(code: str, optimize: bool = True) -&gt; DataikuFlow\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>code</code> <code>str</code> required Python source code string <code>optimize</code> <code>bool</code> <code>True</code> Whether to optimize the flow (merge recipes, remove orphan datasets) <p>Returns: <code>DataikuFlow</code> - The converted pipeline</p> <p>Example:</p> <pre><code>from py2dataiku import convert\n\nflow = convert(\"\"\"\nimport pandas as pd\ndf = pd.read_csv('sales.csv')\ndf = df.dropna(subset=['amount'])\ndf['amount'] = df['amount'].round(2)\nresult = df.groupby('region').agg({'amount': 'sum'})\nresult.to_csv('summary.csv')\n\"\"\")\n\nprint(flow.get_summary())\n# Datasets: 2 (1 input, 1 output)\n# Recipes: 2 (1 prepare, 1 grouping)\n</code></pre> <p>Notes: - This is the legacy method using AST pattern matching - Fast and deterministic but less accurate for complex code - For better results with complex pipelines, use <code>convert_with_llm()</code></p>"},{"location":"api/core-functions/#convert_with_llm","title":"<code>convert_with_llm()</code>","text":"<p>Convert Python code to a Dataiku flow using LLM-based semantic analysis.</p> <pre><code>def convert_with_llm(\n    code: str,\n    provider: str = \"anthropic\",\n    api_key: Optional[str] = None,\n    model: Optional[str] = None,\n    optimize: bool = True,\n    flow_name: str = \"converted_flow\",\n) -&gt; DataikuFlow\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>code</code> <code>str</code> required Python source code string <code>provider</code> <code>str</code> <code>\"anthropic\"</code> LLM provider: <code>\"anthropic\"</code> or <code>\"openai\"</code> <code>api_key</code> <code>Optional[str]</code> <code>None</code> API key (uses <code>ANTHROPIC_API_KEY</code> or <code>OPENAI_API_KEY</code> env var if not provided) <code>model</code> <code>Optional[str]</code> <code>None</code> Model name (provider default if not specified) <code>optimize</code> <code>bool</code> <code>True</code> Whether to optimize the flow <code>flow_name</code> <code>str</code> <code>\"converted_flow\"</code> Name for the generated flow <p>Returns: <code>DataikuFlow</code> - The converted pipeline</p> <p>Raises: - <code>ProviderError</code> - If LLM communication fails - <code>LLMResponseParseError</code> - If LLM response cannot be parsed - <code>ConversionError</code> - If flow generation fails</p> <p>Example:</p> <pre><code>from py2dataiku import convert_with_llm\n\nflow = convert_with_llm(\"\"\"\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndf = pd.read_csv('customers.csv')\ndf = df.merge(pd.read_csv('orders.csv'), on='customer_id')\ndf['total_spent'] = df.groupby('customer_id')['amount'].transform('sum')\nscaler = StandardScaler()\ndf[['total_spent']] = scaler.fit_transform(df[['total_spent']])\n\"\"\", provider=\"anthropic\")\n\nprint(flow.visualize(format=\"ascii\"))\n</code></pre> <p>Notes: - This is the recommended method for production use - Requires an API key (set via parameter or environment variable) - Default models: <code>claude-sonnet-4-20250514</code> (Anthropic), <code>gpt-4o</code> (OpenAI)</p>"},{"location":"api/core-functions/#convert_file","title":"<code>convert_file()</code>","text":"<p>Convert a Python file to a Dataiku flow using rule-based analysis.</p> <pre><code>def convert_file(path: str, optimize: bool = True) -&gt; DataikuFlow\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>path</code> <code>str</code> required Path to a Python file <code>optimize</code> <code>bool</code> <code>True</code> Whether to optimize the flow <p>Returns: <code>DataikuFlow</code> - The converted pipeline (with <code>source_file</code> set)</p> <p>Example:</p> <pre><code>from py2dataiku import convert_file\n\nflow = convert_file(\"pipelines/etl_pipeline.py\")\nprint(f\"Source: {flow.source_file}\")\nprint(flow.get_summary())\n</code></pre>"},{"location":"api/core-functions/#convert_file_with_llm","title":"<code>convert_file_with_llm()</code>","text":"<p>Convert a Python file to a Dataiku flow using LLM-based analysis.</p> <pre><code>def convert_file_with_llm(\n    path: str,\n    provider: str = \"anthropic\",\n    api_key: Optional[str] = None,\n    model: Optional[str] = None,\n    optimize: bool = True,\n    flow_name: Optional[str] = None,\n) -&gt; DataikuFlow\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>path</code> <code>str</code> required Path to a Python file <code>provider</code> <code>str</code> <code>\"anthropic\"</code> LLM provider name <code>api_key</code> <code>Optional[str]</code> <code>None</code> API key <code>model</code> <code>Optional[str]</code> <code>None</code> Model name <code>optimize</code> <code>bool</code> <code>True</code> Whether to optimize the flow <code>flow_name</code> <code>Optional[str]</code> <code>None</code> Flow name (defaults to filename without extension) <p>Returns: <code>DataikuFlow</code> - The converted pipeline (with <code>source_file</code> set)</p> <p>Example:</p> <pre><code>from py2dataiku import convert_file_with_llm\n\nflow = convert_file_with_llm(\n    \"pipelines/etl_pipeline.py\",\n    provider=\"anthropic\",\n    flow_name=\"etl_flow\"\n)\nflow.to_json()\n</code></pre>"},{"location":"api/enums/","title":"Enums","text":"<p>All enumeration types used throughout the library.</p>"},{"location":"api/enums/#recipetype","title":"RecipeType","text":"<p>Dataiku recipe types. Visual recipes appear as colored circles in the flow.</p> <pre><code>from py2dataiku import RecipeType\n</code></pre>"},{"location":"api/enums/#visual-recipes-data-preparation","title":"Visual Recipes - Data Preparation","text":"Value API String Description <code>PREPARE</code> <code>\"prepare\"</code> Data preparation with processors <code>SYNC</code> <code>\"sync\"</code> Dataset synchronization <code>GROUPING</code> <code>\"grouping\"</code> Group-by with aggregations <code>WINDOW</code> <code>\"window\"</code> Window functions <code>JOIN</code> <code>\"join\"</code> Standard join <code>FUZZY_JOIN</code> <code>\"fuzzy_join\"</code> Fuzzy matching join <code>GEO_JOIN</code> <code>\"geo_join\"</code> Geographic join <code>STACK</code> <code>\"stack\"</code> Vertical concatenation <code>SPLIT</code> <code>\"split\"</code> Split into multiple outputs <code>SORT</code> <code>\"sort\"</code> Sort rows <code>DISTINCT</code> <code>\"distinct\"</code> Remove duplicates <code>TOP_N</code> <code>\"topn\"</code> Top N rows <code>PIVOT</code> <code>\"pivot\"</code> Pivot table <code>SAMPLING</code> <code>\"sampling\"</code> Sample rows <code>DOWNLOAD</code> <code>\"download\"</code> Download data"},{"location":"api/enums/#visual-recipes-additional","title":"Visual Recipes - Additional","text":"Value API String Description <code>GENERATE_FEATURES</code> <code>\"generate_features\"</code> Auto feature generation <code>GENERATE_STATISTICS</code> <code>\"generate_statistics\"</code> Statistical analysis <code>PUSH_TO_EDITABLE</code> <code>\"push_to_editable\"</code> Push to editable dataset <code>LIST_FOLDER_CONTENTS</code> <code>\"list_folder_contents\"</code> List managed folder <code>DYNAMIC_REPEAT</code> <code>\"dynamic_repeat\"</code> Dynamic loop <code>EXTRACT_FAILED_ROWS</code> <code>\"extract_failed_rows\"</code> Extract failed rows <code>UPSERT</code> <code>\"upsert\"</code> Upsert operation <code>LIST_ACCESS</code> <code>\"list_access\"</code> List dataset access"},{"location":"api/enums/#code-recipes","title":"Code Recipes","text":"Value API String Description <code>PYTHON</code> <code>\"python\"</code> Python code recipe <code>R</code> <code>\"r\"</code> R code recipe <code>SQL</code> <code>\"sql_query\"</code> SQL query <code>HIVE</code> <code>\"hive\"</code> Hive query <code>IMPALA</code> <code>\"impala\"</code> Impala query <code>SPARKSQL</code> <code>\"sparksql\"</code> Spark SQL <code>PYSPARK</code> <code>\"pyspark\"</code> PySpark code <code>SPARK_SCALA</code> <code>\"spark_scala\"</code> Spark Scala <code>SPARKR</code> <code>\"sparkr\"</code> SparkR code <code>SHELL</code> <code>\"shell\"</code> Shell script"},{"location":"api/enums/#ml-recipes","title":"ML Recipes","text":"Value API String Description <code>PREDICTION_SCORING</code> <code>\"prediction_scoring\"</code> Score with prediction model <code>CLUSTERING_SCORING</code> <code>\"clustering_scoring\"</code> Score with clustering model <code>EVALUATION</code> <code>\"evaluation\"</code> Model evaluation"},{"location":"api/enums/#ai-assisted","title":"AI-Assisted","text":"Value API String Description <code>AI_ASSISTANT_GENERATE</code> <code>\"ai_assistant_generate\"</code> AI assistant recipe"},{"location":"api/enums/#processortype","title":"ProcessorType","text":"<p>Dataiku Prepare recipe processor types. 122 types total.</p> <pre><code>from py2dataiku import ProcessorType\n</code></pre>"},{"location":"api/enums/#column-manipulation","title":"Column Manipulation","text":"Value Description <code>COLUMN_RENAMER</code> Rename columns <code>COLUMN_COPIER</code> Copy column <code>COLUMN_DELETER</code> Delete columns <code>COLUMNS_SELECTOR</code> Select columns <code>COLUMN_REORDER</code> Reorder columns <code>COLUMNS_CONCATENATOR</code> Concatenate column values"},{"location":"api/enums/#missing-value-handling","title":"Missing Value Handling","text":"Value Description <code>FILL_EMPTY_WITH_VALUE</code> Fill with constant value <code>REMOVE_ROWS_ON_EMPTY</code> Remove rows with empty values <code>FILL_EMPTY_WITH_PREVIOUS_NEXT</code> Fill with adjacent values <code>FILL_EMPTY_WITH_COMPUTED_VALUE</code> Fill with computed value <code>IMPUTE_WITH_ML</code> ML-based imputation"},{"location":"api/enums/#string-transformations","title":"String Transformations","text":"Value Description <code>STRING_TRANSFORMER</code> General string transform (see <code>StringTransformerMode</code>) <code>TOKENIZER</code> Split text into tokens <code>REGEXP_EXTRACTOR</code> Extract with regex <code>FIND_REPLACE</code> Find and replace <code>SPLIT_COLUMN</code> Split column by delimiter <code>CONCAT_COLUMNS</code> Concatenate columns <code>HTML_STRIPPER</code> Strip HTML tags <code>MULTI_COLUMN_FIND_REPLACE</code> Multi-column find/replace <code>NGRAMMER</code> Generate n-grams <code>TEXT_SIMPLIFIER</code> Simplify text <code>STEM_TEXT</code> Stem words <code>LEMMATIZE_TEXT</code> Lemmatize words <code>LANGUAGE_DETECTOR</code> Detect language <code>SENTIMENT_ANALYZER</code> Analyze sentiment <code>TEXT_HASHER</code> Hash text <code>UNICODE_NORMALIZER</code> Normalize Unicode <code>URL_PARSER</code> Parse URL components <code>IP_ADDRESS_PARSER</code> Parse IP addresses <code>EMAIL_DOMAIN_EXTRACTOR</code> Extract email domain <code>PHONE_FORMATTER</code> Format phone numbers <code>COUNTRY_NORMALIZER</code> Normalize country names <code>USER_AGENT_PARSER</code> Parse user agent strings"},{"location":"api/enums/#numeric-transformations","title":"Numeric Transformations","text":"Value Description <code>NUMERICAL_TRANSFORMER</code> General numeric transform (see <code>NumericalTransformerMode</code>) <code>ROUND_COLUMN</code> Round values <code>ABS_COLUMN</code> Absolute value <code>CLIP_COLUMN</code> Clip to range <code>BINNER</code> Bin into buckets <code>NORMALIZER</code> Normalize values <code>DISCRETIZER</code> Discretize continuous values <code>QUANTILE_TRANSFORMER</code> Quantile transform <code>ROBUST_SCALER</code> Robust scaling <code>MIN_MAX_SCALER</code> Min-max scaling <code>STANDARD_SCALER</code> Standard scaling (z-score) <code>LOG_TRANSFORMER</code> Log transform <code>POWER_TRANSFORMER</code> Power transform <code>BOX_COX_TRANSFORMER</code> Box-Cox transform"},{"location":"api/enums/#type-conversion","title":"Type Conversion","text":"Value Description <code>TYPE_SETTER</code> Set column type <code>DATE_PARSER</code> Parse date strings <code>DATE_FORMATTER</code> Format dates <code>BOOLEAN_CONVERTER</code> Convert to boolean <code>NUMBER_TO_STRING</code> Number to string <code>STRING_TO_NUMBER</code> String to number"},{"location":"api/enums/#datetime-operations","title":"Date/Time Operations","text":"Value Description <code>DATE_COMPONENTS_EXTRACTOR</code> Extract year, month, day, etc. <code>DATE_DIFF_CALCULATOR</code> Calculate date differences <code>HOLIDAYS_COMPUTER</code> Compute holiday flags <code>TIMEZONE_CONVERTER</code> Convert timezones <code>DATE_RANGE_CLASSIFIER</code> Classify into date ranges <code>DATETIME_FORMATTER</code> Format datetime values <code>TIMESTAMP_EXTRACTOR</code> Extract timestamps"},{"location":"api/enums/#filtering","title":"Filtering","text":"Value Description <code>FILTER_ON_VALUE</code> Filter by value match <code>FILTER_ON_BAD_TYPE</code> Filter invalid types <code>FILTER_ON_FORMULA</code> Filter by formula <code>FILTER_ON_DATE_RANGE</code> Filter by date range <code>FILTER_ON_NUMERIC_RANGE</code> Filter by numeric range <code>FILTER_ON_MULTIPLE_VALUES</code> Filter by multiple values <code>FILTER_ON_NULL_NUMERIC</code> Filter null numerics <code>FILTER_ON_GEO_ZONE</code> Filter by geographic zone <code>FILTER_ON_CUSTOM_CONDITION</code> Custom filter condition"},{"location":"api/enums/#flagging","title":"Flagging","text":"Value Description <code>FLAG_ON_VALUE</code> Flag by value match <code>FLAG_ON_FORMULA</code> Flag by formula <code>FLAG_ON_BAD_TYPE</code> Flag invalid types <code>FLAG_ON_DATE_RANGE</code> Flag by date range <code>FLAG_ON_NUMERIC_RANGE</code> Flag by numeric range"},{"location":"api/enums/#row-operations","title":"Row Operations","text":"Value Description <code>REMOVE_DUPLICATES</code> Remove duplicate rows <code>SORT_ROWS</code> Sort rows <code>SAMPLE_ROWS</code> Sample rows <code>SHUFFLE_ROWS</code> Shuffle rows"},{"location":"api/enums/#computed-columns","title":"Computed Columns","text":"Value Description <code>CREATE_COLUMN_WITH_GREL</code> Create column with GREL expression <code>FORMULA</code> Formula-based column <code>MULTI_COLUMN_FORMULA</code> Multi-column formula <code>COLUMN_PSEUDO_ANONYMIZER</code> Pseudonymize values <code>HASH_COMPUTER</code> Compute hash <code>UUID_GENERATOR</code> Generate UUIDs"},{"location":"api/enums/#categorical-encoding","title":"Categorical Encoding","text":"Value Description <code>MERGE_LONG_TAIL_VALUES</code> Merge rare categories <code>CATEGORICAL_ENCODER</code> General encoding <code>ONE_HOT_ENCODER</code> One-hot encoding <code>LABEL_ENCODER</code> Label encoding <code>ORDINAL_ENCODER</code> Ordinal encoding <code>TARGET_ENCODER</code> Target encoding <code>LEAVE_ONE_OUT_ENCODER</code> Leave-one-out encoding <code>WOE_ENCODER</code> Weight of evidence encoding <code>FEATURE_HASHER</code> Feature hashing"},{"location":"api/enums/#geographic","title":"Geographic","text":"Value Description <code>GEO_POINT_CREATOR</code> Create geo points <code>GEO_ENCODER</code> Geocode addresses <code>GEO_IP_RESOLVER</code> Resolve IP to location <code>GEO_DISTANCE_CALCULATOR</code> Calculate distances <code>GEO_POLYGON_MATCHER</code> Match points to polygons <code>ADDRESS_PARSER</code> Parse addresses <code>REVERSE_GEOCODER</code> Reverse geocode"},{"location":"api/enums/#conditional-logic","title":"Conditional Logic","text":"Value Description <code>IF_THEN_ELSE</code> Conditional branching <code>SWITCH_CASE</code> Multi-case switch"},{"location":"api/enums/#data-reshaping","title":"Data Reshaping","text":"Value Description <code>FOLD_MULTIPLE_COLUMNS</code> Fold columns to rows <code>TRANSPOSE_ROWS_TO_COLUMNS</code> Transpose rows/columns <code>UNFOLD</code> Unfold column values <code>TRANSLATE_VALUES</code> Translate/map values <code>COALESCE</code> Coalesce multiple columns <code>FILL_COLUMN</code> Fill entire column"},{"location":"api/enums/#arrayjson-operations","title":"Array/JSON Operations","text":"Value Description <code>ARRAY_SPLITTER</code> Split arrays <code>ARRAY_JOINER</code> Join array elements <code>ARRAY_SORTER</code> Sort arrays <code>ARRAY_UNFOLD</code> Unfold array to rows <code>ARRAY_FOLD</code> Fold rows to array <code>ARRAY_ELEMENT_EXTRACTOR</code> Extract array element <code>JSON_FLATTENER</code> Flatten JSON <code>JSON_EXTRACTOR</code> Extract from JSON <code>XML_EXTRACTOR</code> Extract from XML <code>EXTRACT_WITH_JSONPATH</code> JSONPath extraction <code>SPLIT_URL</code> Split URL components"},{"location":"api/enums/#other","title":"Other","text":"Value Description <code>NESTED_PROCESSOR</code> Nested processor <code>PROCESSOR_GROUP</code> Group of processors <code>PYTHON_UDF</code> Python user-defined function"},{"location":"api/enums/#datasettype","title":"DatasetType","text":"<pre><code>from py2dataiku import DatasetType\n</code></pre> Value Description <code>INPUT</code> Source dataset <code>INTERMEDIATE</code> Intermediate dataset <code>OUTPUT</code> Final output dataset"},{"location":"api/enums/#datasetconnectiontype","title":"DatasetConnectionType","text":"<pre><code>from py2dataiku import DatasetConnectionType\n</code></pre> Value Description <code>FILESYSTEM</code> Local/network filesystem <code>SQL_POSTGRESQL</code> PostgreSQL <code>SQL_MYSQL</code> MySQL <code>SQL_BIGQUERY</code> Google BigQuery <code>SQL_SNOWFLAKE</code> Snowflake <code>SQL_REDSHIFT</code> Amazon Redshift <code>S3</code> Amazon S3 <code>GCS</code> Google Cloud Storage <code>AZURE_BLOB</code> Azure Blob Storage <code>HDFS</code> Hadoop HDFS <code>MANAGED_FOLDER</code> DSS managed folder <code>MONGODB</code> MongoDB <code>ELASTICSEARCH</code> Elasticsearch"},{"location":"api/enums/#jointype","title":"JoinType","text":"<pre><code>from py2dataiku import JoinType\n</code></pre> Value Description <code>INNER</code> Inner join <code>LEFT</code> Left outer join <code>RIGHT</code> Right outer join <code>OUTER</code> Full outer join <code>CROSS</code> Cross join <code>LEFT_ANTI</code> Left anti join <code>RIGHT_ANTI</code> Right anti join <code>ADVANCED</code> Advanced join conditions"},{"location":"api/enums/#aggregationfunction","title":"AggregationFunction","text":"<pre><code>from py2dataiku import AggregationFunction\n</code></pre> <p>Basic: <code>SUM</code>, <code>AVG</code>, <code>MEAN</code>, <code>COUNT</code>, <code>COUNTD</code>, <code>MIN</code>, <code>MAX</code>, <code>FIRST</code>, <code>LAST</code></p> <p>Statistical: <code>STD</code>, <code>STDDEV</code>, <code>VAR</code>, <code>VARIANCE</code>, <code>MEDIAN</code>, <code>MODE</code>, <code>NUNIQUE</code></p> <p>Percentiles: <code>PERCENTILE_25</code>, <code>PERCENTILE_50</code>, <code>PERCENTILE_75</code>, <code>PERCENTILE_90</code>, <code>PERCENTILE_95</code>, <code>PERCENTILE_99</code></p> <p>Collections: <code>CONCAT</code>, <code>COLLECT_LIST</code>, <code>COLLECT_SET</code></p>"},{"location":"api/enums/#windowfunctiontype","title":"WindowFunctionType","text":"<pre><code>from py2dataiku import WindowFunctionType\n</code></pre> <p>Ranking: <code>ROW_NUMBER</code>, <code>RANK</code>, <code>DENSE_RANK</code>, <code>NTILE</code>, <code>PERCENT_RANK</code>, <code>CUME_DIST</code></p> <p>Offset: <code>LAG</code>, <code>LEAD</code>, <code>LAG_DIFF</code>, <code>LEAD_DIFF</code>, <code>FIRST_VALUE</code>, <code>LAST_VALUE</code>, <code>NTH_VALUE</code></p> <p>Running: <code>RUNNING_SUM</code>, <code>RUNNING_AVG</code>, <code>RUNNING_MIN</code>, <code>RUNNING_MAX</code>, <code>RUNNING_COUNT</code></p> <p>Moving: <code>MOVING_AVG</code>, <code>MOVING_SUM</code>, <code>MOVING_MIN</code>, <code>MOVING_MAX</code>, <code>MOVING_STDDEV</code></p>"},{"location":"api/enums/#splitmode","title":"SplitMode","text":"<pre><code>from py2dataiku import SplitMode\n</code></pre> Value Description <code>FILTER</code> Filter-based split <code>RANDOM</code> Random split <code>COLUMN_VALUE</code> Split by column value <code>PERCENTILE</code> Percentile-based split"},{"location":"api/enums/#samplingmethod","title":"SamplingMethod","text":"<pre><code>from py2dataiku import SamplingMethod\n</code></pre> Value Description <code>RANDOM</code> Random sampling <code>RANDOM_FIXED</code> Random with fixed seed <code>FIRST_ROWS</code> First N rows <code>LAST_ROWS</code> Last N rows <code>STRATIFIED</code> Stratified sampling <code>CLASS_REBALANCE</code> Class rebalancing <code>RESERVOIR</code> Reservoir sampling"},{"location":"api/enums/#stringtransformermode","title":"StringTransformerMode","text":"<pre><code>from py2dataiku import StringTransformerMode\n</code></pre> <p>Case: <code>UPPERCASE</code>, <code>LOWERCASE</code>, <code>TITLECASE</code>, <code>CAPITALIZE</code>, <code>SWAPCASE</code></p> <p>Whitespace: <code>TRIM</code>, <code>TRIM_LEFT</code>, <code>TRIM_RIGHT</code>, <code>NORMALIZE_WHITESPACE</code>, <code>REMOVE_WHITESPACE</code>, <code>COLLAPSE_WHITESPACE</code></p> <p>Character removal: <code>REMOVE_ACCENTS</code>, <code>ASCII_TRANSLITERATE</code>, <code>REMOVE_NON_ALPHANUMERIC</code>, <code>REMOVE_NON_PRINTABLE</code>, <code>REMOVE_PUNCTUATION</code>, <code>REMOVE_DIGITS</code>, <code>KEEP_ONLY_DIGITS</code>, <code>KEEP_ONLY_ALPHA</code></p> <p>Padding: <code>PAD_LEFT</code>, <code>PAD_RIGHT</code>, <code>PAD_CENTER</code></p> <p>Other: <code>REVERSE</code>, <code>QUOTE</code>, <code>UNQUOTE</code></p>"},{"location":"api/enums/#numericaltransformermode","title":"NumericalTransformerMode","text":"<pre><code>from py2dataiku import NumericalTransformerMode\n</code></pre> <p>Arithmetic: <code>MULTIPLY</code>, <code>DIVIDE</code>, <code>ADD</code>, <code>SUBTRACT</code>, <code>POWER</code>, <code>SQRT</code>, <code>LOG</code>, <code>LOG10</code>, <code>LOG2</code>, <code>EXP</code>, <code>ABS</code>, <code>NEGATE</code>, <code>INVERSE</code>, <code>MODULO</code></p> <p>Rounding: <code>ROUND</code>, <code>FLOOR</code>, <code>CEIL</code>, <code>TRUNCATE</code>, <code>ROUND_TO_SIGNIFICANT</code></p> <p>Trigonometric: <code>SIN</code>, <code>COS</code>, <code>TAN</code>, <code>ASIN</code>, <code>ACOS</code>, <code>ATAN</code></p> <p>Conversion: <code>DEGREES_TO_RADIANS</code>, <code>RADIANS_TO_DEGREES</code></p>"},{"location":"api/enums/#filtermatchmode","title":"FilterMatchMode","text":"<pre><code>from py2dataiku import FilterMatchMode\n</code></pre> <p><code>EQUALS</code>, <code>NOT_EQUALS</code>, <code>CONTAINS</code>, <code>NOT_CONTAINS</code>, <code>STARTS_WITH</code>, <code>ENDS_WITH</code>, <code>REGEX</code>, <code>NOT_REGEX</code>, <code>IS_EMPTY</code>, <code>IS_NOT_EMPTY</code>, <code>IS_NULL</code>, <code>IS_NOT_NULL</code>, <code>IN_LIST</code>, <code>NOT_IN_LIST</code></p>"},{"location":"api/exceptions/","title":"Exceptions","text":"<p>Custom exception hierarchy for structured error handling.</p> <pre><code>from py2dataiku import (\n    Py2DataikuError,\n    ConversionError,\n    ProviderError,\n    LLMResponseParseError,\n    InvalidPythonCodeError,\n    ValidationError,\n    ExportError,\n    ConfigurationError,\n)\n</code></pre>"},{"location":"api/exceptions/#hierarchy","title":"Hierarchy","text":"<pre><code>Exception\n\u2514\u2500\u2500 Py2DataikuError              # Base exception for all py2dataiku errors\n    \u251c\u2500\u2500 ConversionError          # Error during code-to-flow conversion\n    \u2502   \u2514\u2500\u2500 InvalidPythonCodeError  # Code could not be parsed or analyzed\n    \u251c\u2500\u2500 ProviderError            # Error communicating with LLM provider\n    \u2502   \u2514\u2500\u2500 LLMResponseParseError   # LLM response could not be parsed as JSON\n    \u251c\u2500\u2500 ValidationError          # Error during flow or recipe validation\n    \u251c\u2500\u2500 ExportError              # Error during DSS project export\n    \u2514\u2500\u2500 ConfigurationError       # Error in py2dataiku configuration\n</code></pre>"},{"location":"api/exceptions/#py2dataikuerror","title":"Py2DataikuError","text":"<p>Base exception for all py2dataiku errors. Catch this to handle any library error.</p> <pre><code>try:\n    flow = convert(code)\nexcept Py2DataikuError as e:\n    print(f\"py2dataiku error: {e}\")\n</code></pre>"},{"location":"api/exceptions/#conversionerror","title":"ConversionError","text":"<p>Raised when code-to-flow conversion fails.</p> <pre><code>try:\n    flow = convert(invalid_code)\nexcept ConversionError as e:\n    print(f\"Conversion failed: {e}\")\n</code></pre>"},{"location":"api/exceptions/#invalidpythoncodeerror","title":"InvalidPythonCodeError","text":"<p>Subclass of <code>ConversionError</code>. Raised when the provided Python code cannot be parsed or analyzed (e.g., syntax errors).</p> <pre><code>try:\n    flow = convert(\"this is not python code !!!\")\nexcept InvalidPythonCodeError as e:\n    print(f\"Invalid Python: {e}\")\n</code></pre>"},{"location":"api/exceptions/#providererror","title":"ProviderError","text":"<p>Raised when communication with an LLM provider fails (network errors, authentication failures, rate limits).</p> <pre><code>try:\n    flow = convert_with_llm(code, provider=\"anthropic\")\nexcept ProviderError as e:\n    print(f\"LLM provider error: {e}\")\n</code></pre>"},{"location":"api/exceptions/#llmresponseparseerror","title":"LLMResponseParseError","text":"<p>Subclass of <code>ProviderError</code>. Raised when the LLM returns a response that cannot be parsed as valid JSON.</p> <pre><code>try:\n    flow = convert_with_llm(code)\nexcept LLMResponseParseError as e:\n    print(f\"Could not parse LLM response: {e}\")\n</code></pre>"},{"location":"api/exceptions/#validationerror","title":"ValidationError","text":"<p>Raised when flow or recipe validation fails (e.g., cycles in DAG, missing datasets).</p> <pre><code>try:\n    result = flow.validate()\nexcept ValidationError as e:\n    print(f\"Validation failed: {e}\")\n</code></pre>"},{"location":"api/exceptions/#exporterror","title":"ExportError","text":"<p>Raised when DSS project export fails (e.g., I/O errors, invalid configuration).</p> <pre><code>try:\n    export_to_dss(flow, \"output/\")\nexcept ExportError as e:\n    print(f\"Export failed: {e}\")\n</code></pre>"},{"location":"api/exceptions/#configurationerror","title":"ConfigurationError","text":"<p>Raised when configuration is invalid (e.g., malformed config file, missing required settings).</p> <pre><code>try:\n    config = load_config(\"bad_config.toml\")\nexcept ConfigurationError as e:\n    print(f\"Config error: {e}\")\n</code></pre>"},{"location":"api/exceptions/#error-handling-patterns","title":"Error Handling Patterns","text":""},{"location":"api/exceptions/#catch-all-library-errors","title":"Catch all library errors","text":"<pre><code>from py2dataiku import Py2DataikuError\n\ntry:\n    flow = convert_with_llm(code)\n    export_to_dss(flow, \"output/\")\nexcept Py2DataikuError as e:\n    logging.error(f\"py2dataiku: {e}\")\n</code></pre>"},{"location":"api/exceptions/#granular-error-handling","title":"Granular error handling","text":"<pre><code>from py2dataiku import (\n    InvalidPythonCodeError,\n    LLMResponseParseError,\n    ProviderError,\n    ConversionError,\n)\n\ntry:\n    flow = convert_with_llm(code)\nexcept InvalidPythonCodeError:\n    print(\"The code has syntax errors\")\nexcept LLMResponseParseError:\n    print(\"LLM returned invalid response, try again\")\nexcept ProviderError:\n    print(\"LLM provider unavailable, falling back to rule-based\")\n    flow = convert(code)\nexcept ConversionError:\n    print(\"Conversion failed for another reason\")\n</code></pre>"},{"location":"api/exceptions/#llm-fallback-pattern","title":"LLM fallback pattern","text":"<pre><code>from py2dataiku import convert, convert_with_llm, ProviderError\n\ntry:\n    flow = convert_with_llm(code)\nexcept ProviderError:\n    flow = convert(code)  # Fall back to rule-based\n</code></pre>"},{"location":"api/exporters/","title":"Exporters","text":"<p>Export Dataiku flows as DSS project bundles.</p>"},{"location":"api/exporters/#export_to_dss","title":"export_to_dss()","text":"<p>Convenience function to export a flow to DSS project format.</p> <pre><code>from py2dataiku import export_to_dss\n\npath = export_to_dss(flow, \"output/my_project\")\npath = export_to_dss(flow, \"output/my_project\", create_zip=True)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>flow</code> <code>DataikuFlow</code> required Flow to export <code>output_dir</code> <code>str</code> required Output directory path <code>project_key</code> <code>Optional[str]</code> <code>None</code> DSS project key <code>config</code> <code>Optional[DSSProjectConfig]</code> <code>None</code> Project configuration <code>create_zip</code> <code>bool</code> <code>False</code> Whether to create a ZIP archive <p>Returns: <code>str</code> - Path to exported project (directory or zip file)</p>"},{"location":"api/exporters/#dssexporter","title":"DSSExporter","text":"<p>Exports a DataikuFlow as a Dataiku DSS project bundle.</p> <pre><code>from py2dataiku import DSSExporter, DSSProjectConfig\n</code></pre>"},{"location":"api/exporters/#constructor","title":"Constructor","text":"<pre><code>DSSExporter(\n    flow: DataikuFlow,\n    project_key: str = None,\n    config: DSSProjectConfig = None,\n)\n</code></pre>"},{"location":"api/exporters/#methods","title":"Methods","text":""},{"location":"api/exporters/#export","title":"<code>export()</code>","text":"<pre><code>def export(self, output_dir: str, create_zip: bool = False) -&gt; str\n</code></pre> <p>Exports the flow to DSS project format on disk.</p> <p>Parameters:</p> Parameter Type Default Description <code>output_dir</code> <code>str</code> required Output directory <code>create_zip</code> <code>bool</code> <code>False</code> Create ZIP archive <p>Returns: <code>str</code> - Path to exported project</p> <p>Raises: <code>ExportError</code> - If export fails</p>"},{"location":"api/exporters/#dssprojectconfig","title":"DSSProjectConfig","text":"<p>Configuration for DSS project export.</p> <pre><code>from py2dataiku import DSSProjectConfig\n</code></pre>"},{"location":"api/exporters/#fields","title":"Fields","text":"Field Type Default Description <code>project_key</code> <code>str</code> <code>\"CONVERTED_PROJECT\"</code> DSS project key <code>project_name</code> <code>str</code> <code>\"Converted Python Pipeline\"</code> Display name <code>owner</code> <code>str</code> <code>\"py2dataiku\"</code> Project owner <code>description</code> <code>str</code> <code>\"Auto-generated...\"</code> Project description <code>tags</code> <code>List[str]</code> <code>[\"py2dataiku\", \"auto-generated\"]</code> Project tags <code>default_connection</code> <code>str</code> <code>\"filesystem_managed\"</code> Default data connection <code>default_format</code> <code>str</code> <code>\"csv\"</code> Default file format <code>include_code_comments</code> <code>bool</code> <code>True</code> Include source code comments"},{"location":"api/exporters/#methods_1","title":"Methods","text":"<pre><code>config.to_dict()                    # -&gt; Dict[str, Any]\n</code></pre>"},{"location":"api/exporters/#example","title":"Example","text":"<pre><code>from py2dataiku import DSSExporter, DSSProjectConfig, convert\n\nflow = convert(code)\n\nconfig = DSSProjectConfig(\n    project_key=\"SALES_PIPELINE\",\n    project_name=\"Sales Data Pipeline\",\n    owner=\"data-team\",\n    default_connection=\"postgresql_prod\",\n)\n\nexporter = DSSExporter(flow, config=config)\nexporter.export(\"output/sales_pipeline\", create_zip=True)\n</code></pre>"},{"location":"api/graph/","title":"Graph","text":"<p>DAG (Directed Acyclic Graph) representation for flow analysis and validation.</p>"},{"location":"api/graph/#flowgraph","title":"FlowGraph","text":"<p>Graph data structure representing the flow as a DAG with datasets and recipes as nodes.</p> <pre><code># Access via DataikuFlow\nflow = convert(code)\ngraph = flow.graph  # FlowGraph instance\n</code></pre>"},{"location":"api/graph/#building-the-graph","title":"Building the Graph","text":"<p>The graph is automatically built from <code>DataikuFlow</code> when you access <code>flow.graph</code>. It creates nodes for all datasets and recipes, and edges based on recipe inputs/outputs.</p>"},{"location":"api/graph/#properties","title":"Properties","text":"Property Type Description <code>nodes</code> <code>List[FlowNode]</code> All nodes <code>dataset_nodes</code> <code>List[FlowNode]</code> Only dataset nodes <code>recipe_nodes</code> <code>List[FlowNode]</code> Only recipe nodes <code>edges</code> <code>List[Tuple[str, str]]</code> All edges as (source, target) pairs"},{"location":"api/graph/#node-operations","title":"Node Operations","text":"<pre><code># Add nodes\nnode = graph.add_node(\"my_dataset\", NodeType.DATASET, metadata={\"type\": \"INPUT\"})\n\n# Add edges\ngraph.add_edge(\"input_data\", \"prepare_1\")\ngraph.add_edge(\"prepare_1\", \"cleaned_data\")\n\n# Get node\nnode = graph.get_node(\"my_dataset\")  # -&gt; Optional[FlowNode]\n\n# Get neighbors\ngraph.get_successors(\"my_dataset\")    # -&gt; List[str]\ngraph.get_predecessors(\"cleaned_data\") # -&gt; List[str]\n</code></pre>"},{"location":"api/graph/#graph-algorithms","title":"Graph Algorithms","text":""},{"location":"api/graph/#topological-sort","title":"Topological Sort","text":"<p>Returns nodes in dependency order (Kahn's algorithm).</p> <pre><code>order = graph.topological_sort()\n# ['input_data', 'prepare_1', 'cleaned_data', 'grouping_1', 'summary']\n</code></pre> <p>Raises: <code>ValueError</code> if the graph contains cycles.</p>"},{"location":"api/graph/#cycle-detection","title":"Cycle Detection","text":"<p>Finds all cycles in the graph using DFS.</p> <pre><code>cycles = graph.detect_cycles()\n# [] if no cycles\n# [['A', 'B', 'C', 'A']] if cycle exists\n</code></pre>"},{"location":"api/graph/#disconnected-subgraphs","title":"Disconnected Subgraphs","text":"<p>Finds groups of nodes that are not connected to each other.</p> <pre><code>subgraphs = graph.find_disconnected_subgraphs()\n# [['input1', 'prepare_1', 'output1'], ['input2', 'join_1', 'output2']]\n</code></pre>"},{"location":"api/graph/#path-finding","title":"Path Finding","text":"<p>Find a path between two nodes.</p> <pre><code>path = graph.find_path(\"input_data\", \"summary\")\n# ['input_data', 'prepare_1', 'cleaned_data', 'grouping_1', 'summary']\n# None if no path exists\n</code></pre>"},{"location":"api/graph/#transitive-closure","title":"Transitive Closure","text":"<p>Get all nodes reachable from a given node.</p> <pre><code>reachable = graph.get_transitive_closure(\"input_data\")\n# {'prepare_1', 'cleaned_data', 'grouping_1', 'summary'}\n</code></pre>"},{"location":"api/graph/#flownode","title":"FlowNode","text":"<p>A node in the flow graph.</p>"},{"location":"api/graph/#fields","title":"Fields","text":"Field Type Default Description <code>name</code> <code>str</code> required Node name <code>node_type</code> <code>NodeType</code> required DATASET or RECIPE <code>metadata</code> <code>Dict[str, Any]</code> <code>{}</code> Arbitrary metadata"},{"location":"api/graph/#special-methods","title":"Special Methods","text":"<pre><code>hash(node)         # Hashable by (name, node_type)\nnode1 == node2     # Equality by name and node_type\n</code></pre>"},{"location":"api/graph/#nodetype","title":"NodeType","text":"<pre><code>from py2dataiku.models.flow_graph import NodeType\n</code></pre> Value Description <code>DATASET</code> Dataset node <code>RECIPE</code> Recipe node"},{"location":"api/graph/#usage-with-dataikuflow","title":"Usage with DataikuFlow","text":"<p>The graph is most commonly accessed through <code>DataikuFlow</code>:</p> <pre><code>flow = convert(code)\n\n# Validate flow structure\nresult = flow.validate()\n# Uses graph internally:\n#   - Checks for cycles\n#   - Checks for disconnected components\n#   - Validates dataset references\n\n# Get execution order\norder = flow.graph.topological_sort()\n\n# Check for cycles\ncycles = flow.graph.detect_cycles()\nif cycles:\n    print(f\"Warning: flow has cycles: {cycles}\")\n\n# Find independent subflows\nsubflows = flow.graph.find_disconnected_subgraphs()\nprint(f\"Flow has {len(subflows)} independent pipeline(s)\")\n</code></pre>"},{"location":"api/llm-providers/","title":"LLM Providers","text":"<p>LLM integration for semantic code analysis. Supports Anthropic (Claude) and OpenAI (GPT).</p>"},{"location":"api/llm-providers/#get_provider","title":"get_provider()","text":"<p>Factory function to create an LLM provider.</p> <pre><code>from py2dataiku import get_provider\n\nprovider = get_provider(\"anthropic\")\nprovider = get_provider(\"openai\", api_key=\"sk-...\")\nprovider = get_provider(\"mock\")  # For testing\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>provider</code> <code>str</code> required <code>\"anthropic\"</code>, <code>\"openai\"</code>, or <code>\"mock\"</code> <code>api_key</code> <code>Optional[str]</code> <code>None</code> API key (env var fallback) <code>model</code> <code>Optional[str]</code> <code>None</code> Model name override <p>Returns: <code>LLMProvider</code> instance</p>"},{"location":"api/llm-providers/#llmprovider-abc","title":"LLMProvider (ABC)","text":"<p>Abstract base class for LLM providers.</p> <pre><code>from py2dataiku import LLMProvider\n</code></pre>"},{"location":"api/llm-providers/#abstract-methods","title":"Abstract Methods","text":"<pre><code>def complete(self, prompt: str, system_prompt: Optional[str] = None) -&gt; LLMResponse\ndef complete_json(self, prompt: str, system_prompt: Optional[str] = None) -&gt; Dict[str, Any]\n</code></pre>"},{"location":"api/llm-providers/#abstract-properties","title":"Abstract Properties","text":"<pre><code>@property\ndef model_name(self) -&gt; str\n</code></pre>"},{"location":"api/llm-providers/#anthropicprovider","title":"AnthropicProvider","text":"<pre><code>from py2dataiku import AnthropicProvider\n</code></pre>"},{"location":"api/llm-providers/#constructor","title":"Constructor","text":"<pre><code>AnthropicProvider(\n    api_key: Optional[str] = None,       # Falls back to ANTHROPIC_API_KEY env var\n    model: str = \"claude-sonnet-4-20250514\",\n    max_tokens: int = 4096,\n    timeout: Optional[float] = None,\n    max_retries: int = 2,\n)\n</code></pre> <p>Requires: <code>pip install anthropic</code> (or <code>pip install py-iku[llm]</code>)</p>"},{"location":"api/llm-providers/#openaiprovider","title":"OpenAIProvider","text":"<pre><code>from py2dataiku import OpenAIProvider\n</code></pre>"},{"location":"api/llm-providers/#constructor_1","title":"Constructor","text":"<pre><code>OpenAIProvider(\n    api_key: Optional[str] = None,       # Falls back to OPENAI_API_KEY env var\n    model: str = \"gpt-4o\",\n    max_tokens: int = 4096,\n    timeout: Optional[float] = None,\n    max_retries: int = 2,\n)\n</code></pre> <p>Requires: <code>pip install openai</code> (or <code>pip install py-iku[llm]</code>)</p>"},{"location":"api/llm-providers/#mockprovider","title":"MockProvider","text":"<p>For testing without API calls.</p> <pre><code>from py2dataiku import MockProvider\n\nprovider = MockProvider()\n</code></pre>"},{"location":"api/llm-providers/#llmcodeanalyzer","title":"LLMCodeAnalyzer","text":"<p>Analyzes Python code using LLM to extract data manipulation steps.</p> <pre><code>from py2dataiku import LLMCodeAnalyzer\n</code></pre>"},{"location":"api/llm-providers/#constructor_2","title":"Constructor","text":"<pre><code>LLMCodeAnalyzer(\n    provider: Optional[LLMProvider] = None,\n    provider_name: str = \"anthropic\",\n    api_key: Optional[str] = None,\n    model: Optional[str] = None,\n)\n</code></pre> <p>Can either pass a pre-created <code>LLMProvider</code> instance or specify <code>provider_name</code>/<code>api_key</code>/<code>model</code> to create one.</p>"},{"location":"api/llm-providers/#methods","title":"Methods","text":""},{"location":"api/llm-providers/#analyze","title":"<code>analyze()</code>","text":"<pre><code>def analyze(self, code: str) -&gt; AnalysisResult\n</code></pre> <p>Analyzes Python code and extracts data manipulation steps.</p> <p>Parameters:</p> Parameter Type Description <code>code</code> <code>str</code> Python source code to analyze <p>Returns: <code>AnalysisResult</code></p> <p>Raises: - <code>LLMResponseParseError</code> - If LLM response cannot be parsed as JSON - <code>ProviderError</code> - If communication with LLM provider fails</p>"},{"location":"api/llm-providers/#analysisresult","title":"AnalysisResult","text":"<p>Result of LLM code analysis.</p>"},{"location":"api/llm-providers/#fields","title":"Fields","text":"Field Type Description <code>code_summary</code> <code>str</code> High-level description of what the code does <code>total_operations</code> <code>int</code> Number of data operations detected <code>complexity_score</code> <code>int</code> Code complexity score (1-10) <code>datasets</code> <code>List[Dict[str, Any]]</code> Detected datasets <code>steps</code> <code>List[DataStep]</code> Ordered list of data manipulation steps <code>recommendations</code> <code>List[str]</code> Conversion recommendations <code>warnings</code> <code>List[str]</code> Conversion warnings"},{"location":"api/llm-providers/#datastep","title":"DataStep","text":"<p>A single data manipulation step extracted from code.</p>"},{"location":"api/llm-providers/#fields_1","title":"Fields","text":"Field Type Description <code>step_number</code> <code>int</code> Sequential step number <code>operation</code> <code>OperationType</code> Type of operation <code>description</code> <code>str</code> Human-readable description <code>input_datasets</code> <code>List[str]</code> Input dataset names <code>output_dataset</code> <code>Optional[str]</code> Output dataset name <code>columns</code> <code>List[str]</code> Affected columns <code>filter_conditions</code> <code>List[FilterCondition]</code> Filter conditions <code>aggregations</code> <code>List[Aggregation]</code> Aggregation configs <code>group_by_columns</code> <code>List[str]</code> Group-by columns <code>join_conditions</code> <code>List[JoinCondition]</code> Join conditions <code>join_type</code> <code>Optional[str]</code> Join type <code>column_transforms</code> <code>List[ColumnTransform]</code> Column transforms <code>rename_mapping</code> <code>Dict[str, str]</code> Column renames <code>sort_columns</code> <code>List[Dict[str, str]]</code> Sort specifications <code>fill_value</code> <code>Optional[Any]</code> Fill value for missing <code>source_lines</code> <code>List[int]</code> Source code line numbers <code>source_code</code> <code>Optional[str]</code> Source code snippet <code>suggested_recipe</code> <code>Optional[str]</code> Suggested Dataiku recipe type <code>suggested_processors</code> <code>List[str]</code> Suggested processors <code>requires_python_recipe</code> <code>bool</code> Whether a Python recipe is needed <code>reasoning</code> <code>Optional[str]</code> LLM reasoning for mapping"},{"location":"api/llm-providers/#operationtype","title":"OperationType","text":"<p>Types of data operations the LLM can detect.</p> <pre><code>from py2dataiku import OperationType\n</code></pre> <p>Data I/O: <code>READ_DATA</code>, <code>WRITE_DATA</code></p> <p>Column operations: <code>FILTER</code>, <code>SELECT_COLUMNS</code>, <code>DROP_COLUMNS</code>, <code>RENAME_COLUMNS</code>, <code>ADD_COLUMN</code>, <code>TRANSFORM_COLUMN</code></p> <p>Missing data: <code>FILL_MISSING</code>, <code>DROP_MISSING</code></p> <p>Deduplication: <code>DROP_DUPLICATES</code></p> <p>Aggregation: <code>GROUP_AGGREGATE</code>, <code>WINDOW_FUNCTION</code></p> <p>Combining: <code>JOIN</code>, <code>UNION</code></p> <p>Reshaping: <code>PIVOT</code>, <code>UNPIVOT</code></p> <p>Ordering: <code>SORT</code>, <code>TOP_N</code>, <code>SAMPLE</code></p> <p>Type conversion: <code>CAST_TYPE</code>, <code>PARSE_DATE</code></p> <p>Transformations: <code>SPLIT_COLUMN</code>, <code>ENCODE_CATEGORICAL</code>, <code>NORMALIZE_SCALE</code></p> <p>Geographic: <code>GEO_OPERATION</code></p> <p>Other: <code>CUSTOM_FUNCTION</code>, <code>UNKNOWN</code></p>"},{"location":"api/mlops/","title":"MLOps","text":"<p>Machine learning operations models for API endpoints, model versioning, and drift detection.</p>"},{"location":"api/mlops/#apiendpoint","title":"APIEndpoint","text":"<p>Model serving endpoint configuration.</p> <pre><code>from py2dataiku import APIEndpoint, EndpointType\n</code></pre>"},{"location":"api/mlops/#fields","title":"Fields","text":"Field Type Default Description <code>name</code> <code>str</code> required Endpoint name <code>model_name</code> <code>str</code> required Model to serve <code>endpoint_type</code> <code>EndpointType</code> <code>EndpointType.REST</code> Endpoint type <code>url_path</code> <code>Optional[str]</code> <code>None</code> URL path <code>input_schema</code> <code>Dict[str, str]</code> <code>{}</code> Input field types <code>output_schema</code> <code>Dict[str, str]</code> <code>{}</code> Output field types <code>auth_required</code> <code>bool</code> <code>True</code> Require authentication <code>rate_limit</code> <code>Optional[int]</code> <code>None</code> Requests per minute"},{"location":"api/mlops/#endpointtype","title":"EndpointType","text":"Value Description <code>REST</code> REST API endpoint <code>BATCH</code> Batch prediction endpoint"},{"location":"api/mlops/#example","title":"Example","text":"<pre><code>endpoint = APIEndpoint(\n    name=\"predict_churn\",\n    model_name=\"churn_model_v2\",\n    endpoint_type=EndpointType.REST,\n    url_path=\"/api/v1/predict/churn\",\n    input_schema={\n        \"customer_id\": \"string\",\n        \"tenure_months\": \"int\",\n        \"monthly_charges\": \"float\",\n    },\n    output_schema={\n        \"prediction\": \"string\",\n        \"probability\": \"float\",\n    },\n    auth_required=True,\n    rate_limit=100,\n)\n</code></pre>"},{"location":"api/mlops/#modelversion","title":"ModelVersion","text":"<p>Model version metadata and tracking.</p> <pre><code>from py2dataiku import ModelVersion, ModelFramework\n</code></pre>"},{"location":"api/mlops/#fields_1","title":"Fields","text":"Field Type Default Description <code>version_id</code> <code>str</code> required Version identifier <code>model_name</code> <code>str</code> required Model name <code>framework</code> <code>ModelFramework</code> <code>ModelFramework.SCIKIT_LEARN</code> ML framework <code>algorithm</code> <code>Optional[str]</code> <code>None</code> Algorithm name <code>metrics</code> <code>Dict[str, float]</code> <code>{}</code> Performance metrics <code>features</code> <code>List[str]</code> <code>[]</code> Feature names <code>target</code> <code>Optional[str]</code> <code>None</code> Target variable <code>active</code> <code>bool</code> <code>False</code> Whether this version is active <code>tags</code> <code>List[str]</code> <code>[]</code> Tags"},{"location":"api/mlops/#modelframework","title":"ModelFramework","text":"Value Description <code>SCIKIT_LEARN</code> scikit-learn <code>XGBOOST</code> XGBoost <code>LIGHTGBM</code> LightGBM <code>TENSORFLOW</code> TensorFlow <code>PYTORCH</code> PyTorch <code>CUSTOM</code> Custom framework"},{"location":"api/mlops/#example_1","title":"Example","text":"<pre><code>version = ModelVersion(\n    version_id=\"v2.1.0\",\n    model_name=\"churn_model\",\n    framework=ModelFramework.XGBOOST,\n    algorithm=\"XGBClassifier\",\n    metrics={\"accuracy\": 0.92, \"f1\": 0.88, \"auc\": 0.95},\n    features=[\"tenure\", \"monthly_charges\", \"contract_type\"],\n    target=\"churn\",\n    active=True,\n    tags=[\"production\", \"validated\"],\n)\n</code></pre>"},{"location":"api/mlops/#driftconfig","title":"DriftConfig","text":"<p>Model drift detection configuration.</p> <pre><code>from py2dataiku import DriftConfig, DriftMetricType\n</code></pre>"},{"location":"api/mlops/#fields_2","title":"Fields","text":"Field Type Default Description <code>enabled</code> <code>bool</code> <code>True</code> Enable drift detection <code>metric</code> <code>DriftMetricType</code> <code>DriftMetricType.PSI</code> Drift metric <code>threshold</code> <code>float</code> <code>0.2</code> Alert threshold <code>columns</code> <code>List[str]</code> <code>[]</code> Columns to monitor <code>check_frequency</code> <code>str</code> <code>\"daily\"</code> Check frequency"},{"location":"api/mlops/#driftmetrictype","title":"DriftMetricType","text":"Value Description <code>PSI</code> Population Stability Index <code>KS</code> Kolmogorov-Smirnov test <code>CHI_SQUARED</code> Chi-squared test <code>WASSERSTEIN</code> Wasserstein distance"},{"location":"api/mlops/#example_2","title":"Example","text":"<pre><code>drift = DriftConfig(\n    enabled=True,\n    metric=DriftMetricType.PSI,\n    threshold=0.15,\n    columns=[\"tenure\", \"monthly_charges\", \"contract_type\"],\n    check_frequency=\"daily\",\n)\n</code></pre>"},{"location":"api/models/","title":"Core Models","text":"<p>Data models representing Dataiku DSS flows, recipes, datasets, and processor steps.</p>"},{"location":"api/models/#dataikuflow","title":"DataikuFlow","text":"<p>The main output class representing a complete Dataiku pipeline.</p> <pre><code>from py2dataiku import DataikuFlow\n</code></pre>"},{"location":"api/models/#fields","title":"Fields","text":"Field Type Default Description <code>name</code> <code>str</code> <code>\"converted_flow\"</code> Flow name <code>source_file</code> <code>Optional[str]</code> <code>None</code> Source Python file path <code>generation_timestamp</code> <code>Optional[str]</code> <code>None</code> When the flow was generated <code>datasets</code> <code>List[DataikuDataset]</code> <code>[]</code> All datasets in the flow <code>recipes</code> <code>List[DataikuRecipe]</code> <code>[]</code> All recipes in the flow <code>zones</code> <code>List[FlowZone]</code> <code>[]</code> Logical grouping zones <code>recommendations</code> <code>List[FlowRecommendation]</code> <code>[]</code> Optimization recommendations <code>optimization_notes</code> <code>List[str]</code> <code>[]</code> Notes from optimizer <code>warnings</code> <code>List[str]</code> <code>[]</code> Conversion warnings"},{"location":"api/models/#properties","title":"Properties","text":"Property Type Description <code>graph</code> <code>FlowGraph</code> DAG representation of the flow <code>input_datasets</code> <code>List[DataikuDataset]</code> Datasets with type INPUT <code>output_datasets</code> <code>List[DataikuDataset]</code> Datasets with type OUTPUT <code>intermediate_datasets</code> <code>List[DataikuDataset]</code> Datasets with type INTERMEDIATE"},{"location":"api/models/#dataset-operations","title":"Dataset Operations","text":"<pre><code>flow.add_dataset(dataset)          # Add a dataset\nflow.get_dataset(\"name\")           # Get dataset by name -&gt; Optional[DataikuDataset]\n</code></pre>"},{"location":"api/models/#recipe-operations","title":"Recipe Operations","text":"<pre><code>flow.add_recipe(recipe)                    # Add a recipe\nflow.get_recipe(\"name\")                    # Get recipe by name -&gt; Optional[DataikuRecipe]\nflow.get_recipes_by_type(RecipeType.JOIN)   # Get all JOIN recipes -&gt; List[DataikuRecipe]\n</code></pre>"},{"location":"api/models/#analysis","title":"Analysis","text":"<pre><code>flow.validate()                    # Validate structure -&gt; Dict[str, Any]\nflow.get_summary()                 # Text summary -&gt; str\nflow.get_column_lineage(\"col\")     # Trace column lineage -&gt; ColumnLineage\nflow.get_recommendations()         # Get recommendations -&gt; List[FlowRecommendation]\n</code></pre>"},{"location":"api/models/#visualization","title":"Visualization","text":"<pre><code>flow.visualize(format=\"svg\")       # SVG (pixel-accurate Dataiku styling)\nflow.visualize(format=\"html\")      # Interactive canvas\nflow.visualize(format=\"ascii\")     # Terminal-friendly\nflow.visualize(format=\"plantuml\")  # Documentation-ready\nflow.visualize(format=\"mermaid\")   # GitHub/Notion compatible\n\nflow.to_svg(\"output.svg\")         # Save SVG to file\nflow.to_html(\"output.html\")       # Save HTML to file\nflow.to_ascii()                    # Return ASCII string\nflow.to_png(\"output.png\")         # Requires cairosvg\nflow.to_pdf(\"output.pdf\")         # Requires cairosvg\n</code></pre>"},{"location":"api/models/#serialization","title":"Serialization","text":"<pre><code># Export\nflow.to_dict()                     # -&gt; Dict[str, Any]\nflow.to_json(indent=2)             # -&gt; str (JSON)\nflow.to_yaml()                     # -&gt; str (YAML)\nflow.to_recipe_configs()           # -&gt; List[Dict] (Dataiku API-compatible)\nflow.export_all(\"output_dir/\")     # Export all artifacts to directory\n\n# Import (classmethods)\nDataikuFlow.from_dict(data)        # -&gt; DataikuFlow\nDataikuFlow.from_json(json_str)    # -&gt; DataikuFlow\nDataikuFlow.from_yaml(yaml_str)    # -&gt; DataikuFlow\n</code></pre>"},{"location":"api/models/#special-methods","title":"Special Methods","text":"<pre><code>len(flow)                          # Number of recipes\nfor recipe in flow:                # Iterate over recipes\n    print(recipe.name)\nflow._repr_svg_()                  # Jupyter notebook inline display\n</code></pre>"},{"location":"api/models/#zone-operations","title":"Zone Operations","text":"<pre><code>flow.add_zone(FlowZone(name=\"ETL\", color=\"#4b96e6\"))\nflow.get_zone(\"ETL\")              # -&gt; Optional[FlowZone]\n</code></pre>"},{"location":"api/models/#flowzone","title":"FlowZone","text":"<p>Logical grouping for flow elements.</p>"},{"location":"api/models/#fields_1","title":"Fields","text":"Field Type Default Description <code>name</code> <code>str</code> required Zone name <code>color</code> <code>str</code> <code>\"#4b96e6\"</code> Display color <code>datasets</code> <code>List[str]</code> <code>[]</code> Dataset names in this zone <code>recipes</code> <code>List[str]</code> <code>[]</code> Recipe names in this zone"},{"location":"api/models/#methods","title":"Methods","text":"<pre><code>zone.add_dataset(\"dataset_name\")\nzone.add_recipe(\"recipe_name\")\nzone.to_dict()\nFlowZone.from_dict(data)\n</code></pre>"},{"location":"api/models/#dataikurecipe","title":"DataikuRecipe","text":"<p>A recipe node in the flow (transformation step).</p> <pre><code>from py2dataiku import DataikuRecipe, RecipeType\n</code></pre>"},{"location":"api/models/#fields_2","title":"Fields","text":"Field Type Default Description <code>name</code> <code>str</code> required Recipe name <code>recipe_type</code> <code>RecipeType</code> required Recipe type <code>inputs</code> <code>List[str]</code> <code>[]</code> Input dataset names <code>outputs</code> <code>List[str]</code> <code>[]</code> Output dataset names <code>steps</code> <code>List[PrepareStep]</code> <code>[]</code> Processor steps (PREPARE recipes) <code>group_keys</code> <code>List[str]</code> <code>[]</code> Group-by columns (GROUPING) <code>aggregations</code> <code>List[Aggregation]</code> <code>[]</code> Aggregation configs (GROUPING) <code>join_type</code> <code>JoinType</code> <code>JoinType.LEFT</code> Join type (JOIN) <code>join_keys</code> <code>List[JoinKey]</code> <code>[]</code> Join key pairs (JOIN) <code>selected_columns</code> <code>Optional[Dict[str, List[str]]]</code> <code>None</code> Column selection (JOIN) <code>partition_columns</code> <code>List[str]</code> <code>[]</code> Partition columns (WINDOW) <code>order_columns</code> <code>List[str]</code> <code>[]</code> Order columns (WINDOW) <code>window_aggregations</code> <code>List[Dict[str, Any]]</code> <code>[]</code> Window functions (WINDOW) <code>sampling_method</code> <code>SamplingMethod</code> <code>SamplingMethod.RANDOM</code> Sampling method <code>sample_size</code> <code>Optional[int]</code> <code>None</code> Sample size <code>split_condition</code> <code>Optional[str]</code> <code>None</code> Split condition (SPLIT) <code>sort_columns</code> <code>List[Dict[str, str]]</code> <code>[]</code> Sort columns (SORT) <code>top_n</code> <code>Optional[int]</code> <code>None</code> Number of rows (TOP_N) <code>ranking_column</code> <code>Optional[str]</code> <code>None</code> Ranking column (TOP_N) <code>code</code> <code>Optional[str]</code> <code>None</code> Python code (PYTHON recipe) <code>settings</code> <code>Optional[RecipeSettings]</code> <code>None</code> Composed settings (takes precedence) <code>source_lines</code> <code>List[int]</code> <code>[]</code> Source code line numbers <code>notes</code> <code>List[str]</code> <code>[]</code> Notes"},{"location":"api/models/#factory-methods","title":"Factory Methods","text":"<pre><code># Create a PREPARE recipe\nrecipe = DataikuRecipe.create_prepare(\n    name=\"prepare_data\",\n    input_dataset=\"raw_data\",\n    output_dataset=\"cleaned_data\",\n    steps=[step1, step2]\n)\n\n# Create a GROUPING recipe\nrecipe = DataikuRecipe.create_grouping(\n    name=\"aggregate_sales\",\n    input_dataset=\"cleaned_data\",\n    output_dataset=\"summary\",\n    keys=[\"region\", \"category\"],\n    aggregations=[Aggregation(\"amount\", \"SUM\")]\n)\n\n# Create a JOIN recipe\nrecipe = DataikuRecipe.create_join(\n    name=\"join_tables\",\n    left_dataset=\"customers\",\n    right_dataset=\"orders\",\n    output_dataset=\"joined\",\n    join_keys=[JoinKey(\"customer_id\", \"cust_id\")],\n    join_type=JoinType.LEFT\n)\n\n# Create a PYTHON recipe\nrecipe = DataikuRecipe.create_python(\n    name=\"custom_transform\",\n    inputs=[\"input_data\"],\n    outputs=[\"output_data\"],\n    code=\"import pandas as pd\\n...\"\n)\n</code></pre>"},{"location":"api/models/#instance-methods","title":"Instance Methods","text":"<pre><code>recipe.add_step(step)                                    # Add processor step (PREPARE)\nrecipe.add_aggregation(\"amount\", \"SUM\", \"total_amount\")  # Add aggregation (GROUPING)\nrecipe.add_join_key(\"left_col\", \"right_col\")             # Add join key (JOIN)\nrecipe.add_note(\"Note text\")\nrecipe.get_step_summary()                                # -&gt; List[str] (PREPARE)\nrecipe.to_dict()                                         # -&gt; Dict[str, Any]\nrecipe.to_api_dict()                                     # Dataiku API-compatible dict\nrecipe.to_json()                                         # Alias for to_api_dict()\nDataikuRecipe.from_dict(data)                            # Classmethod\n</code></pre>"},{"location":"api/models/#dataikudataset","title":"DataikuDataset","text":"<p>A dataset node in the flow.</p> <pre><code>from py2dataiku import DataikuDataset, DatasetType, DatasetConnectionType\n</code></pre>"},{"location":"api/models/#fields_3","title":"Fields","text":"Field Type Default Description <code>name</code> <code>str</code> required Dataset name <code>dataset_type</code> <code>DatasetType</code> <code>DatasetType.INTERMEDIATE</code> INPUT, OUTPUT, or INTERMEDIATE <code>connection_type</code> <code>DatasetConnectionType</code> <code>DatasetConnectionType.FILESYSTEM</code> Connection type <code>schema</code> <code>List[ColumnSchema]</code> <code>[]</code> Column schema <code>source_variable</code> <code>Optional[str]</code> <code>None</code> Original Python variable name <code>source_line</code> <code>Optional[int]</code> <code>None</code> Line number in source <code>notes</code> <code>List[str]</code> <code>[]</code> Notes"},{"location":"api/models/#properties_1","title":"Properties","text":"Property Type Description <code>is_input</code> <code>bool</code> Whether this is an INPUT dataset <code>is_output</code> <code>bool</code> Whether this is an OUTPUT dataset"},{"location":"api/models/#methods_1","title":"Methods","text":"<pre><code>dataset.add_column(\"name\", \"string\", nullable=True)\ndataset.add_note(\"Note text\")\ndataset.to_dict()\ndataset.to_json()                # Dataiku API-compatible\nDataikuDataset.from_dict(data)   # Classmethod\n</code></pre>"},{"location":"api/models/#columnschema","title":"ColumnSchema","text":"<p>Column definition within a dataset.</p>"},{"location":"api/models/#fields_4","title":"Fields","text":"Field Type Default Description <code>name</code> <code>str</code> required Column name <code>type</code> <code>str</code> required Type: <code>\"string\"</code>, <code>\"int\"</code>, <code>\"float\"</code>, <code>\"date\"</code>, <code>\"boolean\"</code> <code>nullable</code> <code>bool</code> <code>True</code> Whether column allows nulls <code>default</code> <code>Optional[Any]</code> <code>None</code> Default value <code>format</code> <code>Optional[str]</code> <code>None</code> Format string (for dates)"},{"location":"api/models/#preparestep","title":"PrepareStep","text":"<p>A processor step within a PREPARE recipe.</p> <pre><code>from py2dataiku import PrepareStep, ProcessorType\n</code></pre>"},{"location":"api/models/#fields_5","title":"Fields","text":"Field Type Default Description <code>processor_type</code> <code>ProcessorType</code> required Processor type <code>params</code> <code>Dict[str, Any]</code> <code>{}</code> Processor parameters <code>disabled</code> <code>bool</code> <code>False</code> Whether step is disabled <code>name</code> <code>Optional[str]</code> <code>None</code> Step label <code>meta_type</code> <code>str</code> <code>\"PROCESSOR\"</code> PROCESSOR or GROUP <code>source_line</code> <code>Optional[int]</code> <code>None</code> Source code line <code>source_code</code> <code>Optional[str]</code> <code>None</code> Source code snippet"},{"location":"api/models/#factory-methods_1","title":"Factory Methods","text":"<pre><code># Fill missing values\nstep = PrepareStep.fill_empty(\"amount\", 0)\n\n# Rename columns\nstep = PrepareStep.rename_columns({\"old_name\": \"new_name\"})\n\n# Delete columns\nstep = PrepareStep.delete_columns([\"temp_col\", \"debug_col\"])\n\n# String transformation\nstep = PrepareStep.string_transform(\"name\", StringTransformerMode.UPPERCASE)\n\n# Set column type\nstep = PrepareStep.set_type(\"age\", \"int\")\n\n# Parse date\nstep = PrepareStep.parse_date(\"date_col\", formats=[\"yyyy-MM-dd\"])\n\n# Filter on value\nstep = PrepareStep.filter_on_value(\"status\", [\"active\", \"pending\"])\n\n# Remove empty rows\nstep = PrepareStep.remove_rows_on_empty([\"required_col\"])\n</code></pre>"},{"location":"api/models/#instance-methods_1","title":"Instance Methods","text":"<pre><code>step.to_dict()                    # -&gt; Dict[str, Any]\nstep.to_json()                    # Dataiku API-compatible\nPrepareStep.from_dict(data)       # Classmethod\n</code></pre>"},{"location":"api/models/#aggregation","title":"Aggregation","text":"<p>Aggregation configuration for GROUPING recipes.</p>"},{"location":"api/models/#fields_6","title":"Fields","text":"Field Type Default Description <code>column</code> <code>str</code> required Column to aggregate <code>function</code> <code>str</code> required <code>\"SUM\"</code>, <code>\"AVG\"</code>, <code>\"COUNT\"</code>, <code>\"MIN\"</code>, <code>\"MAX\"</code>, etc. <code>output_column</code> <code>Optional[str]</code> <code>None</code> Output column name"},{"location":"api/models/#joinkey","title":"JoinKey","text":"<p>Join key pair for JOIN recipes.</p>"},{"location":"api/models/#fields_7","title":"Fields","text":"Field Type Default Description <code>left_column</code> <code>str</code> required Column from left dataset <code>right_column</code> <code>str</code> required Column from right dataset <code>match_type</code> <code>str</code> <code>\"EXACT\"</code> <code>\"EXACT\"</code> or <code>\"FUZZY\"</code>"},{"location":"api/plugins/","title":"Plugin System","text":"<p>Extension system for custom recipe handlers, processor handlers, and pandas mappings.</p>"},{"location":"api/plugins/#pluginregistry","title":"PluginRegistry","text":"<p>Central registry for extending py-iku with custom behavior.</p> <pre><code>from py2dataiku import PluginRegistry\n</code></pre> <p>The registry is instance-based with a global default instance for convenience.</p>"},{"location":"api/plugins/#creating-an-instance","title":"Creating an Instance","text":"<pre><code># Use global default (shared state)\nfrom py2dataiku.plugins import PluginRegistry\nregistry = PluginRegistry.default()\n\n# Create isolated instance\nregistry = PluginRegistry()\n</code></pre>"},{"location":"api/plugins/#registering-mappings","title":"Registering Mappings","text":"<p>Map pandas method names to Dataiku recipe or processor types:</p> <pre><code>from py2dataiku import RecipeType, ProcessorType\n\n# Recipe mapping: pandas method -&gt; Dataiku recipe type\nregistry.add_recipe_mapping(\"df.my_custom_join\", RecipeType.JOIN)\n\n# Processor mapping: pandas method -&gt; Dataiku processor type\nregistry.add_processor_mapping(\"df.custom_transform\", ProcessorType.STRING_TRANSFORMER)\n</code></pre>"},{"location":"api/plugins/#registering-handlers","title":"Registering Handlers","text":"<p>Custom handler functions for recipe/processor generation:</p> <pre><code># Recipe handler\ndef custom_join_handler(recipe, transformation):\n    recipe.join_type = JoinType.LEFT\n    return recipe\n\nregistry.add_recipe_handler(RecipeType.JOIN, custom_join_handler)\n\n# Processor handler\ndef custom_processor_handler(step, transformation):\n    step.params[\"custom_param\"] = True\n    return step\n\nregistry.add_processor_handler(ProcessorType.STRING_TRANSFORMER, custom_processor_handler)\n\n# Method handler (called during AST analysis)\ndef custom_method_handler(node, context):\n    return Transformation(...)\n\nregistry.add_method_handler(\"my_custom_method\", custom_method_handler)\n</code></pre>"},{"location":"api/plugins/#looking-up-registrations","title":"Looking Up Registrations","text":"<pre><code>registry.find_recipe_mapping(\"df.my_custom_join\")       # -&gt; Optional[RecipeType]\nregistry.find_processor_mapping(\"df.custom_transform\")   # -&gt; Optional[ProcessorType]\nregistry.find_method_handler(\"my_custom_method\")         # -&gt; Optional[Callable]\nregistry.find_recipe_handler(RecipeType.JOIN)             # -&gt; Optional[Callable]\nregistry.find_processor_handler(ProcessorType.STRING_TRANSFORMER)  # -&gt; Optional[Callable]\n</code></pre>"},{"location":"api/plugins/#removing-registrations","title":"Removing Registrations","text":"<pre><code>registry.remove_recipe_mapping(\"df.my_custom_join\")      # -&gt; bool\nregistry.remove_processor_mapping(\"df.custom_transform\")  # -&gt; bool\nregistry.remove_method_handler(\"my_custom_method\")        # -&gt; bool\nregistry.clear_all()                                       # Remove everything\n</code></pre>"},{"location":"api/plugins/#copying","title":"Copying","text":"<pre><code>registry_copy = registry.copy()  # Independent copy\n</code></pre>"},{"location":"api/plugins/#convenience-functions","title":"Convenience Functions","text":"<p>Global convenience functions that operate on the default registry:</p> <pre><code>from py2dataiku import (\n    register_recipe_handler,\n    register_processor_handler,\n    register_pandas_mapping,\n    plugin_hook,\n)\n</code></pre>"},{"location":"api/plugins/#register_pandas_mapping","title":"register_pandas_mapping()","text":"<pre><code>register_pandas_mapping(\"df.my_method\", RecipeType.PREPARE)\n</code></pre>"},{"location":"api/plugins/#register_recipe_handler","title":"register_recipe_handler()","text":"<p>Decorator for registering recipe handlers:</p> <pre><code>@register_recipe_handler(RecipeType.PYTHON)\ndef handle_python_recipe(recipe, transformation):\n    recipe.code = \"# custom code\"\n    return recipe\n</code></pre>"},{"location":"api/plugins/#register_processor_handler","title":"register_processor_handler()","text":"<p>Decorator for registering processor handlers:</p> <pre><code>@register_processor_handler(ProcessorType.COLUMN_RENAMER)\ndef handle_rename(step, transformation):\n    step.params[\"prefix\"] = \"cleaned_\"\n    return step\n</code></pre>"},{"location":"api/plugins/#plugin_hook","title":"plugin_hook()","text":"<p>Decorator for registering generic plugin hooks:</p> <pre><code>@plugin_hook(\"pre_convert\")\ndef before_conversion(code, context):\n    return code.replace(\"old_api\", \"new_api\")\n</code></pre>"},{"location":"api/plugins/#class-level-registration","title":"Class-Level Registration","text":"<p>For backward compatibility, class-level methods are also available:</p> <pre><code>PluginRegistry.register_recipe_mapping(\"method\", RecipeType.JOIN)\nPluginRegistry.register_processor_mapping(\"method\", ProcessorType.BINNER)\nPluginRegistry.register_method_handler(\"method\", handler_fn)\nPluginRegistry.register_recipe_handler(RecipeType.JOIN, handler_fn)\nPluginRegistry.register_processor_handler(ProcessorType.BINNER, handler_fn)\n</code></pre> <p>These delegate to the global default instance.</p>"},{"location":"api/py2dataiku-class/","title":"Py2Dataiku Class","text":"<p>Main converter class with hybrid LLM + rule-based approach.</p> <pre><code>from py2dataiku import Py2Dataiku\n</code></pre>"},{"location":"api/py2dataiku-class/#constructor","title":"Constructor","text":"<pre><code>class Py2Dataiku:\n    def __init__(\n        self,\n        provider: str = \"anthropic\",\n        api_key: Optional[str] = None,\n        model: Optional[str] = None,\n        use_llm: bool = True,\n    )\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>provider</code> <code>str</code> <code>\"anthropic\"</code> LLM provider: <code>\"anthropic\"</code> or <code>\"openai\"</code> <code>api_key</code> <code>Optional[str]</code> <code>None</code> API key (uses environment variable if not provided) <code>model</code> <code>Optional[str]</code> <code>None</code> Model name override <code>use_llm</code> <code>bool</code> <code>True</code> Whether to use LLM (<code>True</code>) or rule-based (<code>False</code>) <p>Notes: - If <code>use_llm=True</code> but LLM initialization fails, automatically falls back to rule-based with a warning - Rule-based mode does not require any API key</p> <p>Example:</p> <pre><code># LLM mode (recommended)\nconverter = Py2Dataiku(provider=\"anthropic\")\n\n# Rule-based mode (offline)\nconverter = Py2Dataiku(use_llm=False)\n\n# OpenAI with specific model\nconverter = Py2Dataiku(provider=\"openai\", model=\"gpt-4o\")\n</code></pre>"},{"location":"api/py2dataiku-class/#methods","title":"Methods","text":""},{"location":"api/py2dataiku-class/#convert","title":"<code>convert()</code>","text":"<p>Convert Python code to a Dataiku flow.</p> <pre><code>def convert(\n    self,\n    code: str,\n    flow_name: str = \"converted_flow\",\n    optimize: bool = True,\n) -&gt; DataikuFlow\n</code></pre> Parameter Type Default Description <code>code</code> <code>str</code> required Python source code <code>flow_name</code> <code>str</code> <code>\"converted_flow\"</code> Name for the generated flow <code>optimize</code> <code>bool</code> <code>True</code> Whether to optimize the flow <p>Returns: <code>DataikuFlow</code></p>"},{"location":"api/py2dataiku-class/#analyze","title":"<code>analyze()</code>","text":"<p>Analyze code without generating flow (LLM mode only).</p> <pre><code>def analyze(self, code: str) -&gt; AnalysisResult\n</code></pre> Parameter Type Default Description <code>code</code> <code>str</code> required Python source code <p>Returns: <code>AnalysisResult</code></p> <p>Raises: <code>ValueError</code> if not in LLM mode</p>"},{"location":"api/py2dataiku-class/#generate_diagram","title":"<code>generate_diagram()</code>","text":"<p>Generate a diagram for a flow (legacy method).</p> <pre><code>def generate_diagram(self, flow: DataikuFlow, format: str = \"mermaid\") -&gt; str\n</code></pre> Parameter Type Default Description <code>flow</code> <code>DataikuFlow</code> required Flow to visualize <code>format</code> <code>str</code> <code>\"mermaid\"</code> <code>\"mermaid\"</code>, <code>\"graphviz\"</code>, <code>\"ascii\"</code>, <code>\"plantuml\"</code> <p>Returns: <code>str</code> - Diagram in the specified format</p>"},{"location":"api/py2dataiku-class/#visualize","title":"<code>visualize()</code>","text":"<p>Generate Dataiku-style visualization of a flow.</p> <pre><code>def visualize(self, flow: DataikuFlow, format: str = \"svg\", **kwargs) -&gt; str\n</code></pre> Parameter Type Default Description <code>flow</code> <code>DataikuFlow</code> required Flow to visualize <code>format</code> <code>str</code> <code>\"svg\"</code> <code>\"svg\"</code>, <code>\"html\"</code>, <code>\"ascii\"</code>, <code>\"plantuml\"</code>, <code>\"mermaid\"</code> <p>Returns: <code>str</code> - Visualization in the specified format</p>"},{"location":"api/py2dataiku-class/#save_visualization","title":"<code>save_visualization()</code>","text":"<p>Save flow visualization to a file.</p> <pre><code>def save_visualization(\n    self,\n    flow: DataikuFlow,\n    output_path: str,\n    format: str = None,\n) -&gt; None\n</code></pre> Parameter Type Default Description <code>flow</code> <code>DataikuFlow</code> required Flow to visualize <code>output_path</code> <code>str</code> required Path to save the file <code>format</code> <code>str</code> <code>None</code> Format (auto-detected from extension if <code>None</code>) <p>Auto-detected extensions: <code>.svg</code>, <code>.html</code>/<code>.htm</code>, <code>.txt</code> (ASCII), <code>.puml</code> (PlantUML), <code>.png</code>, <code>.pdf</code></p> <p>Notes: - PNG and PDF export require <code>cairosvg</code> to be installed</p>"},{"location":"api/recipe-settings/","title":"Recipe Settings","text":"<p>Typed settings classes for recipe configuration using the composition pattern. Each recipe type has a dedicated settings class that provides type-safe configuration.</p>"},{"location":"api/recipe-settings/#recipesettings-abc","title":"RecipeSettings (ABC)","text":"<p>Abstract base class for all recipe settings.</p> <pre><code>from py2dataiku.models.recipe_settings import RecipeSettings\n</code></pre>"},{"location":"api/recipe-settings/#abstract-methods","title":"Abstract Methods","text":"<pre><code>def to_dict(self) -&gt; Dict[str, Any]         # Dataiku API-compatible dict\ndef to_display_dict(self) -&gt; Dict[str, Any]  # Human-readable dict\n</code></pre>"},{"location":"api/recipe-settings/#usage-with-dataikurecipe","title":"Usage with DataikuRecipe","text":"<p>Settings are composed into recipes via the <code>settings</code> field:</p> <pre><code>from py2dataiku import DataikuRecipe, RecipeType\nfrom py2dataiku.models.recipe_settings import GroupingSettings\n\nrecipe = DataikuRecipe(\n    name=\"aggregate_sales\",\n    recipe_type=RecipeType.GROUPING,\n    inputs=[\"cleaned_data\"],\n    outputs=[\"summary\"],\n    settings=GroupingSettings(\n        keys=[\"region\", \"category\"],\n        aggregations=[{\"column\": \"amount\", \"function\": \"SUM\"}],\n    ),\n)\n</code></pre> <p>When <code>settings</code> is present, it takes precedence over the recipe's direct fields (<code>group_keys</code>, <code>aggregations</code>, etc.).</p>"},{"location":"api/recipe-settings/#preparesettings","title":"PrepareSettings","text":"<p>Settings for PREPARE recipes.</p>"},{"location":"api/recipe-settings/#fields","title":"Fields","text":"Field Type Default Description <code>steps</code> <code>List[Dict[str, Any]]</code> <code>[]</code> Processor steps"},{"location":"api/recipe-settings/#groupingsettings","title":"GroupingSettings","text":"<p>Settings for GROUPING recipes (group-by + aggregate).</p>"},{"location":"api/recipe-settings/#fields_1","title":"Fields","text":"Field Type Default Description <code>keys</code> <code>List[str]</code> <code>[]</code> Group-by columns <code>aggregations</code> <code>List[Dict[str, Any]]</code> <code>[]</code> Aggregation definitions <code>pre_filter</code> <code>Optional[str]</code> <code>None</code> Pre-aggregation filter"},{"location":"api/recipe-settings/#joinsettings","title":"JoinSettings","text":"<p>Settings for JOIN recipes.</p>"},{"location":"api/recipe-settings/#fields_2","title":"Fields","text":"Field Type Default Description <code>join_type</code> <code>str</code> <code>\"LEFT\"</code> Join type <code>join_keys</code> <code>List[Dict[str, str]]</code> <code>[]</code> Join key pairs <code>selected_columns</code> <code>Dict[str, List[str]]</code> <code>{}</code> Columns to select per input <code>post_filter</code> <code>Optional[str]</code> <code>None</code> Post-join filter"},{"location":"api/recipe-settings/#windowsettings","title":"WindowSettings","text":"<p>Settings for WINDOW recipes (window functions).</p>"},{"location":"api/recipe-settings/#fields_3","title":"Fields","text":"Field Type Default Description <code>partition_columns</code> <code>List[str]</code> <code>[]</code> Partition-by columns <code>order_columns</code> <code>List[Dict[str, str]]</code> <code>[]</code> Order-by columns <code>aggregations</code> <code>List[Dict[str, Any]]</code> <code>[]</code> Window function definitions"},{"location":"api/recipe-settings/#pivotsettings","title":"PivotSettings","text":"<p>Settings for PIVOT recipes.</p>"},{"location":"api/recipe-settings/#fields_4","title":"Fields","text":"Field Type Default Description <code>row_keys</code> <code>List[str]</code> <code>[]</code> Row identifier columns <code>column_key</code> <code>Optional[str]</code> <code>None</code> Column to pivot on <code>aggregations</code> <code>List[Dict[str, Any]]</code> <code>[]</code> Value aggregations"},{"location":"api/recipe-settings/#splitsettings","title":"SplitSettings","text":"<p>Settings for SPLIT recipes.</p>"},{"location":"api/recipe-settings/#fields_5","title":"Fields","text":"Field Type Default Description <code>split_mode</code> <code>str</code> <code>\"FILTER\"</code> Split mode <code>condition</code> <code>Optional[str]</code> <code>None</code> Split condition/formula <code>column</code> <code>Optional[str]</code> <code>None</code> Column for value-based split <code>ratio</code> <code>Optional[float]</code> <code>None</code> Split ratio for random"},{"location":"api/recipe-settings/#sortsettings","title":"SortSettings","text":"<p>Settings for SORT recipes.</p>"},{"location":"api/recipe-settings/#fields_6","title":"Fields","text":"Field Type Default Description <code>columns</code> <code>List[Dict[str, str]]</code> <code>[]</code> Sort columns with direction <p>Column format: <code>{\"column\": \"name\", \"order\": \"ASC\"}</code> or <code>{\"column\": \"name\", \"order\": \"DESC\"}</code></p>"},{"location":"api/recipe-settings/#stacksettings","title":"StackSettings","text":"<p>Settings for STACK recipes (vertical concatenation).</p>"},{"location":"api/recipe-settings/#fields_7","title":"Fields","text":"Field Type Default Description <code>mode</code> <code>str</code> <code>\"UNION\"</code> Stack mode: <code>\"UNION\"</code> or <code>\"INTERSECT\"</code> <code>add_origin_column</code> <code>bool</code> <code>False</code> Add column indicating source <code>origin_column_name</code> <code>str</code> <code>\"origin\"</code> Name of origin column"},{"location":"api/recipe-settings/#samplingsettings","title":"SamplingSettings","text":"<p>Settings for SAMPLING recipes.</p>"},{"location":"api/recipe-settings/#fields_8","title":"Fields","text":"Field Type Default Description <code>method</code> <code>str</code> <code>\"RANDOM\"</code> Sampling method <code>sample_size</code> <code>Optional[int]</code> <code>None</code> Number of rows <code>ratio</code> <code>Optional[float]</code> <code>None</code> Sampling ratio <code>seed</code> <code>Optional[int]</code> <code>None</code> Random seed <code>stratify_column</code> <code>Optional[str]</code> <code>None</code> Stratification column"},{"location":"api/recipe-settings/#topnsettings","title":"TopNSettings","text":"<p>Settings for TOP_N recipes.</p>"},{"location":"api/recipe-settings/#fields_9","title":"Fields","text":"Field Type Default Description <code>n</code> <code>int</code> <code>10</code> Number of rows <code>ranking_column</code> <code>Optional[str]</code> <code>None</code> Column to rank by <code>ascending</code> <code>bool</code> <code>False</code> Sort direction <code>partition_columns</code> <code>List[str]</code> <code>[]</code> Top-N per group"},{"location":"api/recipe-settings/#distinctsettings","title":"DistinctSettings","text":"<p>Settings for DISTINCT recipes.</p>"},{"location":"api/recipe-settings/#fields_10","title":"Fields","text":"Field Type Default Description <code>columns</code> <code>List[str]</code> <code>[]</code> Columns to consider for dedup <code>keep</code> <code>str</code> <code>\"FIRST\"</code> Which duplicate to keep: <code>\"FIRST\"</code>, <code>\"LAST\"</code>"},{"location":"api/recipe-settings/#pythonsettings","title":"PythonSettings","text":"<p>Settings for PYTHON code recipes.</p>"},{"location":"api/recipe-settings/#fields_11","title":"Fields","text":"Field Type Default Description <code>code</code> <code>str</code> <code>\"\"</code> Python code <code>env_name</code> <code>Optional[str]</code> <code>None</code> Code environment name <code>container</code> <code>Optional[str]</code> <code>None</code> Container configuration"},{"location":"api/scenarios-metrics/","title":"Scenarios &amp; Metrics","text":"<p>Automation, monitoring, and data quality models for Dataiku DSS.</p>"},{"location":"api/scenarios-metrics/#dataikuscenario","title":"DataikuScenario","text":"<p>Automation scenario with triggers, steps, and reporters.</p> <pre><code>from py2dataiku import DataikuScenario, ScenarioTrigger, ScenarioStep, ScenarioReporter\n</code></pre>"},{"location":"api/scenarios-metrics/#fields","title":"Fields","text":"Field Type Default Description <code>name</code> <code>str</code> required Scenario name <code>triggers</code> <code>List[ScenarioTrigger]</code> <code>[]</code> Event triggers <code>steps</code> <code>List[ScenarioStep]</code> <code>[]</code> Execution steps <code>reporters</code> <code>List[ScenarioReporter]</code> <code>[]</code> Notification reporters <code>active</code> <code>bool</code> <code>True</code> Whether scenario is active"},{"location":"api/scenarios-metrics/#methods","title":"Methods","text":"<pre><code>scenario.to_dict()\nDataikuScenario.from_dict(data)\n</code></pre>"},{"location":"api/scenarios-metrics/#example","title":"Example","text":"<pre><code>scenario = DataikuScenario(\n    name=\"daily_etl\",\n    triggers=[\n        ScenarioTrigger.time_based(\"daily_6am\", \"0 6 * * *\"),\n        ScenarioTrigger.dataset_change(\"on_raw_update\", \"raw_data\"),\n    ],\n    steps=[\n        ScenarioStep.build(\"build_pipeline\", \"output_dataset\"),\n        ScenarioStep.run_checks(\"validate_output\", \"output_dataset\"),\n    ],\n    reporters=[\n        ScenarioReporter.email(\"notify_team\", [\"team@example.com\"]),\n        ScenarioReporter.slack(\"slack_alert\", \"#data-alerts\"),\n    ],\n)\n</code></pre>"},{"location":"api/scenarios-metrics/#scenariotrigger","title":"ScenarioTrigger","text":"<p>Event that starts a scenario.</p>"},{"location":"api/scenarios-metrics/#fields_1","title":"Fields","text":"Field Type Default Description <code>name</code> <code>str</code> required Trigger name <code>trigger_type</code> <code>TriggerType</code> required Trigger type <code>params</code> <code>Dict[str, Any]</code> <code>{}</code> Trigger parameters <code>active</code> <code>bool</code> <code>True</code> Whether trigger is active"},{"location":"api/scenarios-metrics/#factory-methods","title":"Factory Methods","text":"<pre><code># Cron-based schedule\ntrigger = ScenarioTrigger.time_based(\"daily_6am\", cron=\"0 6 * * *\", timezone=\"UTC\")\n\n# Triggered when dataset changes\ntrigger = ScenarioTrigger.dataset_change(\"on_update\", dataset=\"raw_data\")\n</code></pre>"},{"location":"api/scenarios-metrics/#triggertype","title":"TriggerType","text":"Value Description <code>TIME_BASED</code> Cron schedule <code>DATASET_CHANGE</code> Dataset modification <code>SQL_QUERY</code> SQL condition check <code>PYTHON</code> Custom Python condition"},{"location":"api/scenarios-metrics/#scenariostep","title":"ScenarioStep","text":"<p>An action within a scenario.</p>"},{"location":"api/scenarios-metrics/#fields_2","title":"Fields","text":"Field Type Default Description <code>name</code> <code>str</code> required Step name <code>step_type</code> <code>StepType</code> required Step type <code>params</code> <code>Dict[str, Any]</code> <code>{}</code> Step parameters"},{"location":"api/scenarios-metrics/#factory-methods_1","title":"Factory Methods","text":"<pre><code>ScenarioStep.build(\"build_output\", dataset=\"output_dataset\")\nScenarioStep.train(\"retrain_model\", model_id=\"my_model\")\nScenarioStep.run_checks(\"validate\", dataset=\"output_dataset\")\nScenarioStep.execute_python(\"custom\", code=\"print('done')\")\nScenarioStep.send_message(\"notify\", channel=\"slack\", message=\"Build complete\")\n</code></pre>"},{"location":"api/scenarios-metrics/#steptype","title":"StepType","text":"Value Description <code>BUILD</code> Build a dataset <code>TRAIN</code> Train a model <code>CHECK</code> Run data checks <code>SQL_EXECUTE</code> Execute SQL <code>PYTHON_EXECUTE</code> Execute Python <code>SEND_MESSAGE</code> Send notification"},{"location":"api/scenarios-metrics/#scenarioreporter","title":"ScenarioReporter","text":"<p>Notification configuration for scenario outcomes.</p>"},{"location":"api/scenarios-metrics/#fields_3","title":"Fields","text":"Field Type Default Description <code>name</code> <code>str</code> required Reporter name <code>reporter_type</code> <code>ReporterType</code> required Reporter type <code>params</code> <code>Dict[str, Any]</code> <code>{}</code> Reporter parameters <code>on_success</code> <code>bool</code> <code>True</code> Notify on success <code>on_failure</code> <code>bool</code> <code>True</code> Notify on failure"},{"location":"api/scenarios-metrics/#factory-methods_2","title":"Factory Methods","text":"<pre><code>ScenarioReporter.email(\"email_team\", recipients=[\"team@example.com\"], subject=\"ETL Report\")\nScenarioReporter.slack(\"slack_alert\", channel=\"#data-alerts\")\n</code></pre>"},{"location":"api/scenarios-metrics/#reportertype","title":"ReporterType","text":"Value Description <code>EMAIL</code> Email notification <code>SLACK</code> Slack message <code>WEBHOOK</code> HTTP webhook"},{"location":"api/scenarios-metrics/#dataikumetric","title":"DataikuMetric","text":"<p>Dataset metric definition.</p> <pre><code>from py2dataiku import DataikuMetric, MetricType\n</code></pre>"},{"location":"api/scenarios-metrics/#fields_4","title":"Fields","text":"Field Type Default Description <code>name</code> <code>str</code> required Metric name <code>metric_type</code> <code>MetricType</code> required Metric type <code>column</code> <code>Optional[str]</code> <code>None</code> Target column <code>params</code> <code>Dict[str, Any]</code> <code>{}</code> Additional parameters"},{"location":"api/scenarios-metrics/#factory-methods_3","title":"Factory Methods","text":"<pre><code>DataikuMetric.row_count()\nDataikuMetric.column_min(\"price\")\nDataikuMetric.column_max(\"price\")\nDataikuMetric.column_avg(\"price\")\nDataikuMetric.column_missing(\"email\")\nDataikuMetric.custom_sql(\"revenue\", query=\"SELECT SUM(amount) FROM data\")\n</code></pre>"},{"location":"api/scenarios-metrics/#metrictype","title":"MetricType","text":"Value Description <code>ROW_COUNT</code> Total row count <code>COLUMN_MIN</code> Minimum value <code>COLUMN_MAX</code> Maximum value <code>COLUMN_AVG</code> Average value <code>COLUMN_SUM</code> Sum of values <code>COLUMN_STDDEV</code> Standard deviation <code>COLUMN_DISTINCT</code> Distinct count <code>COLUMN_MISSING</code> Missing value count <code>CUSTOM_SQL</code> Custom SQL metric <code>CUSTOM_PYTHON</code> Custom Python metric"},{"location":"api/scenarios-metrics/#dataikucheck","title":"DataikuCheck","text":"<p>Validation check against a metric.</p> <pre><code>from py2dataiku import DataikuCheck, CheckCondition, CheckSeverity\n</code></pre>"},{"location":"api/scenarios-metrics/#fields_5","title":"Fields","text":"Field Type Default Description <code>name</code> <code>str</code> required Check name <code>metric_name</code> <code>str</code> required Metric to check against <code>condition</code> <code>CheckCondition</code> required Check condition <code>value</code> <code>Any</code> <code>None</code> Comparison value <code>min_value</code> <code>Optional[Any]</code> <code>None</code> Range minimum <code>max_value</code> <code>Optional[Any]</code> <code>None</code> Range maximum <code>severity</code> <code>CheckSeverity</code> <code>CheckSeverity.ERROR</code> Failure severity"},{"location":"api/scenarios-metrics/#factory-methods_4","title":"Factory Methods","text":"<pre><code>DataikuCheck.not_empty(\"has_rows\", metric_name=\"row_count\")\nDataikuCheck.between(\"price_range\", metric_name=\"price_avg\", min_value=10, max_value=1000)\n</code></pre>"},{"location":"api/scenarios-metrics/#checkcondition","title":"CheckCondition","text":"<p><code>EQUALS</code>, <code>NOT_EQUALS</code>, <code>GREATER_THAN</code>, <code>LESS_THAN</code>, <code>GREATER_OR_EQUAL</code>, <code>LESS_OR_EQUAL</code>, <code>BETWEEN</code>, <code>NOT_EMPTY</code></p>"},{"location":"api/scenarios-metrics/#checkseverity","title":"CheckSeverity","text":"Value Description <code>WARNING</code> Non-blocking warning <code>ERROR</code> Blocking error"},{"location":"api/scenarios-metrics/#dataqualityrule","title":"DataQualityRule","text":"<p>Column-level data quality rule.</p> <pre><code>from py2dataiku import DataQualityRule\n</code></pre>"},{"location":"api/scenarios-metrics/#fields_6","title":"Fields","text":"Field Type Default Description <code>name</code> <code>str</code> required Rule name <code>column</code> <code>str</code> required Target column <code>rule_type</code> <code>str</code> required Rule type <code>params</code> <code>Dict[str, Any]</code> <code>{}</code> Rule parameters"},{"location":"api/scenarios-metrics/#rule-types","title":"Rule Types","text":"Type Description <code>\"not_null\"</code> Column must not contain nulls <code>\"unique\"</code> Column values must be unique <code>\"in_range\"</code> Values must be within a range <code>\"regex_match\"</code> Values must match a pattern <code>\"in_set\"</code> Values must be in an allowed set"},{"location":"api/scenarios-metrics/#example_1","title":"Example","text":"<pre><code>rules = [\n    DataQualityRule(\"email_not_null\", \"email\", \"not_null\"),\n    DataQualityRule(\"age_range\", \"age\", \"in_range\", {\"min\": 0, \"max\": 150}),\n    DataQualityRule(\"status_valid\", \"status\", \"in_set\", {\"values\": [\"active\", \"inactive\"]}),\n    DataQualityRule(\"email_format\", \"email\", \"regex_match\", {\"pattern\": r\"^[\\w.]+@[\\w.]+$\"}),\n]\n</code></pre>"},{"location":"api/visualizers/","title":"Visualizers","text":"<p>Visualization engines for rendering Dataiku flows in multiple formats.</p>"},{"location":"api/visualizers/#visualize_flow","title":"visualize_flow()","text":"<p>Convenience function to generate a visual representation of a flow.</p> <pre><code>from py2dataiku.visualizers import visualize_flow\n\nsvg = visualize_flow(flow, format=\"svg\")\nhtml = visualize_flow(flow, format=\"html\")\nascii_art = visualize_flow(flow, format=\"ascii\")\nplantuml = visualize_flow(flow, format=\"plantuml\")\nmermaid = visualize_flow(flow, format=\"mermaid\")\ninteractive = visualize_flow(flow, format=\"interactive\")\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>flow</code> <code>DataikuFlow</code> required Flow to visualize <code>format</code> <code>str</code> <code>\"svg\"</code> Output format <code>**kwargs</code> Additional visualizer arguments <p>Returns: <code>str</code> - Visualization content</p> <p>Supported formats: <code>\"svg\"</code>, <code>\"ascii\"</code>, <code>\"plantuml\"</code>, <code>\"html\"</code>, <code>\"interactive\"</code>, <code>\"mermaid\"</code></p>"},{"location":"api/visualizers/#svgvisualizer","title":"SVGVisualizer","text":"<p>Generates pixel-accurate SVG that matches Dataiku DSS interface styling.</p> <pre><code>from py2dataiku.visualizers import SVGVisualizer\n\nviz = SVGVisualizer()\nsvg = viz.render(flow)\n</code></pre>"},{"location":"api/visualizers/#methods","title":"Methods","text":"Method Returns Description <code>render(flow)</code> <code>str</code> SVG markup <code>export_png(flow, path, scale=2.0)</code> <code>None</code> Export as PNG (requires cairosvg) <code>export_pdf(flow, path)</code> <code>None</code> Export as PDF (requires cairosvg)"},{"location":"api/visualizers/#htmlvisualizer","title":"HTMLVisualizer","text":"<p>Generates interactive HTML canvas with hover and click support.</p> <pre><code>from py2dataiku.visualizers import HTMLVisualizer\n\nviz = HTMLVisualizer()\nhtml = viz.render(flow)\n</code></pre>"},{"location":"api/visualizers/#methods_1","title":"Methods","text":"Method Returns Description <code>render(flow)</code> <code>str</code> Complete HTML document"},{"location":"api/visualizers/#asciivisualizer","title":"ASCIIVisualizer","text":"<p>Generates terminal-friendly text art representation.</p> <pre><code>from py2dataiku.visualizers import ASCIIVisualizer\n\nviz = ASCIIVisualizer()\ntext = viz.render(flow)\nprint(text)\n</code></pre>"},{"location":"api/visualizers/#methods_2","title":"Methods","text":"Method Returns Description <code>render(flow)</code> <code>str</code> ASCII diagram"},{"location":"api/visualizers/#mermaidvisualizer","title":"MermaidVisualizer","text":"<p>Generates Mermaid diagram syntax compatible with GitHub, Notion, and documentation tools.</p> <pre><code>from py2dataiku.visualizers.mermaid_visualizer import MermaidVisualizer\n\nviz = MermaidVisualizer()\ndiagram = viz.render(flow)\n</code></pre>"},{"location":"api/visualizers/#methods_3","title":"Methods","text":"Method Returns Description <code>render(flow)</code> <code>str</code> Mermaid diagram syntax <p>Output example:</p> <pre><code>graph LR\n    raw_data[(raw_data)] --&gt; prepare_1[prepare_1]\n    prepare_1 --&gt; cleaned_data[(cleaned_data)]\n    cleaned_data --&gt; grouping_1[grouping_1]\n    grouping_1 --&gt; summary[(summary)]</code></pre>"},{"location":"api/visualizers/#plantumlvisualizer","title":"PlantUMLVisualizer","text":"<p>Generates PlantUML diagram syntax for documentation.</p> <pre><code>from py2dataiku.visualizers import PlantUMLVisualizer\n\nviz = PlantUMLVisualizer()\npuml = viz.render(flow)\n</code></pre>"},{"location":"api/visualizers/#methods_4","title":"Methods","text":"Method Returns Description <code>render(flow)</code> <code>str</code> PlantUML diagram syntax"},{"location":"api/visualizers/#interactivevisualizer","title":"InteractiveVisualizer","text":"<p>Enhanced HTML with pan/zoom, search, and export capabilities.</p> <pre><code>from py2dataiku.visualizers.interactive_visualizer import InteractiveVisualizer\n\nviz = InteractiveVisualizer()\nhtml = viz.render(flow)\n</code></pre>"},{"location":"api/visualizers/#themes","title":"Themes","text":"<p>Pre-defined visual themes for SVG and HTML visualizers.</p> <pre><code>from py2dataiku import DATAIKU_LIGHT, DATAIKU_DARK, DataikuTheme\n\n# Use built-in themes\nsvg = flow.visualize(format=\"svg\", theme=DATAIKU_LIGHT)\nsvg = flow.visualize(format=\"svg\", theme=DATAIKU_DARK)\n</code></pre>"},{"location":"api/visualizers/#dataikutheme","title":"DataikuTheme","text":"<p>Theme configuration dataclass. Contains colors, fonts, spacing, and styling parameters for recipes, datasets, and flow connections.</p>"},{"location":"api/visualizers/#built-in-themes","title":"Built-in Themes","text":"Theme Description <code>DATAIKU_LIGHT</code> Light theme matching Dataiku DSS default <code>DATAIKU_DARK</code> Dark theme for dark mode"},{"location":"api/visualizers/#using-via-dataikuflow","title":"Using via DataikuFlow","text":"<p>The most common way to generate visualizations is through <code>DataikuFlow</code> methods:</p> <pre><code>flow = convert(code)\n\n# String output\nflow.visualize(format=\"svg\")\nflow.visualize(format=\"html\")\nflow.visualize(format=\"ascii\")\nflow.visualize(format=\"plantuml\")\nflow.visualize(format=\"mermaid\")\n\n# File output\nflow.to_svg(\"output.svg\")\nflow.to_html(\"output.html\")\nflow.to_plantuml(\"output.puml\")\nflow.to_png(\"output.png\")   # Requires cairosvg\nflow.to_pdf(\"output.pdf\")   # Requires cairosvg\n\n# Jupyter notebooks\nflow  # Automatically renders SVG via _repr_svg_()\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#basic-install","title":"Basic Install","text":"<pre><code>pip install py-iku\n</code></pre> <p>This installs the core library with rule-based conversion support.</p>"},{"location":"getting-started/installation/#with-llm-support","title":"With LLM Support","text":"<p>For LLM-based conversion (recommended for complex pipelines):</p> <pre><code>pip install py-iku[llm]\n</code></pre> <p>This adds <code>anthropic</code> and <code>openai</code> as dependencies.</p>"},{"location":"getting-started/installation/#development-install","title":"Development Install","text":"<p>For contributing or running tests:</p> <pre><code>git clone https://github.com/m-deane/py-iku.git\ncd py-iku\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"Package Purpose Install <code>anthropic</code> Claude LLM analysis <code>pip install py-iku[llm]</code> <code>openai</code> GPT LLM analysis <code>pip install py-iku[llm]</code> <code>cairosvg</code> PNG/PDF export from SVG <code>pip install cairosvg</code>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.9+</li> <li>pyyaml &gt;= 6.0</li> </ul>"},{"location":"getting-started/installation/#api-keys","title":"API Keys","text":"<p>For LLM-based conversion, set your API key as an environment variable:</p> <pre><code># Anthropic (Claude)\nexport ANTHROPIC_API_KEY=\"your-key-here\"\n\n# OpenAI (GPT)\nexport OPENAI_API_KEY=\"your-key-here\"\n</code></pre> <p>Or pass it directly:</p> <pre><code>flow = convert_with_llm(code, provider=\"anthropic\", api_key=\"your-key\")\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>import py2dataiku\nprint(py2dataiku.__version__)\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":""},{"location":"getting-started/quickstart/#convert-python-code","title":"Convert Python Code","text":"<pre><code>from py2dataiku import convert\n\nflow = convert(\"\"\"\nimport pandas as pd\n\n# Read data\ndf = pd.read_csv('customers.csv')\norders = pd.read_csv('orders.csv')\n\n# Clean data\ndf = df.dropna(subset=['email'])\ndf['name'] = df['name'].str.strip()\n\n# Join\nmerged = df.merge(orders, on='customer_id', how='left')\n\n# Aggregate\nsummary = merged.groupby('region').agg({\n    'amount': 'sum',\n    'customer_id': 'nunique'\n})\n\n# Save\nsummary.to_csv('regional_summary.csv')\n\"\"\")\n\nprint(flow.get_summary())\n</code></pre>"},{"location":"getting-started/quickstart/#visualize-the-flow","title":"Visualize the Flow","text":"<pre><code># Terminal-friendly\nprint(flow.visualize(format=\"ascii\"))\n\n# Pixel-accurate Dataiku styling\nsvg = flow.visualize(format=\"svg\")\n\n# Interactive HTML\nhtml = flow.visualize(format=\"html\")\n\n# GitHub/Notion compatible\nmermaid = flow.visualize(format=\"mermaid\")\n\n# Save to file\nflow.to_svg(\"flow.svg\")\nflow.to_html(\"flow.html\")\n</code></pre>"},{"location":"getting-started/quickstart/#inspect-the-flow","title":"Inspect the Flow","text":"<pre><code># Datasets\nfor ds in flow.datasets:\n    print(f\"{ds.name} ({ds.dataset_type.value})\")\n\n# Recipes\nfor recipe in flow.recipes:\n    print(f\"{recipe.name}: {recipe.recipe_type.value}\")\n    print(f\"  Inputs: {recipe.inputs}\")\n    print(f\"  Outputs: {recipe.outputs}\")\n\n# Validate\nresult = flow.validate()\nprint(result)\n</code></pre>"},{"location":"getting-started/quickstart/#export","title":"Export","text":"<pre><code># JSON\njson_str = flow.to_json()\n\n# YAML\nyaml_str = flow.to_yaml()\n\n# Round-trip\nfrom py2dataiku import DataikuFlow\nflow2 = DataikuFlow.from_json(json_str)\n\n# DSS project bundle\nfrom py2dataiku import export_to_dss\nexport_to_dss(flow, \"output/my_project\", create_zip=True)\n</code></pre>"},{"location":"getting-started/quickstart/#use-llm-for-better-results","title":"Use LLM for Better Results","text":"<pre><code>from py2dataiku import convert_with_llm\n\n# Requires ANTHROPIC_API_KEY environment variable\nflow = convert_with_llm(code, provider=\"anthropic\")\n\n# Or with OpenAI\nflow = convert_with_llm(code, provider=\"openai\")\n\n# With explicit API key\nflow = convert_with_llm(code, provider=\"anthropic\", api_key=\"sk-...\")\n</code></pre>"},{"location":"getting-started/quickstart/#convert-from-file","title":"Convert from File","text":"<pre><code>from py2dataiku import convert_file, convert_file_with_llm\n\n# Rule-based\nflow = convert_file(\"pipeline.py\")\n\n# LLM-based\nflow = convert_file_with_llm(\"pipeline.py\", provider=\"anthropic\")\n</code></pre>"},{"location":"getting-started/quickstart/#use-the-class-interface","title":"Use the Class Interface","text":"<pre><code>from py2dataiku import Py2Dataiku\n\n# LLM mode (falls back to rule-based if no API key)\nconverter = Py2Dataiku(provider=\"anthropic\")\n\n# Rule-based only\nconverter = Py2Dataiku(use_llm=False)\n\n# Convert\nflow = converter.convert(code)\n\n# Visualize\nconverter.save_visualization(flow, \"output.svg\")\nconverter.save_visualization(flow, \"output.html\")\nconverter.save_visualization(flow, \"output.png\")  # Requires cairosvg\n</code></pre>"},{"location":"getting-started/quickstart/#column-lineage","title":"Column Lineage","text":"<pre><code># Trace a column through the flow\nlineage = flow.get_column_lineage(\"amount\")\nprint(lineage)\n</code></pre>"},{"location":"getting-started/quickstart/#dag-analysis","title":"DAG Analysis","text":"<pre><code>graph = flow.graph\n\n# Execution order\norder = graph.topological_sort()\n\n# Check for cycles\ncycles = graph.detect_cycles()\n\n# Independent sub-pipelines\nsubgraphs = graph.find_disconnected_subgraphs()\n</code></pre>"},{"location":"getting-started/quickstart/#configuration","title":"Configuration","text":"<p>Create a <code>py2dataiku.toml</code> in your project root:</p> <pre><code>[py2dataiku]\ndefault_provider = \"anthropic\"\nproject_key = \"MY_PROJECT\"\noptimize = true\noptimization_level = 2\ndefault_format = \"svg\"\n</code></pre> <p>Then configuration is auto-discovered:</p> <pre><code>from py2dataiku import load_config\nconfig = load_config()\n</code></pre>"},{"location":"reference/","title":"Auto-generated API Reference","text":"<p>API documentation auto-generated from source code docstrings.</p>"},{"location":"reference/#core-module","title":"Core Module","text":""},{"location":"reference/#py2dataiku","title":"<code>py2dataiku</code>","text":"<p>py2dataiku - Convert Python data processing code to Dataiku DSS recipes and flows.</p> <p>This library analyzes Python code (pandas, numpy, scikit-learn) and generates equivalent Dataiku DSS recipe configurations, flow structures, and visual diagrams.</p> <p>Two analysis modes are available: 1. LLM-based (recommended): Uses AI to understand code semantics 2. Rule-based (fallback): Uses AST pattern matching</p> <p>Visualization formats: - SVG: Scalable vector graphics (pixel-accurate Dataiku styling) - HTML: Interactive canvas with hover/click - ASCII: Terminal-friendly text art - PlantUML: Documentation-ready diagrams - Mermaid: GitHub/Notion compatible</p>"},{"location":"reference/#py2dataiku.Py2Dataiku","title":"<code>Py2Dataiku(provider='anthropic', api_key=None, model=None, use_llm=True)</code>","text":"<p>Main converter class with hybrid LLM + rule-based approach.</p> <p>This class provides a unified interface for converting Python code to Dataiku flows, with LLM as primary and rule-based as fallback.</p> <p>Initialize the converter.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>LLM provider name (\"anthropic\", \"openai\")</p> <code>'anthropic'</code> <code>api_key</code> <code>Optional[str]</code> <p>API key for LLM provider</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>Model name override</p> <code>None</code> <code>use_llm</code> <code>bool</code> <p>Whether to use LLM (True) or rule-based (False)</p> <code>True</code>"},{"location":"reference/#py2dataiku.Py2Dataiku.convert","title":"<code>convert(code, flow_name='converted_flow', optimize=True)</code>","text":"<p>Convert Python code to a Dataiku flow.</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>Python source code</p> required <code>flow_name</code> <code>str</code> <p>Name for the generated flow</p> <code>'converted_flow'</code> <code>optimize</code> <code>bool</code> <p>Whether to optimize the flow</p> <code>True</code> <p>Returns:</p> Type Description <code>DataikuFlow</code> <p>DataikuFlow object</p>"},{"location":"reference/#py2dataiku.Py2Dataiku.analyze","title":"<code>analyze(code)</code>","text":"<p>Analyze code without generating flow (LLM mode only).</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>Python source code</p> required <p>Returns:</p> Type Description <code>AnalysisResult</code> <p>AnalysisResult with extracted steps and metadata</p>"},{"location":"reference/#py2dataiku.Py2Dataiku.generate_diagram","title":"<code>generate_diagram(flow, format='mermaid')</code>","text":"<p>Generate a diagram for a flow (legacy method).</p> <p>Parameters:</p> Name Type Description Default <code>flow</code> <code>DataikuFlow</code> <p>DataikuFlow to visualize</p> required <code>format</code> <code>str</code> <p>Diagram format (\"mermaid\", \"graphviz\", \"ascii\", \"plantuml\")</p> <code>'mermaid'</code> <p>Returns:</p> Type Description <code>str</code> <p>Diagram string in specified format</p>"},{"location":"reference/#py2dataiku.Py2Dataiku.visualize","title":"<code>visualize(flow, format='svg', **kwargs)</code>","text":"<p>Generate Dataiku-style visualization of a flow.</p> <p>This method produces pixel-accurate representations matching the Dataiku DSS interface styling.</p> <p>Parameters:</p> Name Type Description Default <code>flow</code> <code>DataikuFlow</code> <p>DataikuFlow to visualize</p> required <code>format</code> <code>str</code> <p>Output format (\"svg\", \"html\", \"ascii\", \"plantuml\")</p> <code>'svg'</code> <code>**kwargs</code> <p>Additional arguments for the visualizer</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Visualization string in the specified format</p>"},{"location":"reference/#py2dataiku.Py2Dataiku.save_visualization","title":"<code>save_visualization(flow, output_path, format=None)</code>","text":"<p>Save flow visualization to a file.</p> <p>Parameters:</p> Name Type Description Default <code>flow</code> <code>DataikuFlow</code> <p>DataikuFlow to visualize</p> required <code>output_path</code> <code>str</code> <p>Path to save the file</p> required <code>format</code> <code>str</code> <p>Output format (auto-detected from extension if not provided)</p> <code>None</code>"},{"location":"reference/#py2dataiku.convert","title":"<code>convert(code, optimize=True)</code>","text":"<p>Convert Python code to a Dataiku flow using rule-based analysis.</p> <p>This is the legacy method using AST pattern matching. For better results, use convert_with_llm() instead.</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>Python source code string</p> required <code>optimize</code> <code>bool</code> <p>Whether to optimize the flow (merge recipes, reorder steps)</p> <code>True</code> <p>Returns:</p> Type Description <code>DataikuFlow</code> <p>DataikuFlow object representing the converted pipeline</p>"},{"location":"reference/#py2dataiku.convert_with_llm","title":"<code>convert_with_llm(code, provider='anthropic', api_key=None, model=None, optimize=True, flow_name='converted_flow')</code>","text":"<p>Convert Python code to a Dataiku flow using LLM-based analysis.</p> <p>This is the recommended method - uses AI to understand code semantics and produces more accurate mappings, especially for complex code.</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>Python source code string</p> required <code>provider</code> <code>str</code> <p>LLM provider (\"anthropic\", \"openai\")</p> <code>'anthropic'</code> <code>api_key</code> <code>Optional[str]</code> <p>API key (uses environment variable if not provided)</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>Model name (uses provider default if not provided)</p> <code>None</code> <code>optimize</code> <code>bool</code> <p>Whether to optimize the flow</p> <code>True</code> <code>flow_name</code> <code>str</code> <p>Name for the generated flow</p> <code>'converted_flow'</code> <p>Returns:</p> Type Description <code>DataikuFlow</code> <p>DataikuFlow object representing the converted pipeline</p> Example <p>flow = convert_with_llm(''' ... import pandas as pd ... df = pd.read_csv('data.csv') ... df = df.dropna() ... result = df.groupby('category').agg({'amount': 'sum'}) ... ''') print(flow.get_summary())</p>"},{"location":"reference/#py2dataiku.convert_file","title":"<code>convert_file(path, optimize=True)</code>","text":"<p>Convert a Python file to a Dataiku flow using rule-based analysis.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to a Python file</p> required <code>optimize</code> <code>bool</code> <p>Whether to optimize the flow</p> <code>True</code> <p>Returns:</p> Type Description <code>DataikuFlow</code> <p>DataikuFlow object representing the converted pipeline</p>"},{"location":"reference/#py2dataiku.convert_file_with_llm","title":"<code>convert_file_with_llm(path, provider='anthropic', api_key=None, model=None, optimize=True, flow_name=None)</code>","text":"<p>Convert a Python file to a Dataiku flow using LLM-based analysis.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to a Python file</p> required <code>provider</code> <code>str</code> <p>LLM provider (\"anthropic\", \"openai\")</p> <code>'anthropic'</code> <code>api_key</code> <code>Optional[str]</code> <p>API key (uses environment variable if not provided)</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>Model name (uses provider default if not provided)</p> <code>None</code> <code>optimize</code> <code>bool</code> <p>Whether to optimize the flow</p> <code>True</code> <code>flow_name</code> <code>Optional[str]</code> <p>Name for the generated flow (defaults to filename)</p> <code>None</code> <p>Returns:</p> Type Description <code>DataikuFlow</code> <p>DataikuFlow object representing the converted pipeline</p>"},{"location":"reference/#models","title":"Models","text":""},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow","title":"<code>py2dataiku.models.dataiku_flow.DataikuFlow(name='converted_flow', source_file=None, generation_timestamp=None, datasets=list(), recipes=list(), zones=list(), recommendations=list(), optimization_notes=list(), warnings=list())</code>  <code>dataclass</code>","text":"<p>Represents a complete Dataiku flow (pipeline).</p> <p>A flow is a directed acyclic graph (DAG) of datasets connected by recipes. This is the main output of the py2dataiku conversion process.</p>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.graph","title":"<code>graph</code>  <code>property</code>","text":"<p>Build and return a DAG representation of this flow.</p>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.input_datasets","title":"<code>input_datasets</code>  <code>property</code>","text":"<p>Get all input datasets.</p>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.output_datasets","title":"<code>output_datasets</code>  <code>property</code>","text":"<p>Get all output datasets.</p>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.intermediate_datasets","title":"<code>intermediate_datasets</code>  <code>property</code>","text":"<p>Get all intermediate datasets.</p>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.add_dataset","title":"<code>add_dataset(dataset)</code>","text":"<p>Add a dataset to the flow.</p>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.get_dataset","title":"<code>get_dataset(name)</code>","text":"<p>Get a dataset by name.</p>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.add_recipe","title":"<code>add_recipe(recipe)</code>","text":"<p>Add a recipe to the flow.</p>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.get_recipe","title":"<code>get_recipe(name)</code>","text":"<p>Get a recipe by name.</p>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.get_recipes_by_type","title":"<code>get_recipes_by_type(recipe_type)</code>","text":"<p>Get all recipes of a specific type.</p>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.get_recommendations","title":"<code>get_recommendations()</code>","text":"<p>Get optimization recommendations for the flow.</p>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.add_recommendation","title":"<code>add_recommendation(type, priority, message, impact=None, action=None)</code>","text":"<p>Add a recommendation.</p>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.get_column_lineage","title":"<code>get_column_lineage(column, dataset=None)</code>","text":"<p>Get lineage information for a column.</p> <p>Traces a column backward through the flow's recipes to find its origin dataset and any transformations applied along the way.</p> <p>Parameters:</p> Name Type Description Default <code>column</code> <code>str</code> <p>The column name to trace.</p> required <code>dataset</code> <code>Optional[str]</code> <p>The dataset containing the column. If None, searches      output datasets first, then all datasets.</p> <code>None</code> <p>Returns:</p> Type Description <code>ColumnLineage</code> <p>ColumnLineage describing the column's origin and transformations.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the column or dataset cannot be found in the flow.</p>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.validate","title":"<code>validate()</code>","text":"<p>Validate the flow structure using DAG analysis.</p>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.add_zone","title":"<code>add_zone(zone)</code>","text":"<p>Add a zone to the flow.</p>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.get_zone","title":"<code>get_zone(name)</code>","text":"<p>Get a zone by name.</p>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary representation.</p>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Reconstruct a DataikuFlow from a dictionary (inverse of to_dict).</p>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.from_json","title":"<code>from_json(json_str)</code>  <code>classmethod</code>","text":"<p>Reconstruct a DataikuFlow from a JSON string.</p>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.from_yaml","title":"<code>from_yaml(yaml_str)</code>  <code>classmethod</code>","text":"<p>Reconstruct a DataikuFlow from a YAML string.</p>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.to_yaml","title":"<code>to_yaml()</code>","text":"<p>Convert to YAML string.</p>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.to_json","title":"<code>to_json(indent=2)</code>","text":"<p>Convert to JSON string.</p>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.to_recipe_configs","title":"<code>to_recipe_configs()</code>","text":"<p>Get Dataiku API-compatible recipe configurations.</p>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.get_summary","title":"<code>get_summary()</code>","text":"<p>Get a text summary of the flow.</p>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.export_all","title":"<code>export_all(directory)</code>","text":"<p>Export all flow artifacts to a directory.</p>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.visualize","title":"<code>visualize(format='svg', **kwargs)</code>","text":"<p>Generate a visual representation of the flow.</p> <p>Parameters:</p> Name Type Description Default <code>format</code> <code>str</code> <p>Output format - \"svg\", \"ascii\", \"plantuml\", \"html\", \"mermaid\"</p> <code>'svg'</code> <code>**kwargs</code> <p>Additional arguments passed to the visualizer</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>String containing the visualization in the requested format</p>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.to_svg","title":"<code>to_svg(output_path=None)</code>","text":"<p>Generate SVG visualization.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str</code> <p>Optional file path to save the SVG</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>SVG content as string</p>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.to_ascii","title":"<code>to_ascii()</code>","text":"<p>Generate ASCII art visualization.</p>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.to_html","title":"<code>to_html(output_path=None)</code>","text":"<p>Generate interactive HTML visualization.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str</code> <p>Optional file path to save the HTML</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>HTML content as string</p>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.to_plantuml","title":"<code>to_plantuml(output_path=None)</code>","text":"<p>Generate PlantUML visualization.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str</code> <p>Optional file path to save the PlantUML code</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>PlantUML content as string</p>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.to_png","title":"<code>to_png(output_path, scale=2.0)</code>","text":"<p>Export flow as PNG image.</p> <p>Requires cairosvg package: pip install cairosvg</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str</code> <p>File path to save the PNG</p> required <code>scale</code> <code>float</code> <p>Scale factor for resolution (default 2.0)</p> <code>2.0</code>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.to_pdf","title":"<code>to_pdf(output_path)</code>","text":"<p>Export flow as PDF.</p> <p>Requires cairosvg package: pip install cairosvg</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str</code> <p>File path to save the PDF</p> required"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of recipes in this flow.</p>"},{"location":"reference/#py2dataiku.models.dataiku_flow.DataikuFlow.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over the recipes in this flow.</p>"},{"location":"reference/#py2dataiku.models.dataiku_recipe.DataikuRecipe","title":"<code>py2dataiku.models.dataiku_recipe.DataikuRecipe(name, recipe_type, inputs=list(), outputs=list(), steps=list(), group_keys=list(), aggregations=list(), join_type=JoinType.LEFT, join_keys=list(), selected_columns=None, partition_columns=list(), order_columns=list(), window_aggregations=list(), sampling_method=SamplingMethod.RANDOM, sample_size=None, split_condition=None, sort_columns=list(), top_n=None, ranking_column=None, code=None, settings=None, source_lines=list(), notes=list())</code>  <code>dataclass</code>","text":"<p>Represents a recipe in a Dataiku flow.</p> <p>Recipes are the transformation nodes that process data. They appear as circles in the Dataiku flow visualization.</p>"},{"location":"reference/#py2dataiku.models.dataiku_recipe.DataikuRecipe.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary representation.</p>"},{"location":"reference/#py2dataiku.models.dataiku_recipe.DataikuRecipe.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Reconstruct a DataikuRecipe from a dictionary (inverse of to_dict).</p>"},{"location":"reference/#py2dataiku.models.dataiku_recipe.DataikuRecipe.to_api_dict","title":"<code>to_api_dict()</code>","text":"<p>Convert to Dataiku API-compatible dictionary.</p>"},{"location":"reference/#py2dataiku.models.dataiku_recipe.DataikuRecipe.to_json","title":"<code>to_json()</code>","text":"<p>Convert to Dataiku API-compatible dictionary.</p> <p>Note: This method returns a dict, not a JSON string. It is an alias for <code>to_api_dict()</code> kept for backward compatibility.</p>"},{"location":"reference/#py2dataiku.models.dataiku_recipe.DataikuRecipe.add_step","title":"<code>add_step(step)</code>","text":"<p>Add a step to a Prepare recipe.</p>"},{"location":"reference/#py2dataiku.models.dataiku_recipe.DataikuRecipe.add_aggregation","title":"<code>add_aggregation(column, function, output_column=None)</code>","text":"<p>Add an aggregation to a Grouping recipe.</p>"},{"location":"reference/#py2dataiku.models.dataiku_recipe.DataikuRecipe.add_join_key","title":"<code>add_join_key(left_column, right_column, match_type='EXACT')</code>","text":"<p>Add a join key to a Join recipe.</p>"},{"location":"reference/#py2dataiku.models.dataiku_recipe.DataikuRecipe.add_note","title":"<code>add_note(note)</code>","text":"<p>Add a note about this recipe.</p>"},{"location":"reference/#py2dataiku.models.dataiku_recipe.DataikuRecipe.get_step_summary","title":"<code>get_step_summary()</code>","text":"<p>Get a summary of steps for Prepare recipes.</p>"},{"location":"reference/#py2dataiku.models.dataiku_recipe.DataikuRecipe.create_prepare","title":"<code>create_prepare(name, input_dataset, output_dataset, steps=None)</code>  <code>classmethod</code>","text":"<p>Factory method to create a Prepare recipe.</p>"},{"location":"reference/#py2dataiku.models.dataiku_recipe.DataikuRecipe.create_grouping","title":"<code>create_grouping(name, input_dataset, output_dataset, keys, aggregations=None)</code>  <code>classmethod</code>","text":"<p>Factory method to create a Grouping recipe.</p>"},{"location":"reference/#py2dataiku.models.dataiku_recipe.DataikuRecipe.create_join","title":"<code>create_join(name, left_dataset, right_dataset, output_dataset, join_keys, join_type=JoinType.LEFT)</code>  <code>classmethod</code>","text":"<p>Factory method to create a Join recipe.</p>"},{"location":"reference/#py2dataiku.models.dataiku_recipe.DataikuRecipe.create_python","title":"<code>create_python(name, inputs, outputs, code)</code>  <code>classmethod</code>","text":"<p>Factory method to create a Python recipe.</p>"},{"location":"reference/#py2dataiku.models.dataiku_dataset.DataikuDataset","title":"<code>py2dataiku.models.dataiku_dataset.DataikuDataset(name, dataset_type=DatasetType.INTERMEDIATE, connection_type=DatasetConnectionType.FILESYSTEM, schema=list(), source_variable=None, source_line=None, notes=list())</code>  <code>dataclass</code>","text":"<p>Represents a dataset (node) in a Dataiku flow.</p> <p>Datasets are the data containers that recipes read from and write to. They appear as blue squares in the Dataiku flow visualization.</p>"},{"location":"reference/#py2dataiku.models.dataiku_dataset.DataikuDataset.is_input","title":"<code>is_input</code>  <code>property</code>","text":"<p>Check if this is an input dataset.</p>"},{"location":"reference/#py2dataiku.models.dataiku_dataset.DataikuDataset.is_output","title":"<code>is_output</code>  <code>property</code>","text":"<p>Check if this is an output dataset.</p>"},{"location":"reference/#py2dataiku.models.dataiku_dataset.DataikuDataset.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary representation.</p>"},{"location":"reference/#py2dataiku.models.dataiku_dataset.DataikuDataset.to_json","title":"<code>to_json()</code>","text":"<p>Convert to Dataiku API-compatible JSON.</p>"},{"location":"reference/#py2dataiku.models.dataiku_dataset.DataikuDataset.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Reconstruct a DataikuDataset from a dictionary (inverse of to_dict).</p>"},{"location":"reference/#py2dataiku.models.dataiku_dataset.DataikuDataset.add_column","title":"<code>add_column(name, col_type, nullable=True, default=None)</code>","text":"<p>Add a column to the schema.</p>"},{"location":"reference/#py2dataiku.models.dataiku_dataset.DataikuDataset.add_note","title":"<code>add_note(note)</code>","text":"<p>Add a note about this dataset.</p>"},{"location":"reference/#py2dataiku.models.prepare_step.PrepareStep","title":"<code>py2dataiku.models.prepare_step.PrepareStep(processor_type, params=dict(), disabled=False, name=None, meta_type='PROCESSOR', source_line=None, source_code=None)</code>  <code>dataclass</code>","text":"<p>Represents a single step/processor in a Dataiku Prepare recipe.</p> <p>Each step performs a specific transformation on the data. Steps are executed in order within a Prepare recipe.</p>"},{"location":"reference/#py2dataiku.models.prepare_step.PrepareStep.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary representation.</p>"},{"location":"reference/#py2dataiku.models.prepare_step.PrepareStep.to_json","title":"<code>to_json()</code>","text":"<p>Convert to Dataiku API-compatible JSON.</p>"},{"location":"reference/#py2dataiku.models.prepare_step.PrepareStep.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Reconstruct a PrepareStep from a dictionary (inverse of to_dict).</p>"},{"location":"reference/#py2dataiku.models.prepare_step.PrepareStep.fill_empty","title":"<code>fill_empty(column, value, source_line=None)</code>  <code>classmethod</code>","text":"<p>Create a FillEmptyWithValue step.</p>"},{"location":"reference/#py2dataiku.models.prepare_step.PrepareStep.rename_columns","title":"<code>rename_columns(renamings, source_line=None)</code>  <code>classmethod</code>","text":"<p>Create a ColumnRenamer step.</p>"},{"location":"reference/#py2dataiku.models.prepare_step.PrepareStep.delete_columns","title":"<code>delete_columns(columns, source_line=None)</code>  <code>classmethod</code>","text":"<p>Create a ColumnDeleter step.</p>"},{"location":"reference/#py2dataiku.models.prepare_step.PrepareStep.string_transform","title":"<code>string_transform(column, mode, source_line=None)</code>  <code>classmethod</code>","text":"<p>Create a StringTransformer step.</p>"},{"location":"reference/#py2dataiku.models.prepare_step.PrepareStep.set_type","title":"<code>set_type(column, target_type, source_line=None)</code>  <code>classmethod</code>","text":"<p>Create a TypeSetter step.</p>"},{"location":"reference/#py2dataiku.models.prepare_step.PrepareStep.parse_date","title":"<code>parse_date(column, formats=None, source_line=None)</code>  <code>classmethod</code>","text":"<p>Create a DateParser step.</p>"},{"location":"reference/#py2dataiku.models.prepare_step.PrepareStep.filter_on_value","title":"<code>filter_on_value(column, values, matching_mode='EQUALS', keep=True, source_line=None)</code>  <code>classmethod</code>","text":"<p>Create a FilterOnValue step.</p>"},{"location":"reference/#py2dataiku.models.prepare_step.PrepareStep.remove_rows_on_empty","title":"<code>remove_rows_on_empty(columns, keep_empty=False, source_line=None)</code>  <code>classmethod</code>","text":"<p>Create a RemoveRowsOnEmpty step.</p>"},{"location":"reference/#py2dataiku.models.prepare_step.PrepareStep.remove_duplicates","title":"<code>remove_duplicates(columns=None, source_line=None)</code>  <code>classmethod</code>","text":"<p>Create a RemoveDuplicates step.</p>"},{"location":"reference/#py2dataiku.models.prepare_step.PrepareStep.create_column_grel","title":"<code>create_column_grel(column, expression, source_line=None)</code>  <code>classmethod</code>","text":"<p>Create a CreateColumnWithGREL step.</p>"},{"location":"reference/#py2dataiku.models.prepare_step.PrepareStep.regexp_extract","title":"<code>regexp_extract(column, pattern, output_columns=None, source_line=None)</code>  <code>classmethod</code>","text":"<p>Create a RegexpExtractor step.</p>"},{"location":"reference/#py2dataiku.models.prepare_step.PrepareStep.python_udf","title":"<code>python_udf(code, input_columns, output_column, source_line=None)</code>  <code>classmethod</code>","text":"<p>Create a PythonUDF step (fallback for complex operations).</p>"},{"location":"reference/#py2dataiku.models.prepare_step.PrepareStep.if_then_else","title":"<code>if_then_else(column, condition, then_value, else_value, output_column=None, source_line=None)</code>  <code>classmethod</code>","text":"<p>Create an IfThenElse step for conditional value assignment.</p>"},{"location":"reference/#py2dataiku.models.prepare_step.PrepareStep.switch_case","title":"<code>switch_case(column, cases, default_value=None, output_column=None, source_line=None)</code>  <code>classmethod</code>","text":"<p>Create a SwitchCase step for multi-branch conditional logic.</p>"},{"location":"reference/#py2dataiku.models.prepare_step.PrepareStep.translate_values","title":"<code>translate_values(column, translations, output_column=None, source_line=None)</code>  <code>classmethod</code>","text":"<p>Create a TranslateValues step for value mapping/replacement.</p>"},{"location":"reference/#py2dataiku.models.prepare_step.PrepareStep.extract_with_jsonpath","title":"<code>extract_with_jsonpath(column, json_path, output_column=None, source_line=None)</code>  <code>classmethod</code>","text":"<p>Create an ExtractWithJSONPath step for JSON data extraction.</p>"},{"location":"reference/#py2dataiku.models.prepare_step.PrepareStep.split_url","title":"<code>split_url(column, extract_components=None, source_line=None)</code>  <code>classmethod</code>","text":"<p>Create a SplitURL step for URL parsing.</p>"},{"location":"reference/#py2dataiku.models.prepare_step.PrepareStep.fold_multiple_columns","title":"<code>fold_multiple_columns(columns, var_name='variable', value_name='value', source_line=None)</code>  <code>classmethod</code>","text":"<p>Create a FoldMultipleColumns step (melt/unpivot).</p>"},{"location":"reference/#py2dataiku.models.prepare_step.PrepareStep.transpose_rows_to_columns","title":"<code>transpose_rows_to_columns(source_line=None)</code>  <code>classmethod</code>","text":"<p>Create a TransposeRowsToColumns step.</p>"},{"location":"reference/#py2dataiku.models.prepare_step.PrepareStep.unfold","title":"<code>unfold(column, source_line=None)</code>  <code>classmethod</code>","text":"<p>Create an Unfold step for exploding list-like columns.</p>"},{"location":"reference/#py2dataiku.models.prepare_step.PrepareStep.coalesce","title":"<code>coalesce(columns, output_column=None, source_line=None)</code>  <code>classmethod</code>","text":"<p>Create a Coalesce step to pick the first non-null value.</p>"},{"location":"reference/#py2dataiku.models.prepare_step.PrepareStep.fill_column","title":"<code>fill_column(column, value, source_line=None)</code>  <code>classmethod</code>","text":"<p>Create a FillColumn step to set an entire column to a constant.</p>"},{"location":"reference/#py2dataiku.models.prepare_step.PrepareStep.get_description","title":"<code>get_description()</code>","text":"<p>Get a human-readable description of this step.</p>"},{"location":"reference/#exceptions","title":"Exceptions","text":""},{"location":"reference/#py2dataiku.exceptions","title":"<code>py2dataiku.exceptions</code>","text":"<p>Custom exception hierarchy for py2dataiku.</p>"},{"location":"reference/#py2dataiku.exceptions.Py2DataikuError","title":"<code>Py2DataikuError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for all py2dataiku errors.</p>"},{"location":"reference/#py2dataiku.exceptions.ConversionError","title":"<code>ConversionError</code>","text":"<p>               Bases: <code>Py2DataikuError</code></p> <p>Error during code-to-flow conversion.</p>"},{"location":"reference/#py2dataiku.exceptions.ProviderError","title":"<code>ProviderError</code>","text":"<p>               Bases: <code>Py2DataikuError</code></p> <p>Error communicating with an LLM provider.</p>"},{"location":"reference/#py2dataiku.exceptions.LLMResponseParseError","title":"<code>LLMResponseParseError</code>","text":"<p>               Bases: <code>ProviderError</code></p> <p>Error parsing the JSON response from an LLM provider.</p>"},{"location":"reference/#py2dataiku.exceptions.InvalidPythonCodeError","title":"<code>InvalidPythonCodeError</code>","text":"<p>               Bases: <code>ConversionError</code></p> <p>The provided Python code could not be parsed or analyzed.</p>"},{"location":"reference/#py2dataiku.exceptions.ValidationError","title":"<code>ValidationError</code>","text":"<p>               Bases: <code>Py2DataikuError</code></p> <p>Error during flow or recipe validation.</p>"},{"location":"reference/#py2dataiku.exceptions.ExportError","title":"<code>ExportError</code>","text":"<p>               Bases: <code>Py2DataikuError</code></p> <p>Error during DSS project export.</p>"},{"location":"reference/#py2dataiku.exceptions.ConfigurationError","title":"<code>ConfigurationError</code>","text":"<p>               Bases: <code>Py2DataikuError</code></p> <p>Error in py2dataiku configuration.</p>"}]}